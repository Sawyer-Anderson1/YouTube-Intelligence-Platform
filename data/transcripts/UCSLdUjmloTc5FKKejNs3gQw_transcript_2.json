[
    {
        "text": " Anthropic have just done something huge. They let 16 clawed agents on the loose to build a C compiler and after running 24/7 for two weeks, they actually built one that could compile the Linux kernel and even run Doom, which is super impressive and definitely wasn't possible with the older versions of Opus 4. But people are calling this achievement clickbait and a halftruth because of the questionable techniques Anthropic used to get this result. So, did Anthropic cheat? Hit subscribe and let's find out. We'll split this video into three parts. First, we'll go through how the experiment was set up. Then, we'll go through the key findings which I think every developer will learn a lot from. And finally, we'll go through if the results are valid because I really have some opinions on how anthropic was able to build this compiler. Okay, this experiment was carried out by Nicholas Carlini who in my opinion is a very intelligent human being. I mean, let's take a look at how he set this up. So, the actual project lived in a directory called upstream, which was mounted to 16 different Docker containers. I know there are only four here, but let's imagine there are 16. And each one of these Docker containers contained a version of Claude Code running Opus 4.6. And would clone the upstream repo to workspace and would make all the changes in workspace and then push to upstream. This was really clever because each agent could work in isolation without affecting the work of the other agents. Now, if there was ever a merge conflict, then Claude would be clever enough to resolve it and then push it back up to upstream. Each agent will pick from some tasks. Now, I'm not sure if these tasks were generated by a human or were generated by the agent based on running some tests, but there were some tasks that existed with names. And each agent would take a new task and whenever it took a new task, it would create a new session. So, in order to keep these agents running for a long time, a Ralph loop was used. And so the agents would work on the task, finish the task, push to upstream, then pick a new task with a fresh session, and keep doing that over and over again. Now, if you've watched our video on Ralph, you'll know that the key to having longunning agents is to have clearly defined tasks. But if you have 16 agents running at the same time, how do you prevent them from picking the exact same task? Task locking. The way this works is somewhere not mentioned by the author, a list of tasks exists and an agent will pick a task and give it a text file matching the name of that task will create a commit to lock that task so only they can work on that task and",
        "start": 0.08,
        "duration": 316.79999999999995
    },
    {
        "text": "this works is somewhere not mentioned by the author, a list of tasks exists and an agent will pick a task and give it a text file matching the name of that task will create a commit to lock that task so only they can work on that task and another agent picks the same task and makes the same text file, if they try to push it to the upstream repo, Git will reject it saying that that file already exists and then they'll have to work on a different task. And this is the basis of how Carini stress tested the ability of longunning agents powered by Opus 4.6 and the results are truly amazing. But from doing this experiment, he has found some interesting things that I think every single developer can learn from. The first thing is to build a test harness or a script that runs different types of tests. Because when Nick was running the experiment, yes, we're on firstname terms now, he experienced Claude breaking existing features whenever a new feature was worked on. So he built a testing harness consisting of highquality tests from popular open- source repos like SQ Lite, LIB, JPEG, and Reddis. And to prevent context pollution, he made sure the test harness only outputed logs that were useful to the agent. So basically error logs and created a file with all the other type of logs so that Claude could look into it whenever it needed. However, with thousands of tests, it would take agents hours to run the whole test suite when they could be using that time to do something else. So this is where Nick did something really clever. He added a fast flag to his testing harness, meaning an agent will only run either 1% or 10% of the total tests based on a figure that Nick wanted. And if each agent ran 10% then that would be 160% of the tests which is more than enough but this isn't a bad thing. And the way it worked is the tests so the specific tests that were run by each agent were randomized but the seed number was the same making it deterministic. So each agent will have the exact same random tests and eventually go through all the test suite much faster than if they were running the whole thing by themselves. The next point is also clever, but a bit of a controversial one since it's to make use of existing technology. So far, each agent has been running unit tests from a bunch of existing open-source projects, which was working well, splitting them up into 1% or 10% chunks. But when it came to compiling the Linux kernel, since these source files aren't individual unit tests, things became a bit difficult because each agent will try to compile the whole thing, come up with the same error, and so try to fix it and overwrite each other's fix. So",
        "start": 158.319,
        "duration": 611.6
    },
    {
        "text": "But when it came to compiling the Linux kernel, since these source files aren't individual unit tests, things became a bit difficult because each agent will try to compile the whole thing, come up with the same error, and so try to fix it and overwrite each other's fix. So to have each agent run a percentage of the compilation and then have GCC, so the GNU compiler, run the rest of it. Nick called GCC the Oracle since the Linux kernel should compile perfectly with it. So if an agent compiled a section of the Linux kernel with its own compiler, so a different section for each agent and the rest with GCC, if something broke, it was definitely the agent's compiler and not GCC and therefore the agent would just fix that thing instead of fixing a bug from another agent. Now, this is controversial because it's using an existing compiler to do something that Claude was asked to do from scratch, but we'll talk more about this towards the end of the video. Let's move on to the next point, which is to give your agent memory. Since new tasks are worked on by fresh clawed sessions that have pretty much no memory of what was done before it, Nick found it useful to update the readme file and to have different progress files with instructions of where things left off and the progress of the project so that new sessions would have a good base to start off from and not introduce bugs that have already been fixed before. And the final more obvious point is to give your agents different roles. The beauty of having multiple agents work on a code base in parallel is that multiple things can be done to the exact same piece of the code at the same time. So when new code wasn't being written, Nick gave unique roles to agent like one to check for duplicated code, another to find a way of making the code as performant as possible and he even got one to critique the design from the perspective of a Rust developer who I hope didn't announce to the other agents that it was a Rust developer. But as successful as this project was, the real question is, did Anthropic cheat to get this result? Well, kind of. So the task was to build a C compiler from scratch and the agent didn't have access to the internet. So it came up with all the code itself. Or did it? Because it did have access to the test suites of open-source projects and it had access to the compiled version of GCC. So technically it could have poked and prodded the GCC compiler giving it inputs and inspecting the outputs and using that to direct the design of its own compiler written in Rust. But to be fair if I was building a C compiler from scratch, I would do the",
        "start": 308.24,
        "duration": 896.3189999999998
    },
    {
        "text": "GCC. So technically it could have poked and prodded the GCC compiler giving it inputs and inspecting the outputs and using that to direct the design of its own compiler written in Rust. But to be fair if I was building a C compiler from scratch, I would do the compilers, see how they were implemented and use that to shape the direction of my own compiler. Now, if I was building a compiler for a brand new language, then of course things would be much difficult and maybe this would be a really good test to do for Claude to see if it's actually good at creating compilers from scratch. Maybe that's another idea for Nick to try out. But let's move on to talk about the autonomous nature of the experiment since that was also tested. And to be fair, yes, Claude did write all of the code, but it had some heavy steering from a human. A human decided on what test suite to run. A human started the loop and decided to use Ralph. A human was the one that built the test harness and gave agents specific roles. So while this is far from someone telling Claude to build a compiler and leaving it to run forever and ever, I wouldn't say the code was written by an agent that was 100% autonomous. Because how good would the compiler have been if a human wasn't involved in the first place? And even with all the systems in place designed by a human, the claude C compiler did have some key limitations. For example, it used the assembler and linker from GCC because the one it created was too buggy. It also needed GCC's 16bit x86 compiler in order to boot up Linux. And to top that all off, the code wasn't very efficient. The most optimized version of the Claude's compiler was less performance than the least optimized version of the GCC compiler. So, it looks like developers aren't going anywhere anytime soon, or at least for",
        "start": 452.72,
        "duration": 1086.96
    }
]