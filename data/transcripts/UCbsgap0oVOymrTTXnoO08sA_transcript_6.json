[
    {
        "text": " So Meta is secretly building a new AI model that might already be miles ahead of its own llama systems. Google researchers just created an AI that can draw full scientific diagrams and charts for researchers. And at the same time, a brand new Gemini model is quietly being tested that can design user interfaces and perfect SVG graphics like an experienced designer. All of this is happening right now, mostly behind the scenes. So let's talk about it. Let's start with Meta [music] because the avocado leak has the most internal reset energy. According to a report that cites an internal memo from Meta's Elite Super Intelligence Labs, Meta's next model camed avocado is already outperforming the Llama 4 series in both power and efficiency. The memo is reportedly written by Megan Fu, a product manager inside that super intelligence labs group, and it describes Avocado as Meta's most capable pre-trained base model so far. That line is doing a lot of work because base model means the version right after pre-training [music] before all the polishing and specialization that usually makes a model feel sharp in the real world. The headline claim is efficiency. Internal testing allegedly shows Avocado getting 10 times compute efficiency wins on text tasks [music] compared to Llama 4 Maverick, which had been the flagship high performance model in that Llama 4 lineup. 10 times more compute efficient is the kind of jump that changes budgets, [music] product plans, and what you can afford to run at scale. When people talk about frontier level models, the hard part is usually the cost. You can train something huge, [music] then you realize serving it at real scale is brutal. If those numbers are even close, Meta is basically saying they found a way to get a lot more output per unit of compute. Then the memo goes further and claims Avocado is over 100 times more efficient than Llama for Behemoth. Behemoth is described as delayed and it's been talked about like the big heavy monster that costs a fortune [music] to run. So this is meta basically admitting internally that the old path was painful and the new path makes that old approach look outdated. It also hints that the bigger is always better era is getting replaced by smarter training and smarter design wins. This avocado work is also framed as the first major milestone for meta super intelligence labs. That unit was formed by Mark Zuckerberg in 2025 and the lab is led by Alexander Wang, the founder of Scale AI. That detail matters because Scale AI is deeply tied to data and evaluation infrastructure. When a company puts someone like that in charge of its next AI push, it signals a focus on training pipelines, [music] data quality, testing discipline, and building models that can actually ship and hold up under pressure. The memo paints Avocado as a groundup rebuild. Llama 4 leaned into native multimodality and mixture of experts architecture.",
        "start": 2.72,
        "duration": 332.4
    },
    {
        "text": "in charge of its next AI push, it signals a focus on training pipelines, [music] data quality, testing discipline, and building models that can actually ship and hold up under pressure. The memo paints Avocado as a groundup rebuild. Llama 4 leaned into native multimodality and mixture of experts architecture. something rebuilt from scratch to deliver frontier level performance without the compute pain that plagued earlier projects to use the vibe of how this is being described. It has completed pre-training and the claim is that it's already outperforming leading open-source models in knowledge and visual perception. That's a very specific pairing. Knowledge implies better general understanding and recall. Visual perception implies stronger performance on vision tasks where models interpret images or visual inputs. There's another interesting detail in the leak. Despite being just a base model and not yet fine-tuned for specific tasks, Avocado is reportedly matching the performance of models that have already gone through extensive [music] post-training optimization. That's a major flex because most base models feel like raw potential. You usually see them improve after alignment and fine-tuning [music] where they pick up instruction following behavior and guard rails and better task performance. Meta is basically suggesting Avocado comes out of pre-training unusually strong. Now layer in the strategy angle. Meta built a reputation as the open-source champion with the llama series. Releasing weights gave them adoption community and thousands of developers building on top of their work. The avocado memo reinforces rumors that Meta may pivot toward a closed model strategy here. [music] The logic is simple. If you have something truly ahead, releasing weights gives competitors a free look, and it also makes monetization harder. Keeping weights private opens the door to premium enterprise tools, controlled APIs, and product lockin across Meta's own ecosystem. The report even mentions this rumored shift being tied to the departure of Meta's longtime AI chief, Yan Lun, who is known as a strong supporter of open development. The leak also places Avocado on a timeline. Pre-training is done. Post-training and safety alignment still need to happen before public release. Meta is reportedly aiming for a first half 2026 launch with hints toward Q1 or early spring. Alongside Avocado, Meta is also developing Mango, described [music] as a next-gen model focused on highfidelity image and video generation. That pairing is important because it suggests Meta wants a full stack. Avocado for text and code, plus Mango for visual generation. If they land both, they can power everything from assistance to content tools to ads and creative workflows [music] at meta scale. And that's the piece people sometimes miss. Meta isn't building this for a small developer niche. They have billions of users and huge surfaces where models can be deployed. If Avocado truly delivers big efficiency gains, Meta can run it more widely, more cheaply, [music] and more often. That could mean smarter content understanding across Facebook and Instagram, stronger AI assistance, better ranking and retrieval, better",
        "start": 169.12,
        "duration": 648.6410000000001
    },
    {
        "text": "They have billions of users and huge surfaces where models can be deployed. If Avocado truly delivers big efficiency gains, Meta can run it more widely, more cheaply, [music] and more often. That could mean smarter content understanding across Facebook and Instagram, stronger AI assistance, better ranking and retrieval, better tooling. The same model performance jump looks very different when it's plugged into systems that touch billions of daily interactions. Now, let's jump to paper banana because this one sounds academic at first, but the idea is super easy to understand when you translate it into real workflow pain. Paper banana is a framework introduced [music] by a team from Google and Peeking University. The whole problem they target is simple. Researchers waste a ridiculous amount of time turning ideas and results into figures that look good enough for publication. You can have the best paper in the world and then you lose hours fighting with diagram layout, alignment, arrows, spacing, fonts, colors, and making [music] charts look clean. This is a real bottleneck because science moves through papers and papers move through visuals. Paper Banana tries to automate that by acting like a mini production team that makes figures for you. It uses a multi-agent setup with five specialized agents. Instead of one model guessing everything from one prompt, the work is split into roles so the output gets cleaner and closer [music] to what researchers actually want. The workflow is described in two phases. The first is linear planning. One agent, the retriever, searches a database and finds 10 reference examples that match what you're trying to draw. Think of it like pulling similar figures from real papers. So the system follows the house style of academic publishing instead of making something that looks like random clip art. Another agent, the planner, reads the methodology text and translates it into a detailed description of what the figure should show. In normal human terms, [music] it's turning dense research writing into a clear blueprint. What boxes exist, what connects to what, what order the process flows in. Then the stylist steps in and acts like a design consultant. Its job is matching the Nurip look, meaning the visual style people expect at top AI conferences, including the kinds of color palettes and layout choices that feel professional. Then comes the second phase, iterative refinement. This is where Paper Banana tries, checks, and improves. The visualizer agent actually generates the visual output. For diagrams, it uses image models like Nano Banana Pro. For statistical plots, it takes a different path and writes executable Python mattplot lib code instead of drawing pixels. The reasoning is straightforward. [music] Charts need numerical precision and image generation models have a habit of making numbers look right while being wrong. Paper Banana treats charts like code, so the final plot matches the data exactly. After the visualizer produces something, the critic agent inspects the output and compares it with the source [music]",
        "start": 329.12,
        "duration": 946.161
    },
    {
        "text": "need numerical precision and image generation models have a habit of making numbers look right while being wrong. Paper Banana treats charts like code, so the final plot matches the data exactly. After the visualizer produces something, the critic agent inspects the output and compares it with the source [music] visual glitches. Then it gives feedback and the system repeats that loop for three rounds of refinement. That detail matters because a lot of AI drawing fails on the second order problems. Labels missing, arrows pointing the wrong way, repeated elements, weird spacing, little mistakes that a human would catch instantly. The critic is basically there to keep the figure honest and readable. They also built a benchmark to measure this called paper banana bench. It's a data set of 292 test [music] cases curated from actual NURIP's 2025 publications. That's not toy data. That's real paper figures. The kind people actually submit and review. They evaluated paper banana using a VLM as a judge approach and compared it against leading baselines. The reported gains are specific. Overall score improves by 17.0%, conciseness by 37.2%, readability by 12.9%, aesthetics by 6.6, 6% and faithfulness by 2.8%. Those numbers paint a consistent picture. The biggest jump is conciseness, meaning it's cutting clutter and making figures [music] communicate faster. Paper Banana also performs especially well in the agent and reasoning diagram category with a 69.9% overall score. That lines up with what you see in modern AI papers where agent workflows get complicated quickly. A figure that visually explains an agent loop, [music] tools, memory, planning, feedback, and environment interaction can take a human a long time to draw cleanly. Paper Banana is targeting that exact kind of diagram. There's also a built-in aesthetic guideline that favors soft tech pastels over harsh primary colors. That might sound like a small detail, yet it matches what you actually see in modern AI conferences. Cleaner pallets, gentle gradients, a calmer vibe, and less visual noise. The system even talks about domain specific aesthetic preferences. Agent and reasoning figures lean toward friendly illustrative visuals with two D vector robots, human avatars, emojis, and UI aesthetics like chat bubbles and document icons. Computer vision and 3D visuals lean into geometric density like camera frustms, ray lines, point clouds, and RGB axis correspondence. Generative and learning figures often use modular flow visuals like 3D cuboids for tensors, matrix grids, and zone grouping with light pastel fills. Theory and optimization figures go minimalist with graph nodes, manifolds, grayscale pallets, and a single highlight color. This matters because a figure that looks right to a community often gets understood faster. Paper Banana also lays out the trade-off between image-made plots and codemade plots. Image generation tends to look attractive and that's why people like it. Yet, it can hallucinate numbers or repeat elements. Code-based mattplot lib plots have the standard academic look and preserve exact data fidelity. Paper banana chooses based on the job.",
        "start": 479.759,
        "duration": 1295.0410000000002
    },
    {
        "text": "out the trade-off between image-made plots and codemade plots. Image generation tends to look attractive and that's why people like it. Yet, it can hallucinate numbers or repeat elements. Code-based mattplot lib plots have the standard academic look and preserve exact data fidelity. Paper banana chooses based on the job. hybrid approach is the core trick. Now, shift to the third story, Gemini. A new Gemini 3 Pro checkpoint has been spotted in AB testing inside Google's AI studio. People testing it are reporting unusually strong performance on UI generation and SVG output. SVG is a pain point because it demands precision. Sloppy SVG has wrong shapes, broken paths, misaligned elements, and it becomes unusable fast. Early testers are saying this checkpoint outputs complex interfaces and SVG structures with high accuracy. And the examples being shared are credited to communities like Dev Mode Discord. Google hasn't confirmed a release and the report points out something important. Google has tested different Gemini checkpoints in the past that never became public launches. The writeup even mentions how something like a Gemini 3 Pro preview was observed before and broader release didn't necessarily follow. Multiple checkpoints are circulating, so it's unclear whether this is an upcoming update, a move toward a 3.5 version, or a limited experiment. This checkpoint has also been identified in testing on LM Arena, which usually means it's being compared in publicish benchmarking environments. What is clear is Google's intent. The company is positioning Gemini as its primary large language model across consumer and business applications, targeting creative and productivity workflows. Strong UI generation and SVG accuracy fit that strategy perfectly because product teams and designers constantly need quick prototypes. A model that can generate clean interface code or SVG assets speeds up mockups, internal demos, landing pages, dashboards, and even design system components. All right, that's it for this one. Drop your thoughts in the comments. Thanks for watching, and I'll catch you in the next one.",
        "start": 656.32,
        "duration": 1507.4420000000005
    }
]