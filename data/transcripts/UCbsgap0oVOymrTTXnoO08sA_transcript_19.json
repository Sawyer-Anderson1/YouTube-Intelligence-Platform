[
    {
        "text": " Deepseek might have just exposed its next flagship model on GitHub and it looks huge JIPU launched a long context model built for real coding and reasoning Japan just revealed emotion computing AI based on body signals And news research dropped a competitive programming monster that learns by running code and getting punished for failure All right let's start with the most suspicious story of the week Deep Seek. This one isn't an official product announcement It's basically developers catching deepseek, leaving music evidence of what they're building next And when the evidence comes from code it's always more serious because companies can control press releases yet code changes tend to expose what's actually music happening internally On January 21st, 2026, new information surfaced suggesting that Deepseek may unveil its next flagship AI model Deepseek V4. And the timing that people are guessing is midFebruary around the Lunar New Year period The biggest expectations a large jump in coding ability So, where is that coming from On January 20th, which also happens to be the first anniversary of Deepseek R1, developers noticed Deepseek updated a large batch of Flash MLA code on GitHub. And it wasn't like one or two files It was music massive 114 files were changed Inside those updates developers started seeing this unknown model identifier model one and it appears 28 times Obviously, that's not some placeholder somebody forgot to delete It's repeated enough times that it clearly refers music to something real What makes it even more important is that this modal one label shows up alongside another model identifier called V32, which refers to Deepseek V3.2. And in some places the code explicitly separates Modell 1 from V32, meaning Modell 1 is not treated like a normal minor version bump That immediately triggered the bigger question Is DeepSeek building a completely new flagship on a new architecture rather than just iterating on V3? The developer analysis started noticing technical differences between modal 1 and van in a few key areas And these are the kinds of areas that point to deep changes under the hood not surface level improvements One is KV cache layout KV cache is basically the memory structure that makes modern transformer inference efficient It's what allows models to keep track of context without reprocessing everything from scratch If Deepseek is changing KV cache layout it means they're redesigning how the model stores and retrieves information during generation And that's usually tied to performance memory efficiency and speed especially in long context The second thing is parity handling If there are differences in how sparsity is handled that's often the sign of compute music efficiency tricks The whole idea of parity in AI models is that not everything needs to be fully active all the time You can skip parts reduce computer speed things up and still keep quality high if you do it right And the third thing is FP8 decoding support FP8 is about squeezing performance out of",
        "start": 2.56,
        "duration": 358.6410000000001
    },
    {
        "text": "parity in AI models is that not everything needs to be fully active all the time You can skip parts reduce computer speed things up and still keep quality high if you do it right And the third thing is FP8 decoding support FP8 is about squeezing performance out of increasing throughput and making huge models practical If their new system is explicitly built around FP8 decoding support that tells you this model is being engineered music for efficiency at scale not just raw intelligence So when you combine these signals KV cache redesign parity differences and FP8 decoding support it looks less like Deepseek v3.3 and more like Deepseek is building something structurally new And this aligns perfectly with Deepseek's recent research direction too Deepseek previously released two technical papers One about a new training method called modified hierarchical connections or MHC music and another about a biologically inspired AI memory module called Engram. So now people are speculating that V4 could be the first DeepSseek music flagship model to actually integrate these research directions New training structure through MHC music and memory behavior through engrams That's where the whispers of a redesigned architecture become even more believable because you don't publish research like that unless you're planning to deploy it into a real system Deepseek still hasn't confirmed anything officially Yet the code is basically screaming that something new is being prepared And if the rumored Lunar New Year midFebruary timeline is accurate we're not music talking about a distant future release We're talking about something already in the final stretch All right Now, we cover big AI breakthroughs here but the real shift is tools music people can actually used AI3D is moving fast and most tools look good until you rotate the model and it falls apart This one actually holds up It's called Heightm 3D, and they're sponsoring today's video Hem 3D 2 generates textures in a structure way so details feel built into the geometry not pasted on It can fill in unseen areas like under sides and interiors And it reduces baked lighting for cleaner more consistent materials great for PBR and 3D printing There's also a portrait mode that keeps character details like hair and faces sharp Bottom line Heidi 3D2 music is built for real workflows, game assets product concepts prototyping and 3D music printing where usable output actually matters You can try it with a free test and the links in the description All right now back to the video Now, right after that we jump to China again but this time it's not rumors It's an actual model release from Zoo AI. So, GU AI, also known as Z.A.I., just released a model called GLM4.7 Flash. And this one is very targeted It's built for people who want something strong at reasoning and coding while still being realistic to run locally So you can actually deploy it in your own workflow without needing some massive",
        "start": 181.68,
        "duration": 666.0
    },
    {
        "text": "also known as Z.A.I., just released a model called GLM4.7 Flash. And this one is very targeted It's built for people who want something strong at reasoning and coding while still being realistic to run locally So you can actually deploy it in your own workflow without needing some massive than people admit because most best-in-class AI models right now are either too expensive to run too heavy to host or locked behind paid APIs. GLM 4.7 flash is described as a 30BA 3B model That sounds technical so let's translate it into normal human language It means the model has around 31 billion total parameters Yet, it doesn't use all of them every time it music generates a token It's built using an architecture called mixture of experts In Japu's case they say the model is a text generation model with 31B parameters It supports English and Chinese and it's configured for chat and conversational used not only raw completion It uses an architecture tag called GLM4_MO_. and it sits inside the wider GLM 4.7 family right next to GLM 4.7 and GLM 4.7 FP8. And here's a key details ZAI positions GLM 4.7 Flash as the lightweight and even free tier option compared to their full GLM 4.7 model while still targeting serious tasks like coding reasoning and agent workflows. In other words it's meant for the people who can't deploy something in the 358B music class but still want something that plays in the same modern league Now, one of the biggest reasons this model is relevant is context music length GLM 4.7 Flash supports 128,000 tokens of context and it uses a standard interface and a chat template which makes it easier to integrate into existing AI tools without rewriting your whole pipeline Now, benchmarks matter too music because every model claims it's amazing So Z AI compares GLM music 4.7 flash against other models in the same category like music Quen 330 BA3B thinking 257 and GPT OSS 20B. Their claim is GLM 4.7 flash leads or stays competitive across math reasoning long horizon benchmarks and coding agent tests For most tasks their default settings are temperature 1.0, 0 top P 0.95 and max new tokens 131,072. That means the model is allowed to generate huge outputs and it's not forced into short answers For terminal bench and S sub bench verified they use temperature 0.7 top p 1.0 and max new tokens 16,384. For to squared bench they go even stricter temperature zero and max new tokens 16,384. Basically, if the task needs precision and stable step-by-step tool music usage they reduce randomness They also recommend enabling something called preserved thinking mode for multi-turn agent tasks like to squared bench and terminal bench 2. The idea is that the model preserves its internal reasoning across turns which matters when you're building agents that need long chains of actions corrections and function calls And finally they point out that GLM 4.7 Flash supports VLLLM, SGLANG,",
        "start": 337.28,
        "duration": 1020.0819999999998
    },
    {
        "text": "agent tasks like to squared bench and terminal bench 2. The idea is that the model preserves its internal reasoning across turns which matters when you're building agents that need long chains of actions corrections and function calls And finally they point out that GLM 4.7 Flash supports VLLLM, SGLANG, And there's already a growing ecosystem of fine-tunes and quantization including MLX conversions on hugging face So that's JIPU's move Strong coding and reasoning MOI efficiency long context and actually deployable Now, let's shift away from models and rumors and move into research that sounds almost sci-fi. Emotion computation A research team in Japan just released a new AI framework designed to model how emotions form not as some magical feelings engine but as a computational process tied to the body This research comes from assistant professor Chihi from Nar Institute of Science and Technology NIS working with assistant professor Kazuki Miyazawa and then master student Kazuki Tsurumaki from Osaka University. The study was made available online July ard 2025 and later published in e transactions on effective computing volume 16 issue 4 on December ard 2025. So this isn't an early concept post It's fully published research The whole model is based on the theory of constructed emotions This theory says emotions are not simply hardwired reactions They're concepts built by the brain in real time by combining internal bodily signals with external sensory information Internal signals are introspection like heart rate External signals are interception like what you see or hear The researchers explained that while theories like this exist the actual computational process behind emotion concept formation has not been explored enough So what did they do They built a computational model using multilayered multivocal latent durishlay allocation called MMLDA. MLDDA is basically a probabilistic generative model that discovers hidden patterns and categories by looking at how different data types occur together without being told explicit labels So the model is not trained on this is fear this is joy or music this is sadness It learns from patterns They trained it using unlabeled data from human participants watching emotion evoking content like images and videos The system wasn't informed what emotion each thing was supposed to represent It had to discover the categories on its own They collected real human data 29 participants viewed 60 images from the international effective picture system a standard data set in psychological research While participants viewed these images researchers recorded physiological responses like heart rate using wearable sensors and also collected verbal descriptions So you basically get three layers of data visual input physiological response and language Then they compared what the model learned against participants self-reported emotional evaluations And here's the headline number about 75% agreement That's way above chance which suggests that the model's discovered emotion categories align strongly with how people actually experience the emotions And the practical implication here is surprisingly big This kind of AI could be integrated into emotion robots and",
        "start": 516.8,
        "duration": 1369.9229999999995
    },
    {
        "text": "emotional evaluations And here's the headline number about 75% agreement That's way above chance which suggests that the model's discovered emotion categories align strongly with how people actually experience the emotions And the practical implication here is surprisingly big This kind of AI could be integrated into emotion robots and in more contextsensitive ways based on what someone is experiencing not just what they're typing They even mentioned that because the music model can infer emotional states people struggle to express in words it could be useful in mental health support healthcare monitoring and assisting technologies for conditions like developmental disorders or dementia So yeah this is not AI feels This is AI understands emotional experience patterns Now, let's close with the most hardcore part of this whole set Now coder job news research just released newcomer 14b. And this is basically an AI that's trained for the most difficult type of coding you can test Competitive programming The kind of coding where there's a huge set of hidden tests strict time limits strict memory limits and if your solution is even slightly wrong you fail No mercy They built it on top of Quen 314b, then improved it using reinforcement learning And in plain language that means they trained it like this The model writes code then the code is actually run in a sandbox and it only gets rewarded if the program truly works So, it's not learning to talk like a programmer It's learning to survive real test cases And the results show it On Live Codebench V6, which contains 454 competitive programming problems and covers tasks from August este 2024 to May este 2025, now coder 14b, reaches 67.87% 87% pass at one And pass at one is the toughest version of scoring because it means the first answer the model gives has to be correct One shot The base model Quen 314B scores 60.79% on the same test So this training boosted it by 7.08 percentage points which is a serious jump in a benchmark like this They trained it on 24,000 verified coding problems using 48 Nvidia B200 GPUs for 4 days and they released it under the Apache 2.0 license on HuggingFace. So, it's actually usable for real projects Now, the way they trained it is honestly the coolest part because it's very different from normal AI training Each time the model generates a solution that code gets executed If it passes every hidden test it gets a reward of plus one If it fails or takes longer than 15 seconds or uses more than 4GB of memory it gets hit with minus one That's it Simple and brutal To do this safely they run the code inside isolated containers using modal so nothing can break the system And they also optimized how they test things Instead of wasting time running every single test they prioritize the hardest test cases first And if the solution fails early they stop immediately that",
        "start": 693.6,
        "duration": 1691.2019999999995
    },
    {
        "text": "this safely they run the code inside isolated containers using modal so nothing can break the system And they also optimized how they test things Instead of wasting time running every single test they prioritize the hardest test cases first And if the solution fails early they stop immediately that and more efficient On the training side they tested multiple ways of doing reinforcement learning GRPO as the base method then three objective variants called DAPo, GSPO, and GSPO plus You don't really need to memorize those names The point is they tried different learning recipes to see what works best At a very long context length of 81,920 tokens performs best with 67.87%. pass at one while GSPO scores 66.26% and GSPO plus scores 66.52%. At 40,960 tokens context length they all land around 63%. They also train the model to handle long prompts in stages First at ask context then ask and later they extend it at evaluation time to about ask using yarn context extension reaching 81,920 tokens music And there's one more smart details If the model generates something too long that goes beyond the maximum context window they don't punish it They just ignore it by setting its advantage to zero That prevents the model from learning shortcuts like forcing short answers just to avoid penalties All right that's it for today If you want more AI breakthroughs like this subscribe and drop a like Thanks for watching and I'll catch you in the next one",
        "start": 856.72,
        "duration": 1846.8039999999999
    }
]