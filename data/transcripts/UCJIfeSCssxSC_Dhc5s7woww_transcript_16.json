[
    {
        "text": " If we could take at this point a bit of a tangent and talk about education and learning If you're somebody listening to this who's a smart person interested in programming interested in AI, so I presume building something from scratch is a good beginning So can you just take me through like what you would recommend people do i So I would personally start like you said uh implementing a simple model from scratch that you can run on your computer The goal is not if you build a model from scratch to have like something you use every day for your personal projects Like it's not going to be your personal assistant replacing an existing overweight model or CHPD. It's to see what exactly goes into the LLM, what exactly comes out of the LLM, how the restraining works in that sense on your own computer preferably Um, and then you learn about the pre-training, the supervised fine-tuning, the attention mechanism You get a solid understanding of how things work But at some point you will reach a limit because small models can only do so much And the problem with learning about LLMs at scale is I would say it's exponentially more complex to make a larger model because it's not that the model just becomes larger You have to now think about sharing your parameters across multiple GPUs. You even for the KV cache there are multiple ways you can implement it One is just to understand how it works just to grow the cache let's it's like a cache you grow step by step by let's say containing lists um growing it but then that wouldn't be optimal in GPUs you wouldn't do that you would reallocate a tensor and then fill it in but that adds again another 20 30 line lines of code and for each thing you add so much code and I think the trick with a book is basically to understand how the LLM works it's not going to be your production level LLM but once you have that you can understand the production level LM i so you're trying to always build an LLM that's going to fit on one GPU i yes the most of them I have they I have some bonus materials on some models I think one of or two of them they may require multiple GPUs but the goal is to have it on one GPU and the beautiful thing is also you can selfverify it's almost like RLVR when you code these from scratch you can take uh an existing model from the hugging face transformer library um so the hugging face transformer library is great but if you want to learn about LMS I think that's not the best place to start because the code is so complex because it has to full it has to fit so many use cases also some people use it in production it has to be",
        "start": 2.879,
        "duration": 285.1999999999999
    },
    {
        "text": "library is great but if you want to learn about LMS I think that's not the best place to start because the code is so complex because it has to full it has to fit so many use cases also some people use it in production it has to be intertwined and really hard It's not linear to ready It was started as a fine-tuning library and then it grew to be like the standard representation of every model architecture and the way this loaded So hugging face is like the default place to get a model and transformers is the software that enables it So people can easily load a model and do something basic with it i And all frontier labs that have open weight models have a hugging phase transformers version of it like from deepseek to GPTOSs. So it's like the canonical weight that you can load there But again also even transformers the library is not used in production People use then slang or elm and it adds another layer of complexity i We should say that the transformers library has like 400 models i So it's a one library that tries to implement a lot of LLMs and so you have a huge code based Basically it's like huge It's like it's I don't know maybe millions hundreds of thousands of lines of code and find it's like understanding the part that you want to understand is finding the needle in the haststack. But what's beautiful about it is you have a working implementation and so you can work backwards from it What I would recommend doing or what I also do is if I want to understand for example how almost 3 is implemented I would look at the weights in the model hub the confirm file and then you can see oh they used so many layers they use let's say group query attention or mufti head attention in that case then you see all the components in like a human readable I don't know 100 lines of confirm file and then you start let's say with your GPD2 model and add these things you know and the cool thing here is you can then load the restrained weights and see if they work in your model and you want to match the same output that you get with a transformer model and then you can use that as a basically as a verifiable reward to make your architecture correct and then it's kind of sometimes it takes me a day to with almost three the challenge was the rope for the position embeddings they had a yarn extension and there was some custom uh scaling there and I couldn't quite match the these things and in this struggle you kind of understand things but the cool thing is at the end you know you have it correct because you can unit test it You can check against the reference",
        "start": 145.599,
        "duration": 541.6799999999997
    },
    {
        "text": "there was some custom uh scaling there and I couldn't quite match the these things and in this struggle you kind of understand things but the cool thing is at the end you know you have it correct because you can unit test it You can check against the reference one of the best ways to learn really like to basically reverse engineer something Yeah, i I think that that is something that everybody that's interested in getting to AI today should do And I think that's why I liked your book is like I came to language models from this RL and robotics field Like I've never had taken the time to just like learn all the fundamentals And this transformer architecture I describe as being like so fundamental as like deep learning was a thing that I had to learn in the past and people need to do this I think that where a lot of people kind of get overwhelmed is how do I apply this to have impact or find like a career path because like AI and language models make this fundamental stuff so accessible and people with motivation will learn it and then it's like how do I get the cycles on goal to contribute to research and I think that I'm actually fairly optimistic in this because the field moves so fast that a lot of times the best people like don't fully solve a problem because there's a bigger lower hat like a bigger problem to solve that's very low hanging fruit so they move on and I think that a lot of what I was trying to do in this RHF book is like take post-trading techniques and just describe how people think about them influencing the model and what people are doing and then it's remarkable how many things I just think are just like people stop studying them or don't so I think people trying to get narrow after doing the fundamentals is good and then reading the relevant papers and being engaged in the ecosystem It's like you actually the proximity that random people have online from the leading researchers like no one knows who all the anonymous account on X and ML is very popular for whatever reason and no one knows who all these people are like it could just be random people that study the stuff deeply especially with the AI tools to just be like keep I don't understand this keep digging into it I think is a very useful thing but there's a lot of research areas that just like are maybe three papers that you need to read and then one of the authors will probably email pay you back But you have to put in a lot of effort into these emails to understand the field Like I think it would be for a newcomer easily weeks of work to feel like they can truly grasp",
        "start": 275.36,
        "duration": 801.4399999999995
    },
    {
        "text": "read and then one of the authors will probably email pay you back But you have to put in a lot of effort into these emails to understand the field Like I think it would be for a newcomer easily weeks of work to feel like they can truly grasp think going narrow after you have the fundamentals be very useful to people because i it's like I became very interested in like character training which is like how you make the model funny or sarcastic or serious and like what do you do to the data to do this And it's like a student at Oxford reached out to me He's like \"Hey, I'm interested in this And I advised him and I was like \"That paper now exists And it's like I don't know there's like two or three people in the world that were very interested in this He's a PhD student which gives you an advantage But like for me that was a topic I was waiting for someone to be like \"Hey, I have time to spend cycles on this And I'm sure there's a lot more very narrow things where you're just like \"Oh, it doesn't make sense that there was no answer to this And I think that it's just like there's so much information coming that people are like I can't grab onto any of these But if you just actually stick in an area I think there's a lot of interesting things to learn Yeah, I think you can't try to do it all because it would be very overwhelming and you would burn out if you try to keep up with everything For me for example I haven't kept up with computer vision a long time just focus on LMS. But coming back to your book for example I think this is also a really great book and a really good bang for the buck because you want to learn about RLHF. I wouldn't go out there and read RL HF papers because I would be you would be spending i contradict there I just edited the book and I was like there's a chapter where I had to be like X papers say one thing and X papers say another thing and we'll see what comes out to be true i What are some of the just to go through some of the table of contents some of the ideas we might have missed in the bigger picture of the post training So first of all you do the problem setup training overview what are preferences preferences data in the optimization tools reward modeling regularization instruction tuning rejection sampling reinforcement learning I policy gradients direct alignment algorithms then constitutional AI and AI feedback reasoning and inference time scaling tool use and function calling synthetic data and distillation evaluation and then open question section over optimization style and information and then product UX character and post",
        "start": 406.4,
        "duration": 1045.1999999999994
    },
    {
        "text": "instruction tuning rejection sampling reinforcement learning I policy gradients direct alignment algorithms then constitutional AI and AI feedback reasoning and inference time scaling tool use and function calling synthetic data and distillation evaluation and then open question section over optimization style and information and then product UX character and post mentioning that connect both the educational component and the research component You mentioned the character training which is pretty interesting i Character training is interesting because there's so little out of it but we talked about how people engage with these models and like like we feel good using them because they're positive but that can go too far or it could be too positive and it's like essentially it's how do you change your data and or decision to make it exactly what you want and like OpenAI has this thing called a model spec which is essentially their internal guideline for what they want the model to do and they publish this to developers so essentially you can know what is a failure of OpenAI's training which is like they have the intentions and they haven't met it yet versus what is something that they like actually wanted to do and that you don't like and that transparency is very nice But all the methods for curating these documents and how easy it is to follow them is not very wellnown. I think the way the book is designed is that the reinforce learning chapter is obviously what people want because everybody hears about it with RLVR and it's the same algorithms and the same math but it's just like you can use it in in very different documents So I think the core of prep is like how messy preferences are is essentially rehash of a paper I wrote years ago This is essentially the chapter that's tell you why RHF is never ever fully solvable because like the way that even RL is set up is that um it assumes that preferences can be quantified and that multiple preferences can be reduced to single values And I think it relates in the economics literature to the conman Morganston utility theorem And like that is the chapter where all of that philosophical economic and like psychological context it tells you what gets compressed into doing RHF. So it's like you have all of this and then at later in the book it's like you use this RL math to make the number go up And I think that that's why I think it would be very rewarding for people to do research on is because it's like quantifying preferences is something that is just like humans have designed the problem in order to make preferences studyable. But there's kind of fundamental debates on like an example is in a language model response you have different things you care about whether it's accuracy or style and when you're collecting the data they all get",
        "start": 530.48,
        "duration": 1311.9989999999998
    },
    {
        "text": "just like humans have designed the problem in order to make preferences studyable. But there's kind of fundamental debates on like an example is in a language model response you have different things you care about whether it's accuracy or style and when you're collecting the data they all get than another and it's like like that is happening and there's a lot of philosophy there's a lot of research in other areas of the world that go into like how should you actually do this I think social choice theory is the sub field of economics around how you should aggregate preferences laughter and there's Like I was I went to a workshop that published a white paper I'm like how can you think about using social choice theory for ROHF? So I mostly would want people that get excited about the math to come and have things where they could stumble into and learn this kind of broader context I think there's a fun thing I just keep a list of all the teach reports I like of reasoning models So in the in chapter 14, which is kind of like a short summary of RLVR, there's just like a gigantic table where I just like list every single reasoning model that I like So there's just like I think in education a lot of it needs to be like at this point it's like what I like because the language models are so good at the math where it's like famous paper direct preference optimization which is like a much simpler way of pro solving the problem than RL um the derivations in the appendix skip steps of math and it's like I tried for this book like I redid the derivations and I'm like what the heck is this log trick that they use to change the math but doing it with language models they're like this is the log trick and I'm like I I don't know if I like this that the math is so commoditized. I think like some of the struggle and reading this appendix and following the math I think is good for learning and I i Yeah. So we're actually returning to this often just on the topic of education You both have brought up the word struggle i quite a bit So there is value if you're not struggling as part of this process You're not fully following the the proper process for learning I suppose some of the providers are starting to work on models for education which are designed to not give actually I haven't used them but I would guess they're designed to not give all the information at once i and make people work to do this So I think you could train models to do this and it would be a wonderful contribution where like all of this stuff in the book you have to reevaluate every decision",
        "start": 666.24,
        "duration": 1557.599999999999
    },
    {
        "text": "they're designed to not give all the information at once i and make people work to do this So I think you could train models to do this and it would be a wonderful contribution where like all of this stuff in the book you have to reevaluate every decision think there's there's a chance you work on it at AI too which I which I was like oh I think this be i it makes sense I do something like that uh did that the other day for video games for example I sometimes for my past time play video games like I like uh video games with puzzles so you know like Zelda and Metroid and there's this new game where I got stuck and I already got stuck and was okay I you know I don't want to struggle like two two days and so I used an LLM but then you say hey please don't add any spoilers just you know I'm here and there what do I have to do next and the same thing you can do I guess for math where you say okay I'm here at this point I'm getting stuck don't give me the full solution But what is something I could try you know like where you kind of carefully probe it but the problem here is I think it requires discipline and a lot of people do math for like I mean there are a lot of people who enjoy math but there also a lot of people who need to do it for their homework and then it's like the shortcut and yeah we can develop an educational LLM but the other LLM is still there and there's still a temptation to use the other LLM. I think a lot of people especially in college they they understand the stuff they're passionate about They're self-aware about it and they understand it shouldn't be easy i Like I think we just have to develop a good taste i Mhm. i We talk about research taste like school taste about stuff that you should be struggling on i and and stuff you shouldn't be struggling on which is tricky to know cut sometimes you don't have good um long-term vision about what would be actually useful to you in your career But you have to you have to develop that taste Yeah. i I was talking to maybe my fianc\u00e9 or friends about this and it's like there's this brief 10-year window where all of the homework and all the exams could be digital but before that everybody had to do all the exams in Blue Book cut there was no other way And now after AI, everybody's going to need to be in Blue Books and oral exams cut everybody could cheat so easily It's like this brief generation that had a different laughter education system that like everything could be digital and but you",
        "start": 791.04,
        "duration": 1786.0809999999992
    },
    {
        "text": "Book cut there was no other way And now after AI, everybody's going to need to be in Blue Books and oral exams cut everybody could cheat so easily It's like this brief generation that had a different laughter education system that like everything could be digital and but you going to go back laughter which is pretty funny i You mentioned character training Just zooming out on on a more general topic for that topic How much compute was required and in general to contribute as a researchers Are there places where not too much compute is required where you can actually contribute as an individual researcher i for on the character training thing I think this research is built on fine-tuning about 7 billion parameter models with Laura which is like a essentially you only fine-tune a small subset of the weights of the model I don't know exactly how many GPU hours that would take but it's doable i not doable for every academic So the situation for some academics is like so dire that the only work you can do is doing inference where you have closed models or open models and you get completions from them and you can look at them and understand the models and that's very well suited to evaluation which you become ex you want to be the best at creating representative problems that the models fail on or show certain abilities which I think that you can break through with this So I've like I think that the top end goal for a researcher working on evaluation if you want to have career momentum is the frontier labs pick up your evaluation So it's like you don't need to have every project do this But if you go from a small university with no compute and you figure out something that claude struggles with and then the next cloud model has it in the blog post like there there's your career rocket ship I think that that's hard but it's like if you want to scope the maximum possible impact with minimum compute it's something like that which is just get very narrow and it takes learning of where the models are going So you need to like build a tool that tests where not cloud 4.5 will fail If you're going to do a re if I'm going to start a research project I need to think where the models in 8 months are going to be struggling i Well what about developing totally novel ideas i This is a tradeoff I think that if you're doing a PhD, you could also be like it's too risky to work in language models I'm going way longer terms which is like what is i what is the thing that's going to define language model development in 10 years which I think that I end up being a person that's pretty practical I mean",
        "start": 907.36,
        "duration": 2043.3609999999996
    },
    {
        "text": "be like it's too risky to work in language models I'm going way longer terms which is like what is i what is the thing that's going to define language model development in 10 years which I think that I end up being a person that's pretty practical I mean into Berkeley, worst case I get a masters and then I go work in techs It's like I'm very practical about it on like the life afforded to people to work at these AI companies The amount of like OpenAI's average compensation is over a million dollars in stock a year per employee Any normal person in the US to get into this AI lab is transformation for your life So I'm pretty practical of like there's still a lot of upward mobility working in language models if you're focused and the outcomes is like look at these jobs But from a research perspective the transformation impact in these academic awards and like be the next Yan Lacun is from not working on not caring about language model development very much It's a big financial sacrifice in that case i So I get to work with some awesome students and they're like should I go work at an AI lab And I'm like like you're getting a PhD at a top school or you're going to leave to go to a lab I'm like I don't know Like if you go work at a top lab I don't blame you Don't go work at some random startup that might go to zero but if you're going to open AI, I'm like it could be worth leaving a PhD for Let's more rigorously think through this So, where would you give a recommendation for people to do a research contribution So, the options are academia so get get a PhD, spend 5 years publishing Computer resources are constrained there's uh there's research labs that are more focused on open weight models and so working there or closed frontier labs research labs i open AI anthropic ai so on i um the two gradients are the more closed the more money you tend to get and but also the le you get less credit so in terms of building a like a portfolio of things that you've done like it's very clear of what you have done as an academic and you have done this and versus if you are going to go be like trade this fairly reasonable progression for being a cog in the machine which could also be very fun So I think it's a very different career paths but the like the opportunity cost for being a researcher is very high because PhD students are paid essentially nothing So I think it ends up rewarding people that have a fairly stable safety net and they realize that they can operate in the long term which is they want to do very interesting work and get a very",
        "start": 1037.12,
        "duration": 2325.2010000000005
    },
    {
        "text": "researcher is very high because PhD students are paid essentially nothing So I think it ends up rewarding people that have a fairly stable safety net and they realize that they can operate in the long term which is they want to do very interesting work and get a very it is a privileged position to be like I'm going to see out my PhD and figure it out after because I I want to do this and I think a lot of arcade like at the same time the academic ecosystem is getting bombarded by funding getting cut and stuff So, there's just like so many different tradeoffs where I understand plenty of people that are like \"Oh, I'm I can't deal with this funding search I grant got cut for no reason by the government or I don't know what's going to happened So, I think there's a lot of uncertainty and tradeoffs that in my opinion favor just like take the take the well-paying job with meaningful impact So, it's like not also like you're getting paid to sit around at OpenAI. building like the cutting edge of things that are i changing millions of people's relationship to techs i but publication wise they're being more secretive increasingly so So you're publishing less and less and less and less and so you're you are having a positive impact at scale but it's you're a cog in the machine i I think it's honestly it hasn't changed that much Uh so I have been in academia I'm not in academia anymore At the same time I wouldn't want to miss my time in academia But what I wanted to say before I get to that part I think it hasn't changed that much I um was working in um like I was using AI or machine learning methods um for applications and computational biology with collaborators and a lot of people went from academia directly to Google and I think it's the same thing back then the professors were like you know sad that their students went into um industry because they couldn't carry on their legacy in that sense and I I think it's the same thing it's like it it hasn't changed I think that much the only thing that has changed is the scale but you know cool stuff was always developed in industry that was closed you could couldn't talk about it and I think the difference now is um well your preference do you like to talk about your work publish or you know you you are more in a closed step uh the that's one difference the compensation of course but it's always been like that I think so it really depends on you know where you feel comfortable and it's also nothing is forever the only thing right now is there's a third option which is um starting a startup that's a lot of people doing startups very risky move uh",
        "start": 1179.44,
        "duration": 2598.5610000000006
    },
    {
        "text": "always been like that I think so it really depends on you know where you feel comfortable and it's also nothing is forever the only thing right now is there's a third option which is um starting a startup that's a lot of people doing startups very risky move uh reward type of situation where joining an industry lab I think is pretty safe you know also upward mobility Honestly, I think if once you have been at a industry lab it will be easier to find future jobs But then again you know you know it's like yeah how much do you enjoy the team and working on propriety things versus how do you like the publishing work I mean publishing is stressful It is um you know like uh acceptance rate at conferences can be arbitrary can be very frustrating but also high reward If you have a paper published you feel good because your name is on there You have a high accomplishment and you know i I feel like my friends who are professors seem on average happier than my friends who work at a frontier lab laughter to be totally honest cut that's just grounding and the frontier labs definitely do this 996 which essentially is shorthand for work all the time i Can you describe 996 as culture that's I believe you could say invented in China and uh adopted in Silicon Valley. What's what's 996? Just 9:00 am to 9:00 pump i 6 days a week i 6 days a week What is that 72 hours Okay. So, what is this basically the standard in AI companies in Silicon Valley? More and more this kind of grind mindset Yeah. I mean not maybe not exactly like that but I think there is a trend towards it And it's interesting I think it almost flipped because when I was in academia I felt like that because uh as a professor you had to write grants you had to do you had to teach and you had to do your research It's like three jobs in one and it is more than a full-time job if you want to be successful And um I feel like now like Nathan just said the professors in comparison to a lab I think they have less like even maybe pressure or workload than at a frontier lab because i they work a lot They're just so fulfilled by like working with students and having a constant runway of mentorship and like a mission that is very people oriented I think in an era when things are moving very fast and are very chaotic is very rewarding to people Yeah. And I think at a startup I think it's this pressure It's like you have to make it and it's like it is really important that people put in the time but well it is really hard because you have to deliver constantly and I've",
        "start": 1318.88,
        "duration": 2866.561000000001
    },
    {
        "text": "very rewarding to people Yeah. And I think at a startup I think it's this pressure It's like you have to make it and it's like it is really important that people put in the time but well it is really hard because you have to deliver constantly and I've I don't know if I could do it forever It's like a interesting pace Um and is exactly like we talked about in the beginning these models are leapfrogging each other and they are just constantly like trying to take the next step compared to the competitors It's just ruthless I think right now i I think this leapfrogging nature and having multiple players is actually an underrated driver of language modeling process where competition is so deeply ingrained to people and these companies have intentionally created very strong culture like anthropic is known to be so culturally like deeply committed and organized I mean like we hear so little from them and everybody at anthropic seems very aligned and it's like being at a culture that is super tight and having this competitive dynamic is like talk about a thing that's going to make you work hard and create things that are better So I think that this that comes at the cost of human capital which is like i you can only do this for so long and people are definitely burning out I think I I wrote a post on burnout was like I've tried in and out of this myself especially trying to like be a manager of bull mode training It's a crazy job doing this The book Apple in China by Patrick McGee. He talked about the how hard the Apple engineers worked to set up the supply chains in China and he was like they had saving marriage programs and he told in a podcast he was like people died from this level of working hard So I think there just like it's a perfect environment for creating progress based on human expense and I it's there's going to be a lot there's a lot of the human expense is the 996 that we started this with which is like people do really grind I also read this book I think they had a code word for if someone had to go home to spend time with their family to save the marriage and it's crazy Then colleagues want to say okay this is like red alert for this situation We have to let that person go home this weekend And um but at the same time I don't think they were forced to work It's really they were so passionate about the product I guess that it is it is you you get into that mindset and I I had that sometimes as an academic but also as an independent person I have that sometimes I overwork and it's unhealthy I had you know I had back issues I had neck issues",
        "start": 1454.72,
        "duration": 3133.043
    },
    {
        "text": "product I guess that it is it is you you get into that mindset and I I had that sometimes as an academic but also as an independent person I have that sometimes I overwork and it's unhealthy I had you know I had back issues I had neck issues maybe should have taken But it's not because no one forced me to It's because I wanted to work because i that's what Open AI and like they want to do this work i Yeah. There's also there's also a a feeling a fervor that's building especially in Silicon Valley aligned with the scaling laws idea where there's this hype where the world will be transformed on a scale of weeks and you want to be at the center of it and then you know um I have this great fortune of having conversations with wide variety of human beings and from there I get to see all these bubbles and echo chambers across snorts the world and it's fascinating to see how we humans form And I think it's fair to say that Silicon Valley is a kind of echo chamber uh a kind of u silo and bubble I think bubbles are actually really useful and effective It's not necessarily a negative thing cut you could be ultra productive It could be the the Steve Jobs reality distortion field cut you just convince each other the breakthroughs are imminent and by convincing each other of that you make the breakthroughs imminent Mhm. i Burn Hobart wrote a book classifying bubbles but essentially one of them is financial bubbles which is like speculation which is bad and the other one is like I don't know the term but effectively for bailouts because it pushes people to build these things and I do think AI is in this but I worry about it transitioning to a financial bubble which is like it's i yeah but also in the space of ideas that bubble you are doing a reality distortion field and that means you are deviating from reality and if you go too far from reality while also working you know 996 and you you might miss some fundamental aspects of the human experience including in Silicon Valley and this is a common problem in Silicon Valley is like it's a very specific geographic area you might not understand the Midwest perspective the full experience of all the other different humans in the United States and across the world and you and you speak a certain way to each other you convince each other of a certain thing and that that gets you into real trouble Whether AI is a big success and becomes a powerful technology or it's not in either trajectory you can get yourself into trouble So, you have to consider all of that Here you are a young person trying to decide what you want to do with your life The thing",
        "start": 1590.24,
        "duration": 3416.4020000000005
    },
    {
        "text": "Whether AI is a big success and becomes a powerful technology or it's not in either trajectory you can get yourself into trouble So, you have to consider all of that Here you are a young person trying to decide what you want to do with your life The thing this but the SF AAI memos have gotten to the point where permanent underclass was one of them which was the idea that the last 6 months of 2025 was the only time to build durable value in an AI startup or model otherwise all the value will be captured by existing companies and you will therefore be poor which like that's an example of the SF thing that goes so far I still think for young people that going to be able to tap into it if you are really passionate about wanting to have impact in AI like being physically in SF is the most likely place where you're going to do this but it has it has tradeoffs I think SF is an incredible place but there is a bit of a bubble and if you go into that bubble which is extremely valuable just get out also read history books read literature uh visit other places in the world Twitter is not and Substack is not the entire world I think I would One of my one of people I worked with is moving to SF and it's like I need to get him a copy of the season of the witch which is a history of SF from like 1960 to 1985 which goes through like the hippie re revolution like they all the um gays kind of taking over the city and that culture emerging and then the HIV AIDS crisis and other things and it's just like that is so recent and so much turmoil and hurt but also like love and SF and it's like no one knows about this It's a great book Season of the Witch. I recommend it a bunch of my SF friends were who do get out recommended it to me and I think that it's just like living there like I lived there and I didn't appreciate this context and it's just like so recently",
        "start": 1733.679,
        "duration": 3618.4009999999994
    }
]