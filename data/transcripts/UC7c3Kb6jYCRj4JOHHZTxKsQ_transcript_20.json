[
    {
        "text": " The biggest news of the week is GPT 5.2 was dropped. I covered it in a video yesterday, so I won't go too deep into it, but it is a really, really good model. It excels at coding, knowledge work, tasks, frontier math, and so much more. If you have any paid OpenAI accounts, it will be available to you in chatbt.com. Next, speaking of OpenAI, the unthinkable happened. Disney is partnering with them. OpenAI is going to bring Disney characters to Sora. So, you're going to be able to prompt Sora and use the likeness of many beloved Disney characters that you know well. This is the first major announcement of its kind. I believe there is a huge market for fans to create fanfiction based on real IP of companies and Disney is the biggest holder of them all. Marvel, Star Wars, Pixar, of course, all of the original Disney characters. There's just so much there and I'm excited to play around with it. Obviously, OpenAI needs to go above and beyond to put guard rails on Sora so that Disney's characters are not misused. Now, I'm immediately thinking of Ply the Prompter and letting him go wild on Sora. And basically, there's no possible way to fully prevent these models from generating inappropriate videos of these characters. There's just no way. That is the nature of non-deterministic artificial intelligence. So, we're going to see how this plays out. It's going to be interesting cuz there's certainly going to be some viral videos featuring Disney characters that shouldn't be doing what they're doing. This is a three-year licensing agreement. According to the announcement, Sora will be able to generate short user prompted social videos that can be viewed and shared by fans drawing on more than 200 Disney, Marvel, Pixar, and Star Wars characters. And listen to how cool this is. A selection of these fanpired Sora short form videos will be available to stream on Disney Plus. So, I really think an audience being able to create stories around this well-known IP is actually going to be super beneficial for Disney. Disney will also become a major customer of OpenAI, committing to using its interface and its API. And most importantly, Disney is making a billiondoll investment in Open AI for equity. they are investing and taking a piece of open AI which just seems like incredible deal making really on both sides. Bob Iger from Disney, Sam Alman from Open AI and so I'm really excited for this and literally on the same day Disney sent a cease and desist to Google. So this was really a big win for OpenAI this week. According to Variety, Disney has accused Google of widpread copyright infringement using, of course, Disney's IP. And they sent a cease and desist letter to Google. And Bob Iger and Sam Alman went on CNBC this morning to talk about it. And he also said, \"Yeah, we just sent the cease and desist",
        "start": 0.08,
        "duration": 357.67900000000003
    },
    {
        "text": "Disney has accused Google of widpread copyright infringement using, of course, Disney's IP. And they sent a cease and desist letter to Google. And Bob Iger and Sam Alman went on CNBC this morning to talk about it. And he also said, \"Yeah, we just sent the cease and desist tried to have conversations with Google and those conversations stalled out probably in terms of licensing and ultimately they went with Sam Alman and OpenAI. This is the face of a man that just had a big win. And specifically, what are they accusing Google of? Listen to this. Google is infringing Disney's copyrights on a massive scale by copying a large corpus of Disney's copyrighted works without authorization to train and develop generative artificial intelligence models and services and by using AI models and services to commercially exploit and distribute copies of its protected works to consumers in violation of Disney's copyrights. Now, that is a massive accusation if it ends up being true. So, at this point, Sam Alman is known in Silicon Valley to be just a masterful dealmaker. And if this doesn't highlight his abilities, I don't know what will. In the same week that they launched GPT 5.2, they also landed a blockbuster deal with Disney to be really the first company to leverage Disney's IP or a major company's IP, no bigger than Disney, in artificial intelligence. And then on the same day, a cease and desist to Google. One thing I've learned is never bet against Sam Alman. All right, next. Time magazine has revealed the cover of the 2025 person of the year. And this year it is the architects of artificial intelligence. So let's go through the two covers. And the one on the right is obviously more interesting. There's Zuckerberg of Meta, of course, Lisa Sue AMD. We have Elon Musk, we all know who he is. Jensen of Nvidia, right in the middle. We have Sam Alman, Open AI. We have Dennis Hassabis of Deep Mind and Google right here. Surprisingly, no Sundar. We also have Daario Amade of Anthropic and Feale Lee on the right. Now, missing as I mentioned, Sundar, but also Ilia Sudskver, Mirror Morati, Satia Nadella's missing as well. And if this image looks familiar, by the way, let me just show you what this is referencing. This is a very famous photo of construction workers building the Rockefeller Center way back in the 30s and it was taken during the Great Depression. It's just a very iconic photo and it's interesting to see that they use this photo which has a lot of negative connotations to it to AI architects. And you know who else should be Times person of the year? The sponsor of today's video. With the amount of code being written increasing so much because of artificial intelligence, the bottleneck is becoming clear. It's code review. I've heard stories where PRs can take weeks to get merged just because",
        "start": 178.879,
        "duration": 673.3590000000002
    },
    {
        "text": "know who else should be Times person of the year? The sponsor of today's video. With the amount of code being written increasing so much because of artificial intelligence, the bottleneck is becoming clear. It's code review. I've heard stories where PRs can take weeks to get merged just because humans to actually review them. That's where Gretile comes in. Gretile is an AI code review tool that takes code review off of your hands. Grapile can review all of your PRs and it actually learns as it goes. And so how does it actually learn? Well, you can do it manually by tagging Greile with direct feedback or giving it thumbs up and thumbs down to show comments that you like or don't like. But it also learns from the review discussions in the comments itself. So if you give another engineer feedback, Greile will see that and learn it and provide that feedback automatically next time. And it adds it to your repo rules without you having to do anything. You can also add your own custom rules and basically you're just unlocking the ability to take all of the AI code, review it as if it were reviewed by a human and get it merged much more quickly. Try it today. 14 days for free. I'll drop links down in the description below. Thanks to GPile for sponsoring this video. And now back to the video. All right. Next, in which seems like the most obvious announcement ever, Adobe is bringing their major products to Chat GPT. You can now edit in Photoshop in ChatGBT, but also you can use Adobe Express and Acrobat all within the Chat GBT environment. I've been saying this for a while, but ChatGBT is quickly becoming the default internet. The same way that Google has been the internet for the last 15, 20 years. ChatGpt is now quickly becoming the standard. It is the place you go. They are going to define the internet for better or worse for you for the foreseeable future. So within chatbt just like so you'll be able to edit images using Photoshop plus natural language. It's actually really neat. I mean to be honest dolly and really just chatbt image generation is coming for Photoshop's lunch. So the fact that they're getting ahead of it and getting to where the users are I very much appreciate. So again all of this is done in natural language but now you have the tools that you might be more familiar with with the Adobe suite of products. All right. Next. According to the information, China's Deepseek might actually be using the Frontier chips from Nvidia to train the next version of Deepseek. And why is that a big deal? Well, those chips were banned. Chinese companies should not have them. I think it was probably the most well-known secret in Silicon Valley is that no, they kind of probably were using these chips all along. All right.",
        "start": 339.12,
        "duration": 962.3980000000004
    },
    {
        "text": "the next version of Deepseek. And why is that a big deal? Well, those chips were banned. Chinese companies should not have them. I think it was probably the most well-known secret in Silicon Valley is that no, they kind of probably were using these chips all along. All right. has been developing its next major model using several thousand Nvidia state-of-the-art Blackwell chips, which the US has forbidden from being exported to China, according to six people with knowledge of the matter. The chips DeepSeek is using were smuggled into China, the people said. And specifically the strategy, listen to this, they're probably going to make a movie about this in the future, involves sending them, them being the chips, to data centers in countries that are allowed to buy them, dismantling the servers containing the chips, and importing the equipment in pieces to China to then reassemble. Wild. And so even more, why is this a big deal? Well, there's been a lot of talk about Huawei chips being good enough to train the next generation of models on, and it's becoming clear those chips are actually not good enough. Deepseek in particular has been known for their efficiency in pre-training and RL, but it is becoming apparent maybe there's a little fudging of the numbers there. And they even talk about Deepseek's innovations. By the way, I'm not taking anything away from the Deep Seek team. They put all of these cutting edge techniques out in the open. People reviewed them and they are very real, specifically sparse attention which allows DeepSeek to get these models to answer questions with far fewer parameters used at the inference time. And Blackwell chips specifically are really good at this sparse attention mechanism. So, we'll see what happens. And I really am super excited to see what R2, Deepseek R2 ends up looking like. And according to this article, maybe we'll see it in February for the Lunar New Year, but we'll see. All right, something that also just came out is Runway ML's Gen 4.5. This is their new iteration of their Frontier texttovideo model. So, you can see everything you're seeing in this video was created by Runway. And the realism, the detail, the colors, the taste, it all looks incredible. This is on par with anything I've seen from Sora, from Google, with VO. And if we look at the benchmarks, here's Runway Gen 405 at an ELO of 1247 on the artificial analysis texttovideo leaderboards as of November 30th. VO3 coming in second place. Sora 2 Pro all the way down here at 1206, although keep in mind the Y scale. Definitely seeing some chart crimes lately, but still performing very, very well. A few other examples. In this one, we can see a complex scene. Lots of different objects in the scene. We can see reflections in that picture frame. There's a very realistic looking bird over those dirty dishes. The tissue, the",
        "start": 486.319,
        "duration": 1276.1580000000013
    },
    {
        "text": "chart crimes lately, but still performing very, very well. A few other examples. In this one, we can see a complex scene. Lots of different objects in the scene. We can see reflections in that picture frame. There's a very realistic looking bird over those dirty dishes. The tissue, the incredible. Over here we have animals. It looks like a goat pulling a polar bear that's sitting in an ice cube. Very, very realistic. We have displacement of fluid motion. Here we have physics accuracy. So, we see a cactus hugging a balloon. And of course, the balloon pops. Very cool looking indeed. And human expressiveness, really detailed, uncanny valley has been closed. Look how real that girl looks. So you can go try it out. Go to Runway ML. Let me know what you think. Next, according to Bloomberg, Meta is moving away from open source. This is incredibly disappointing to hear. My heart is breaking a little bit. Of course, I'm reminded from the famous quote from Batman. you either die a hero or live long enough to become the villain. Well, unfortunately, when you make so many statements and so many positive affirmations towards a open source and then now it's seeming like they're moving away from that, the perception becomes even worse versus if they were just closed source all along. So, a few things to know. Meta Platform Inc. Mark Zuckerberg is getting personally involved in day-to-day work and pivoting the company's focus to an artificial intelligence model it can make money off of. Now, making money off of it doesn't necessarily mean it's going to go closed source. The new model, which is codenamed Avocado, is expected to debut sometime next spring and may be launched as a closed model that Meta can sell access to. Meta can still sell access to open source and they can provide services on top of selling their open source model and still build all of that goodwill, but again, maybe they're not going to be doing that. Meta Strategy Shift comes after the company released Llama 4, which was a while ago. Kind of a flop. an open- source model that disappoints in Silicon Valley and Zuckerberg and the company is now using several thirdparty models as part of the training process for Avocado. Very interesting. It was also reported that they're moving money away from their ARVR division and into AI. So, I think this is the most blatant example of him saying, \"Yeah, we probably made a mistake with ARVR, at least to that degree of investment.\" They've also been recruiting some of the top people in the AI industry, so they are definitely going all in on AI. Zuckerberg is another one of those people who I would just never bet against. So don't count them out. I just wish they continued their open source journey. Now here's something really interesting. The TBD group, which is their group led by Alexander Wing, is",
        "start": 645.2,
        "duration": 1562.7170000000012
    },
    {
        "text": "going all in on AI. Zuckerberg is another one of those people who I would just never bet against. So don't count them out. I just wish they continued their open source journey. Now here's something really interesting. The TBD group, which is their group led by Alexander Wing, is of the training process for Avocado, distilling from rival models, including Google's Gemma, OpenAI's GPT OSS, and Quen. So yeah, why not? They put them out open source. Why not use them? I still think Meta is in an incredible position to really compete strongly in artificial intelligence. I made this little chart about which companies are well positioned, where they are strong and where they're weak. And if you look at Meta right here, they don't have a frontier model, but they have basically everything else. I know they're working on custom silicon. The only thing that they're not doing right now is serving a diversified set of models, which means serving your competition's models. And last, Rivian, the automaker. and I actually have a Rivian. I love it. Has announced a bunch of updates on their autonomy day. First, it seems they're developing their own custom chips. They are going the Tesla route and they are verticalizing as much as they can. They are introducing their third generation compute platform. Now, one of the weaker points of Rivan right now is the fact that they don't have great self-driving, but hopefully that gets fixed soon. It seems like it's coming in 2026. And they also announced that their in-house neural network engine will run directly on the chip that they're building. And they're also adding lidar, which is super interesting. Tesla and Elon Musk explicitly moved away from LAR, saying it was overly expensive, overly complicated. And because humans can drive with just their eyes, AI should be able to do the same thing with just cameras. But now they have a multimotal sensor system coming, including cameras and radars, providing more information about the surroundings. And if we look at Whimo and how well they're doing, they use LAR. So maybe there is something to that. So all of this is very cool. I can't wait to get my hands on it. I can't wait to have better self-driving in my car. So keep up the good work, Rivian. So that's it for today. If you enjoyed this video, please consider giving a like and subscribe. and I'll see you in the next",
        "start": 790.24,
        "duration": 1782.1570000000015
    }
]