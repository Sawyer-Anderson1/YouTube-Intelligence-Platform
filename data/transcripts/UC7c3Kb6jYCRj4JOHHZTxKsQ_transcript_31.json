[
    {
        "text": " This video is brought to you by Vulture. More on them later. We now have a humanoid robot built for home use available for pre-sale right now. This is 1X's robot, Neo, and it is really the first mass market pre-orderable humanoid robot that we've seen. And their launch video went absolutely viral. Almost 30 million views in less than 24 hours. And yes, I already pre-ordered it. It's going to be available in early 2026. That is right around the corner. Neo is offered at a $20,000 purchase price or $4.99 a month. Now, let's talk about that purchase price. Obviously, $20,000 is a lot of money, but relative to what it's actually going to be able to do, it's quite affordable. There are only three cars in the United States sold for under $20,000. So, this is really on par with the cheapest cars to buy in America. It weighs 66 pounds and can lift, listen to this, 150 lb. It has 22\u00b0 of hand movement and is extremely quiet coming in at about 22 dB. This thing hopefully will be able to do things like fold your laundry, do your dishes, organizing, tidying up, but not everybody is convinced it is going to be really great. Some initial reviews of demos that were given say that it's still teaoperated for the vast majority of use cases. And here's the thing, when it launches in 2026, if it's not able to accomplish something, somebody will be there ready to teleoperate it at any moment to complete the task. That's kind of nuts to think about. Imagine how much infrastructure, how many humans they're going to have to hire to make that happen. Plus, the whole promise of humanoid robots is to be able to be autonomous and run 24 hours a day. Congrats to 1X. I am very excited for the humanoid robot revolution. Of course, as soon as I get mine, I'm going to test it out. I'm going to make a video about it and I will share my thoughts. And next, Extropic has released new information about their thermodynamic computing platform, a completely new way to do compute. If you're on Twitter, you've probably heard of based Bef Jesus. He was an anonymous account that was really popular, started the effective acceleration movement, and then it was revealed that he is actually one of the founders of Extropic. And he's been talking about what Extropic does. And to be honest, it is very difficult to understand, but I'm going to try to break it down for you. Their pitch is that current computing paradigms require far too much energy. They are completely energy inefficient. And they have been able to unlock incredible efficiencies not seen anywhere else. And they have introduced the first scalable probabilistic hardware, inventing both new hardware and new software to run on it. It is called the thermodynamic sampling unit TSU. The TSU is very different from a",
        "start": 0.16,
        "duration": 383.4409999999999
    },
    {
        "text": "energy inefficient. And they have been able to unlock incredible efficiencies not seen anywhere else. And they have introduced the first scalable probabilistic hardware, inventing both new hardware and new software to run on it. It is called the thermodynamic sampling unit TSU. The TSU is very different from a executing deterministic commands, it samples from probability distributions. If that sounds confusing, that's because it definitely is. But you already have been working with non-deterministic systems. If you've ever used AI before, that is exactly how AI works. it comes up with a probability of what the next word in a sequence of words is. And so they're taking that approach and saying, well, why can't we do that with hardware? And they are claiming that the TSU can be up to 10,000 times more efficient than traditional CPUs and GPUs. Now, it's still very early. These are just prototypes and simulations, but it's exciting. Anybody who's working on something cutting edge, completely novel, you have my love. So, I really hope they continue and have success with this new computing paradigm. Next, we have a new open-source model, of course, from China, that has broken previous intelligence records. According to artificial analysis, Miniaax's M2 achieves a new all-time high intelligence score for an open weights model and offers impressive efficiency with only 10 billion active parameters out of a 200 billion total. It is very efficient to serve. And look at this. Miniax M2 coming in at 61 on the artificial analysis intelligence index. Right below cloud 4.5 Sonnet, right above Grock 4 fast, and of course leading the pack GPT5. But it's kind of crazy to see that it is better than Gemini 2.5 Pro. This is an open- source open weights model that you can download. You could potentially even run it locally. I'll drop a link below to this benchmark. And if you want to run models like Miniax and other open- source open weights models, you should do so on the sponsor of this video, Vulture. Vulture is the world's largest independent cloud provider, and they've been a fantastic partner to us. So, I'm really excited to tell you about them again today. So, if you need to provision GPUs, whether you're just tinkering on your own AI project or you're scaling up to production, Vulture is the place to go. They offer the latest AMD and Nvidia GPUs spanning 32 locations across six continents, so you're going to get the lowest latency. They also offer industry-leading price to performance with serious accessibility and reliability. So, with Vulture's global, fully composable cloud infrastructure, you move your applications closer to your users and frees you from vendor lockin, which you know I've talked about quite a bit on this channel. They also have Vulture Kubernetes Engine which allows you to scale beyond just a single container. So if you're tired of waiting in line for other GPU providers, check out Vulture",
        "start": 191.28,
        "duration": 710.3199999999998
    },
    {
        "text": "users and frees you from vendor lockin, which you know I've talked about quite a bit on this channel. They also have Vulture Kubernetes Engine which allows you to scale beyond just a single container. So if you're tired of waiting in line for other GPU providers, check out Vulture in credits for your first 30 days when you visit getvulture.com/burerman. And remember to use code bur300. Thanks again to Vulture. Back to the video. All right, next. In news that I did not see coming, Nvidia took a massive stake in Nokia. You remember Nokia? They used to make phones before the iPhone came out. And to be honest, I don't really even know what they do anymore. But those phones were iconic. They were absolute beasts. They were bricks. You can drop them from a skyscraper and they would still work afterwards. And according to CNBC, Nvidia takes a billion-dollar stake in Nokia, sending the 5G equipment maker shares up 22%. And listen to this, the two companies also struck a strategic partnership to work together to develop next generation 6G cellular technology and the 5G and 6G software will run on Nvidia's chips. Nvidia maybe doesn't know what to do with all of that cash. They are buying huge stakes in different companies like OpenAI and then OpenAI takes that cash and buys chips from Nvidia. It just seems like either the ultimate 4D chess move or potentially a Ponzi scheme. Either way, much smarter people than myself are executing these financial instruments. So, I guess we'll see. Next, we got another open- source model. This time from a US company, IBM. Granite 4.0 Nano is now out. According to their hugging face post, today we are excited to share Granite 4.0 Nano, our smallest models yet. Released as part of IBM's Granite 4.0 model family, which I've covered in the past, they are fantastic for enterprise use cases. Designed for the edge and ondevice applications, these models demonstrate excellent performance for their size and represent IBM's continued commitment to develop powerful, useful models that don't require hundreds of billions of parameters to get the job done. So we have granite 401b that is a 1.5 billion parameter model dense LLM featuring a hybrid SSM based architecture. We have the 350 million parameter version and then we also have a transformer version of the 1 billion and 350 million parameter models. Now here it is on general performance verse model size. As we can see, it is very performant and very small. Compared to other models of its size, it is excellent. So, congrats to IBM on a fantastic release. All right, next. Apparently, Elon Musk said something on an earnings call that he's kind of hinted at in the past that I find fascinating as Tesla rolls out these incredible vehicles with essentially supercomputers in them. Remember, they need high-end hardware to be able to do the autonomous driving. Because they",
        "start": 358.0,
        "duration": 1035.201
    },
    {
        "text": "next. Apparently, Elon Musk said something on an earnings call that he's kind of hinted at in the past that I find fascinating as Tesla rolls out these incredible vehicles with essentially supercomputers in them. Remember, they need high-end hardware to be able to do the autonomous driving. Because they idle the majority of the time. It's only really being used when you are driving the car or when it's driving you. But Elon Musk said, \"Hey, hey, what do we do with all of that idle hardware?\" Well, we should be using it. And he thinks as the number of Teslas scales up in the market, they could actually have a supercomputer cluster by leveraging the compute in these cars while they're idle. So his words, actually one of the things I thought if we've got all these cars that are maybe bored, we could actually have a giant distributed inference fleet. So he actually gives some calculations at some point if we've got tens of millions of cars or maybe at some point 100 million cars in the fleet and each of them had a kilowatt of inference capability that is 100 gawatts of inference distributed with power and cooling taken. So cool to think about and yeah we should be using this hardware. We are building the hardware this incredible compute infrastructure and it's just sitting idle. So why not use it? Imagine you buy a Tesla, it drives you around. It goes and drives potentially other people around earning you money. And when it's idle, when it's charging, or whenever it's just not being used, it could be powering AI inference. And you could be getting paid for that as well. It's super interesting to think about. Next, Cursor just came out with Cursor 2.0, a major update. Let me show you all the changes. So, first, introducing Composer. Composer is a frontier model that is four times faster than similarly intelligent models. It is built for low latency agentic coding in cursor. Now I have been talking about this for a while. AI speed is incredibly important completely overlooked by most people. They will choose quality over speed 10 times out of 10. But for me speed is just as important as quality. Next they have built in a multi- aent interface. So the first thing you'll notice when you open cursor is our new interface. It's more focused and designed from the ground up to be centered around agents rather than files. We are moving away and abstracting away from actually writing and reading code. This is just another step in that direction. With vibe coding plus with all of these UI changes inside the IDE, it seems like their vision for coding is really not coding at all. It is working with an agent to build what you want. But here is what happens when we have agents that can code for us. As we use agents more for coding, we've",
        "start": 524.72,
        "duration": 1335.5219999999997
    },
    {
        "text": "changes inside the IDE, it seems like their vision for coding is really not coding at all. It is working with an agent to build what you want. But here is what happens when we have agents that can code for us. As we use agents more for coding, we've Reviewing code and testing the changes. With Cursor 2.0, we're also starting to solve both of these. We've made it much easier to quickly review the changes an agent has made and dive deeper into the code when you need to. We've also built a native browser tool that allows Cursor to test its work and iterate until it has produced the correct final result. So, very cool. Check out Cursor link down below. And by the way, if you like these news videos, you should definitely subscribe to our newsletter, forwardfuture.ai. We put together the best newsletter of the latest AI news every single day, delivered to your inbox absolutely free. Check it out. forward future.ai. Next, a company called Substrate, a US-based startup, has announced something really cool. Let me tell you about it. Substrate is building a next generation foundry to return America to dominance in semiconductor production. To achieve this, we will use our technology, a new form of advanced X-ray lithography. And remember, lithography is a main part of the process for creating silicon to power them. America invented semiconductors. We will lead again. So, what we're seeing here are two examples. And they say that their technology enables features printed at the 2nanmter semiconductor node and below which is insane. 2nmter is essentially kind of beyond frontier in terms of silicon creation. So definitely excited to bring semiconductor manufacturing back to the US for many reasons. And because Microsoft and Open AAI seem to be, as I said earlier in the video, heading towards this collision course of competition, of course, Microsoft Edge had to release their agentic capabilities. So Microsoft Edge, which is Microsoft's web browser, it's time to question your browser. Meet co-pilot mode in Edge, turning your browser into a dynamic and intelligent companion with the latest AI innovations available on Windows and Mac. Now remember, Atlas, Chachib Atlas, the browser from OpenAI is only currently available on Mac. So again, it's all these little subliminal digs at each other, it seems. Or maybe it isn't, but I'm going to read between the lines here. All right. And next, Amazon has made massive layoffs. 14,000 people. These are all corporate jobs, not factory jobs. And it is a mass layoff aimed at readying the company for wide adoption of AI technology. Now, I have so many thoughts about this. Do we really think that they're laying off 14,000 people because of AI? I know a lot of you probably do. I personally don't. This is kind of the natural eb and flow of company growth. They probably overhired and need to start trimming down. And of course, AI is",
        "start": 677.92,
        "duration": 1659.7629999999997
    },
    {
        "text": "Do we really think that they're laying off 14,000 people because of AI? I know a lot of you probably do. I personally don't. This is kind of the natural eb and flow of company growth. They probably overhired and need to start trimming down. And of course, AI is individual employee. But this is probably just more of a function of overhiring in the past and now correcting for it. Remember, for years, these companies were hiring so many people every single year and eventually they get a little bloated and they need to trim the fat. 14,000 is a lot of people though. And they said they're not actually done with these layoffs. We expect to continue hiring in key strategic areas while also finding additional places we can remove layers, increase ownership, and realize efficiency gains. This is kind of again just the normal eb and flow of a company's employee growth. Most employees will be given 90 days to look for new roles internally while people that can't get new jobs at Amazon will be given severance pay and additional benefits. So definitely I feel for the people who are not going to be able to find a job internally at Amazon and a lot of people are going to have to reinvent themselves in this new era of artificial intelligence. But of course if you're watching this channel you're already ahead of the curve. But Andy Jasse, the CEO, did specifically say artificial intelligence was allowing them to reach efficiency gains and that is the reason that people are getting laid off. Listen to this. In June, Jasse said in a separate blog post to employees that efficiency gains from artificial intelligence would allow the company to eventually have a reduced human workforce. As we roll out more generative AI and agents, it should change the way our work is done. We will need fewer people doing some of the jobs that are being done today and more people doing other types of jobs. And this article, as well as most people kind of skip over that second half, which is we have new jobs that are coming that we're going to need. I interviewed Matt Garmin, the AWS CEO, and asked him specifically about engineers and if AI is replacing engineers or augmenting them. Check out this clip. Do you recommend engineering as a career to somebody who is entering college in college? I think I know the answer to that. Yeah. You know, sometimes a lot of the skills that I think there that that should be emphasized is how do you think for yourself? You know, how how do you develop critical reasoning of solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing? Because with the pace that technology that's advancing right now, , if you spend all of your time",
        "start": 841.839,
        "duration": 1930.801999999999
    },
    {
        "text": "how do you develop critical reasoning of solving problems? How do you develop creativity? How do you develop a learning mindset that you're going to go learn to do the next thing? Because with the pace that technology that's advancing right now, , if you spend all of your time like, \"Okay, that's the thing I'm going to be expert at for the next 30 years.\" The most thing I can promise you is that's probably not the thing you're going to be like, that's not going to be valuable 30 years from now. But if you learn how to learn or you learn how to think, I think that's actually where schools can be incredibly valuable. I think engineering is really great not at teaching you the the the blocking and tackling of the specific thing, but really kind of how do you think and how do you decompose problems and I think that's where kids coming out of school if they really focus on that then you're going to be in great shape. And a big thank you to Vulture once again for sponsoring this video. Get your inference, get your finetuning done all with Vulture. Check them out link down below. They've been a fantastic partner. So click that link, let them know I sent you. So that's it for today. If you enjoyed this video, please consider giving a like and subscribe.",
        "start": 979.519,
        "duration": 2020.161999999999
    }
]