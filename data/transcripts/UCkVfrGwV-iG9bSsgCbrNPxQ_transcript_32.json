[
    {
        "text": " So, an exgoogler is claiming that they've achieved AGI. No, this isn't clickbait and we need to talk about it. So, this is Jad Tafari. Now, this guy is a prominent AI expert and he's the CEO and co-founder of a company called Integral AI. Now, what's crazy about this is that this guy actually pioneered work at Google's early generative AI teams before leaving to build his own company focused on artificial general intelligence. Now, he's actually advocating for a new era of freedom based AI, pushing for efficient world modeling systems and recently suggested that traditional PhDs might become obsolete. Now, that went pretty viral. Most you guys didn't know this, but of course, this is what you're here for. So, you can see here, he tweeted this. This week, Integral AI unveiled the world's first AGI capable model, a breakthrough enabling agents to autonomously learn new skills safely, efficiently, and reliably, a foundation model for embodied super intelligence. Now, what I find most interesting about this is that as of recording, you can see the tweet only has 565 views, only five retweets, and six likes. Now, maybe it's just because nobody knows, or maybe there's something up with this claim. usually claims of AGI, I would have expected them to get a little bit more attention, especially from an exgoogler, but we're going to dive into those claims and exactly what they're talking about. So, when you go onto the link that they've posted, you can see that they say the first AGI capable model. So, they say that we are thrilled to unveil the first AI capable model, a breakthrough in machine intelligence that autonomously learns new skills safely, efficiently, and reliably. And this milestone addresses the inherent limitations of current AI systems and establishes a scalable framework for achieving true generality and super intelligence. So at first what they do is they give us this rigorous definition of AGI and they say that the term AGI is often overused without precision. To bring clarity we define AGI as a system that fulfills three core criteria. Autonomous skill learning. The model must independently teach itself new skills in novel domains without relying on pre-existing data set or human intervention, which I think is a really important one. A lot of the models that we see now, they're probably overfit on a lot of the benchmarks. Then safe and reliable mastery. It says it must learn without side effects or catastrophic failures. For example, a kitchen robot learning to cook must not cause a fire during training. And of course, energy efficiency. The total cost of learning must be comparable to or less than of a human mastering the same skill. I think, you know, a lot of these points are really important because number one, autonomous skill learning is something that humans do naturally. You're able to, you know, really learn on the fly a lot better than most AI systems. And of",
        "start": 0.08,
        "duration": 339.36
    },
    {
        "text": "of a human mastering the same skill. I think, you know, a lot of these points are really important because number one, autonomous skill learning is something that humans do naturally. You're able to, you know, really learn on the fly a lot better than most AI systems. And of and master things pretty easily, provided, of course, they're not some super complex tasks. And I think, you know, one of the key key things here is energy efficiency. human brains are remarkably energy efficient like just magnitudes so much more energy efficient than currents. I don't even need to get into the details of that. Now they talk about how you know and I will get into the of course claims but the reason I'm showing you guys all of this data is to show you guys what is actually on the page and what they were talking about. So they talk about the fact that, you know, I'm going to show you guys an interview clip later, but they talk about the fact that like AI systems currently function as black boxes mapping inputs to output without explicit abstractions or a coherent understanding of the underlying world. These systems conflate memorization and generalization leading to inefficiency, which is lacking structured abstractions. Models depend on brute force optimization, consuming vast computational resources while remaining vulnerable to local minima. And the brittleleness is where the absence of interpretable representations makes them fragile and errorprone in novel scenarios. Very very true. Both of these things are pretty key when it comes to standard LLMs. Now they basically say that because of this and what they're looking at this is their new approach. So they are doing you know I guess you could say a paradigm shift. So it says abs are a paradigm shift designed to create explicit hierarchical abstractions that mirror the human neoortex. So there are three stages of how they've built this thing. And it says number one is multimodal and embodied. Simulators integrate data from diverse modalities. So vision, language, audio, and physical sensors, producing unified world models that generalize across domains. Then you've got hierarchical abstractions. By compressing and structuring sensory data recursively, simulators build layered representations of reality, enabling highle reasoning and predictions. Then of course scalable growth. Unlike static systems, universal simulators grow dynamically through lifelong learning and retaining and refining knowledge without catastrophic forgetting and gradual expansion, increasing the parameter size, context length, modality, and coverage as needed. Now, of course, one of the things that you wanted to see was the demonstration. So, this I know the screen is black right now, but I'm about to play to you one of their first 3D AGI demonstrations. They state that the models are trained to navigate and solve problems in small 2D and 3D environments developing skills like memory and spatial reasoning. Now I am a little bit skeptical of this claim which I will, you know, start to talk",
        "start": 170.0,
        "duration": 617.7599999999998
    },
    {
        "text": "one of their first 3D AGI demonstrations. They state that the models are trained to navigate and solve problems in small 2D and 3D environments developing skills like memory and spatial reasoning. Now I am a little bit skeptical of this claim which I will, you know, start to talk just, you know, show this to you guys with absolutely no judgment. Did I leave my mug in the bedroom? What color is the living room wall? I want to hang a painting. How wide is my bedroom wall? Are we out of toilet? How many cars can I in the other bedroom? When our agent steps into a new space, it doesn't know what's ahead. No maps or prior instructions, only the goal of understanding its world. Step by step, it begins to explore, gathering fragments of information and forming a mental picture. Our model uses future prediction to guide its exploration. We provide questions that define its objectives. Did I leave my laptop somewhere in the house? Yes, it's on the table by the door. Is there enough seating for all of the guests? There is one couch in the room. By learning how to explore, it doesn't just observe. It predicts outcomes and weighs possibilities using confidence levels to ensure reliable results. To plan its next move, the agent samples possible futures and evaluates them internally. It scores each prediction based on how well it might solve the task before choosing the most promising path. As it moves, the multimodal agent keeps evaluating what it knows. Through active learning, it refineses its perception and movement. The more it explores, the better it plans. And so then here is where they talk about efficient planning. This is their second demonstration. You may not notice it, but your eyes move in quick, tiny jumps called secades. Cicades move your focus from one area of importance to another. Between each jump, your brain constructs what's unseen. So the eyes move to where it matters the most to perceive. You see the world in fragments and your brain fills in the rest. Our AI learns to do something similar. Like a human eye, the AI seeks out the most important patches to uncover. It starts with the eyes and the corners of the mouth, unpatching the focus points to determine if the person is smiling. The model unmasks patches where it needs to be. If the question is about background color, it begins from the outer edges instead, focusing on what's relevant. Accuracy improves. Confidence rises with fewer glimpses. Like us, it builds a picture one patch at a time. Their next demonstration is something that they call the Shooan experiment where they aim to demonstrate how AI can achieve efficiency in problem solving significantly faster than humans. Just as it might take someone around 10 years to be a grandmaster in chess, AI can reach professional level in a fraction of the time by leveraging computational",
        "start": 310.72,
        "duration": 933.9199999999997
    },
    {
        "text": "call the Shooan experiment where they aim to demonstrate how AI can achieve efficiency in problem solving significantly faster than humans. Just as it might take someone around 10 years to be a grandmaster in chess, AI can reach professional level in a fraction of the time by leveraging computational look. This is so a game of movement and planning. The real challenge is finding the most efficient way to reach the goal. At every step, it can move in any direction, push blocks, or go around them. So, how does it decide? Instead of guessing, the AI asks itself, \"How far am I from solving this puzzle?\" It looks at the puzzle and estimates [music] the number of steps it would take to finish. The fewer the steps, the more promising the path. When considering a path, if there are too many remaining steps, the AI holds off [music] and searches for better options. If it finds one with fewer steps, it gives it higher priority. The AI isn't just trying everything at random. It's thinking ahead, checking each possibility, and choosing the most promising moves first. This first puzzle shows the AI still learning, [music] testing out paths, including some inefficient ones. But with each new puzzle, it gets better. You'll see fewer wrong turns and smarter decisions as the AI continues to learn over [music] time. What initially starts as system two planning gets internalized as system one intuitive generation. a system that learns not just what to do, [music] but why one path is better than another and how to choose it with full confidence. Now, what's interesting is that I've actually done a little bit of research and I found out that, you know, Deep Mind actually did something which is pretty similar. So, Deep Mind's 2017 work, imagination and augmented agents, their imaginative agents solved 85% of circan levels compared to 60% for a standard Model 3 agent. And this work was essentially peer-reviewed. This was done in 2017 and it was you know published at new IPS 2017 with code and methodology available. And the reason I'm a little bit skeptical of this entire AGI claim is because there isn't anything to peer review. There's no code. There's no methodology people. You know they're claiming that their model master took from a blank state learning the rules and professional strategy simply by interacting with the simulation. But the critical thing is that we don't know how these demos are conducted. There's no independent verification. Usually when there are, you know, companies that claim things like this, usually external companies can verify them through API access or via different benchmarks like the ARC AGI benchmark. You'd have the company test it through the API and you would see relatively to other models where it scores. And another thing that's super interesting as well is that they have a self-defined success criteria. The problem is is that this company invented",
        "start": 469.52,
        "duration": 1219.6789999999994
    },
    {
        "text": "benchmarks like the ARC AGI benchmark. You'd have the company test it through the API and you would see relatively to other models where it scores. And another thing that's super interesting as well is that they have a self-defined success criteria. The problem is is that this company invented superior. It's like organizing a race and then saying, \"Okay, these are the rules.\" and then saying that you're the winner without having anyone else run. Now, all of these things lead me to a little bit more skeptical. I'm not saying they didn't achieve AGI. I'm just saying where's the results that everyone in the AI community is usually expecting and where's the large scale funding that usually AI companies have. Maybe these guys are rich enough to the point where they don't need any funding, but still so many things just aren't adding up, including the demos. They just seem a bit low quality for a company that, you know, achieved AGI. Now, I do want to show you guys the rest of this interview clip here because it was super interesting where they do actually talk about the architectures needed for AGI. And interestingly enough, I actually don't believe they're wrong about the general architecture for AJI. Right now, there's just no demos. So, unless they release something super interesting, it's going to be really hard to believe that they actually achieved this. Now, of course, once again, like I've said, they're ex Googlers. Well, this guy's an ex Googler. So, I'm not doubting the capabilities. It's just interesting the way the information is presented. so we're dividing the breakthrough into three subcomponents. One is an architectural breakthrough to move from prediction only models to abstraction and prediction models. So models that both explicitly abstract the world and then predict it as opposed to purely predicted. And we can talk about why that's important and dig into why there are actually many scientists trying to do that as well. but we think we have the right approach to do that. That's an architectural breakthrough. And then there's a learning method breakthrough which is going beyond current methods which can be classified as limitation learning or reinforcement learning. So the new learning paradigm is interactive learning that enables both efficient planning to surgically take action and that action could be a physical action or could be a question and then feeding it back to the model and applying a breakthrough in continual learning so the model can improve itself. So we've reached interactive learning allows model to self-improve. And the third breakthrough is a breakthrough in alignment. And this is basically how to make sure these models operate safely in the world. So when they are doing these surgical actions, they don't cause unintended consequences and make sure that there there's safety as they scale. Wow, this is mindboggling. So to be precise what we will reveal is what we call an AGI capable model",
        "start": 615.839,
        "duration": 1513.2009999999998
    },
    {
        "text": "these models operate safely in the world. So when they are doing these surgical actions, they don't cause unintended consequences and make sure that there there's safety as they scale. Wow, this is mindboggling. So to be precise what we will reveal is what we call an AGI capable model model and its learning method. It's not the end system. The end system is when you scale that out through interaction with the environment through experience. And we believe that this is the right model to scale to super intelligence. With that being said, let me know what you guys think and I'll see you in the next",
        "start": 763.6,
        "duration": 1543.4399999999998
    }
]