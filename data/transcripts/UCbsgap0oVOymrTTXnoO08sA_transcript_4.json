[
    {
        "text": " New Chinese AI agent Codebrain 1 just shocked the AI world by hitting approximately 72.9% on Terminal Bench 2.0, placing it second globally against major labs and doing it in its very first appearance on the leaderboard. Bite Danc's Cedance 2.0 just dropped and it's already generating multimodal storydriven AI video that finally feels intentional and consistent instead of random visual chaos. Alibaba's Quen Image 2.0 is pushing image generation forward with much stronger prompt following and deeper creative control. So much that people online are calling it the Chinese nano banana. And researchers introduced Fine R1, a new vision model that can distinguish ultra similar objects like different aircraft types using only a tiny amount of training data. All right, let's start with agents. Terminal Bench 2.0 zero has basically turned into the stress test everyone points to when they want to see whether an AI agent can actually do things inside a computer, not just talk about them. Even the strongest AI models still struggle there with many top systems hovering around the 60% success range. That's the level where small improvements start to look huge. OpenAI showed powerful results on this test using GPT 5.3 codeex combined with their simple codec agent setup. On this benchmark, they reported scores around 77.3% with another closely related setup showing 75.1%. That put them at the very top of the leaderboard. Anthropic also brought in Claude Opus 4.6 which showed strong performance too, landing around 65.4% on the same type of agent coding tasks. Then a Chinese startup called Feeling AI entered the picture with its own AI agent system, CodeBrain 1, and immediately took second place worldwide. Their reported scores were 72.9% with another configuration showing 70.3%. That places them right behind OpenAI and ahead of many other major labs and agent systems. So what's driving CodeBrain 1's performance? The focus is extremely practical. Increase the chance that the code actually runs correctly. Codebrain 1 is designed to stay locked onto the task instead of drowning in extra information. When it needs details, it pulls very specific code references and documentation tied directly to what it's working on rather than relying on broad guesses. That tighter focus makes its work loop more efficient, especially when things go wrong and it needs to adjust fast. When it works on a task, it uses something called the language server protocol. Basically, tools that understand code bases to pull only the exact pieces of code and documentation that are relevant. For example, if it's programming a gamebot, it doesn't guess how the bot works. It looks up the real function names, parameters, and examples like move to target or do action and uses those directly. Then comes error handling. When the AI writes code and something breaks, codebrain 1 reads detailed diagnostics, looks at examples of how similar code is written correctly, checks documentation for the exact parameter that failed, and then adjusts. That loop, write, test, fix,",
        "start": 1.839,
        "duration": 373.202
    },
    {
        "text": "or do action and uses those directly. Then comes error handling. When the AI writes code and something breaks, codebrain 1 reads detailed diagnostics, looks at examples of how similar code is written correctly, checks documentation for the exact parameter that failed, and then adjusts. That loop, write, test, fix, tested it on a smaller group of 47 Python tasks. And the system stayed stable while also using fewer tokens than some other agent setups, more than 15% less in certain comparisons. That means cheaper and faster runs, which matters a lot when companies use these systems at scale. On top of that, CodeBrain 1 is built to adjust plans while working. It's described like a brain that can change strategy based on what's happening. In an openw world game, a player might say, \"Build me a house or make a pickaxe.\" The AI agent breaks that into steps. Gather resources, clear space, craft tools, build structure, then actually executes the plan. In more tactical games, the system can build group memory. If a player keeps using the same path, enemy characters remember that and change their strategy later. They might guard that route more heavily, change their formation, or react differently when they see the player somewhere unexpected. The key idea is that the AI agent isn't just running a fixed script. It's adjusting behavior based on experience. Feeling AI also released something called Membrane earlier, which focuses on long-term memory for AI agents. It set new top scores on several memory benchmarks like locomo, longme eval, and personame v2 and even improved results by more than 300% on a very hard test called nomi bench level 3. Put together membrane and codebrain are like memory plus planning. Two big pieces needed for AI agents that can handle long complicated tasks. Now let's jump to something way more visible. AI video. AI video has basically been stuck in this weird phase where it looks mind-blowing for 2 seconds and then everything breaks. Creators end up hitting generate 20 times just to get one usable clip. That's where Bite Dance's new AI video model Cedance 2.0 comes in. And yeah, this one is being talked about as a real turning point. Cedence 2.0 basically understands how a scene should flow. It supports text, images, video, and even audio as inputs, which means you can guide it in multiple ways. Bite Dance even made access ridiculously cheap on their gyming platform. New users can unlock it for one UNO with auto renewal. That's clearly meant to flood the market with users fast. The big leap is that it behaves more like a digital director than a visual effect tool. It handles camera motion, push-ins, pans, tilts, tracking shots in a way that feels intentional. Scenes don't just move, they're framed like someone planned them. And most importantly, consistency. Characters keep their faces. Backgrounds stay stable. The scene doesn't randomly collapse into visual chaos. That's what allows longer storydriven clips instead",
        "start": 188.56,
        "duration": 695.5199999999996
    },
    {
        "text": "handles camera motion, push-ins, pans, tilts, tracking shots in a way that feels intentional. Scenes don't just move, they're framed like someone planned them. And most importantly, consistency. Characters keep their faces. Backgrounds stay stable. The scene doesn't randomly collapse into visual chaos. That's what allows longer storydriven clips instead we're on AI video, this is something every AI creator should know about. Higsfield is sponsoring today's video and they've built one of the fastest growing creator focused AI production platforms out there designed to feel more like a real studio workflow than just a prompt box. When you land on Higsfield, everything is structured around how videos actually get made. You start with an idea, shape your shots, and generate and refine inside one connected pipeline. Instead of bouncing between different AI tools for scripts, visuals, and edits, your whole project stays in one place from concept to export. That creator first setup is what really makes the platform stand out, especially if you're trying to produce more than just quick test clips. They already host many of the newest and strongest AI video models. And the latest addition is Cling 3.0, which is honestly one of the most impressive video models out right now. Inside Higsfield's workflow, your script, references, and audio direction all feed into one structure generation process, so scenes flow naturally, camera movement makes sense, and characters and objects stay consistent from shot to shot. In practice, you open a project, drop in a short script or idea, map out the scene, generate your video, tweak timing or visuals, and export something ready for social ads or storytelling. Definitely worth checking out if you're serious about AI film making. Link is in the description. All right, back to bite dance seedance 2. A well-known game industry founder Fang G from game science said this shift will cause content inflation. Translation: The cost of making regular videos drops closer and closer to just paying for compute. When making videos gets that cheap, the amount of content explodes. The first industry feeling this is e-commerce. Product videos don't need Hollywood. They just need to show the product clearly. Now merchants can generate those themselves instead of paying small studios or renting shooting spaces. That's a big hit to low-end video production businesses. In gaming, studios can now create worldbuilding trailers, concept previews, and promo material way faster and cheaper. For platforms like Tik Tok, this creates a title wave of content. Suddenly, the competition isn't who can make videos, it's who can filter them best. Their recommendation algorithms become the real battleground. Film and TV production also gets shaken. Traditionally, you shoot tons of footage. Then editors stitch the story together later. With tools like Cedance 2.0, the storytelling, camera logic, and editing start happening during generation. The workflow shifts toward describe the scene generate the scene. Editors move into roles more like creative directors, guiding the tool rather than assembling raw footage frame",
        "start": 351.759,
        "duration": 1011.2809999999996
    },
    {
        "text": "footage. Then editors stitch the story together later. With tools like Cedance 2.0, the storytelling, camera logic, and editing start happening during generation. The workflow shifts toward describe the scene generate the scene. Editors move into roles more like creative directors, guiding the tool rather than assembling raw footage frame issues start showing up. People are already using these AI video tools to create parodies in the style of Steven Chow's classic movies. They're copying his facial expressions, humor style, and iconic moments at very low cost. That caught the attention of his team. Chen Jen Yu, Steven Chow's agent, publicly questioned whether this counts as infringement, especially since these videos are spreading widely and some creators are earning money from them. At the same time, this shows something important about the AI era. When everyone has access to the same tools, recognizable IP becomes the real advantage. Famous characters and well-known styles grab attention instantly in a sea of AI generated content. And just to show how fast this is moving, it's only been 2 years since OpenAI launched Sora 1.0 in early 2024. Now, Cedance 2.0 is making 60-second audioddriven narrative videos with multimodal inputs. That speed is wild. Now, let's talk about AI images because there's been another long-running headache there. You write a long, detailed prompt, and the model ignores half of it. Alibaba's new model, Quen image 2.0, is built to fix exactly that. It can handle up to 1,000 tokens of instructions, follow complex scene descriptions, render Chinese text properly, edit existing images, and output up to 2K resolution. They tested it with a long prompt to create a five panel inkstyle comic based on Journey to the West. The characters stayed consistent across scenes and the environments like night travel and the Flaming Mountains were clearly different. Another test was a hamburger breakdown infographic where every ingredient and its position were described in detail. The result looked like a clean commercial diagram with realistic textures and properly stacked layers. They also generated a Shanghai city scene combining scroll painting style, 3D depth, miniature modeling, and night lighting. The composition stayed balanced instead of turning into visual clutter. Then there's a macro rice kingdom scene with tiny workers moving giant rice grains. The scale, relationships, and depth of field effects made it look like real macro photography. Beyond generation, Quen Image 2.0 is strong at editing. You can upload multiple photos and tell it to combine them, for example, swapping outfits or changing backgrounds, and it blends them naturally. It can even turn one selfie into a polished NG grid studio photo set. One more highlight, Chinese text rendering. The model can accurately reproduce complex classical text like preface to the Orchid Pavilion collection, which older models constantly messed up. In international tests, Quen image 2.0 Zero is described as ranking just behind Nano Banana Pro, putting it among the top image models globally. Finally, there's a development from PKing University that sounds niche,",
        "start": 512.56,
        "duration": 1344.4819999999997
    },
    {
        "text": "complex classical text like preface to the Orchid Pavilion collection, which older models constantly messed up. In international tests, Quen image 2.0 Zero is described as ranking just behind Nano Banana Pro, putting it among the top image models globally. Finally, there's a development from PKing University that sounds niche, tell apart very similar objects. Think about airplanes. Not just plane, but Boeing 77 versus 717 versus 727. There are over 500 types of fixedwing aircraft in civil databases. Recognizing these tiny differences is called fine grained recognition and it's hard even for AI. The university team built a model called fine R1. It doesn't just look at an image and guess. It goes step by step. Analyze visual details, list possible subcategories, compare them, then decide. It's basically structured visual reasoning. What's impressive is how little data it needs. With only four training images per category, Fine R1 beats well-known systems like Clip and Sigpip on several fine grain data sets. It can even name categories directly without being given options. They trained it using a method where the model sees an image, another image from the same subcategory, and a very similar image from a different subcategory. That forces it to learn the tiny details that separate almost identical objects. So across AI agents, AI video, AI image generation, and visual recognition, AI systems are getting more structured, more reliable, and more capable in general. All right, that's it for this one. Drop your thoughts in the comments. Thanks for watching, and I'll catch you in the next one.",
        "start": 680.48,
        "duration": 1512.1629999999998
    }
]