[
    {
        "text": " GLM 4.7 just dropped and it's a huge win for open source The model is now hitting numbers we used to associate with closed systems especially in agent coding and tool heavy workflows. And at the same time Manis rolled out a design update that finally fixes one of the most annoying problems in AI images making real edits without starting over China's been shipping some serious AI updates lately So, let's talk about it All right so GLM 4.7 is the latest model in Japu's GLM line and it's being pushed as a coding first agent-friendly system That framing matters This isn't a model optimized to look clever in a short chat exchange It's aimed at longer runs where a model has to plan execute call tools and stay consistent across many steps Honestly, that's where most models start to show cracks It's rarely about not knowing how to write the code The real problem is that over time the model loses the thread forgets earlier changes starts contradicting itself and the whole workflow slowly unravels What's interesting here is that GLM 4.7 isn't just a mild improvement over 4.6. The jumps show up across coding benchmarks agent benchmarks and reasoning with tools benchmarks which is exactly the combination you'd want if you're using coding agents like Claude Code, Klein, Rue Code, Kilo Code, or Trey style setups And yeah the model is being positioned as compatible with those workflows, including a think before act style that makes agent execution more stable So, let's talk about coding performance first because this is where the numbers are hard to ignore On SWEBench verified GLM 4.7 hits 73.8%. For an open model that's a serious milestone A model has to read a problem understand a code base it didn't train on locate the right area apply changes that match project structure and produce something that passes So, when a model performs well here it usually means it's tracking the task properly On Live Codebench version i GLM 4.7 comes in at 84.9%. And the reason people care about Live Codebench is that it's closer to the kind of coding you actually do in the wild It's not just write a function It's handling constraints edge cases and correct reasoning around what the code should do The multilingual angle is also important SWEBench multilingual moves to 66.7% which is a big jump compared to GLM 4.6. Now, terminal workflows. This is where models often feel the least reliable because terminal tasks demand correct sequencing and state awareness On terminal bench 2.0, GLM 4.7 reaches 41% up massively from the previous version There's also a hard terminal benchmark where GLM 4.7 sits in the lowers and the exact number matters less than the fact that the jump is large and consistent across the terminal suite That usually means fewer broken command chains and better ability to recover when something doesn't work Here's the thing When people say this model is good for",
        "start": 2.56,
        "duration": 368.4009999999999
    },
    {
        "text": "in the lowers and the exact number matters less than the fact that the jump is large and consistent across the terminal suite That usually means fewer broken command chains and better ability to recover when something doesn't work Here's the thing When people say this model is good for question nicely They mean it can handle multi-step work without turning into a mess Terminal tasks are basically a stress test for that The model has to decide what to do next run the right command interpret the output and then adapt That's very different from responding to a prompt GLM4.7 improving here points to better control and better internal planning Reasoning is another area where the model moved forward especially when tools are involved On humanity's last exam with tools enabled GLM 4.7 hits 42.8%. Without tools it's much lower which is normal The key point is that with tools the model makes a major jump compared to the previous generation That tells you something important about how it's meant to operate It's designed to reason with external capability like browsing context management and interactive tool calls rather than pretending everything is already in the model weights You also see gains in other reasoning heavy benchmarks including MMLURO, GPQA Diamond, and a stack of math and competition style evaluations The exact rankings vary depending on the benchmark but the pattern is consistent The model is more capable and the biggest leaps show up when you evaluate it as part of a tool using system instead of a standalone chat model Now, the how matters not just the scores GLM 4.7 introduces three thinking modes that are clearly designed for agent stability Interled thinking means the model reasons before every response or tool call Preserved thinking means the reasoning can persist across multiple turns rather than being regenerated every time Turnle level thinking control means you can dial it down for simple tasks and dial it up for complex ones Honestly, preserved thinking is the one that changes the feel of long sessions A lot of models even strong ones start to lose coherence over time because their reasoning resets They receive decisions from scratch and tiny differences accumulated Then you get drift Preserved thinking reduces that drift because the model can carry forward its internal reasoning state across turns That tends to make long horizon tasks less fragile and it also saves cost because you're not paying for the model to rethink the same plan over and over And this is where you start to see why GLM 4.7 is being talked about as a serious coding agent backbend model It's not just that it can code it's that it's more stable when it's running inside an agent framework that breaks tasks into steps There's also a tool use angle that's pretty strong On browse comps which is a web task benchmark GLM 4.7 hits the lowers in the base setting and then jumps to 67.5",
        "start": 187.36,
        "duration": 688.3219999999997
    },
    {
        "text": "code it's that it's more stable when it's running inside an agent framework that breaks tasks into steps There's also a tool use angle that's pretty strong On browse comps which is a web task benchmark GLM 4.7 hits the lowers in the base setting and then jumps to 67.5 a key detail because context management is exactly what web browsing and tool orchestration need On Tao Squared Bench, which focuses on interactive tool used it hits 87.4. That places it right up there with top systems in practical tool interaction Now, let's talk about real world developer signals because benchmarks are one thing but adoption and integration patterns tell you what people actually do with a model GLM 4.7 is integrated into Z.AI with an API platform that supports standard and streaming usage It's also available through Open Router, which matters for global access and quick integration into existing stacks and it's being advertised as compatible with common coding agents which means you can swap it into the same workflows people already used There's also mention of performance on specific hardware setups including very high token throughput for earlier GLM versions on specialized inference providers The point isn't the exact tokens per second figure The point is that the ecosystem around the model is clearly thinking about practical deployment not just research releases One more detail that gets overlooked is the model's improvements in fronted output and vibe coding Basically, UI generation quality The model is claimed to produce cleaner web pages better layout balance more coherent styling and better slide generation There's a specific metric around 16x9 slide layout accuracy improving dramatically which is the kind of boring sounding detail that actually matters if you've ever tried to generate slides and then fix them If the default output is closer to usable you spend less time fighting it Now, limitations Even with these gains a top proprietary model can still outperform GLM 4.7 on certain hard tasks especially in zero scenarios where you want a perfect solution instantly And for local deployment the full precision model is heavy You're talking about a deployment that requires serious hardware and even quantized variants still need a competent setup So, it's not a casual install and go for everyone But here's the more practical framing A lot of teams don't need the absolute best model on Earth. They need a model that's strong stable affordable and controllable Open weights matter for compliance customizations and long-term cost And on the cost side GLM 4.7 is positioned as significantly cheaper than premium proprietary alternatives with larger usage quotas that changes who can afford to run agent workflows at scale All right that's GLM 4.7. Now, let's switch to Manis because this is a completely different category of update but it hits a very real pain point that basically everyone doing AI visuals has run into So, Manis launched design view and the best way to understand it is",
        "start": 349.36,
        "duration": 1024.3199999999997
    },
    {
        "text": "scale All right that's GLM 4.7. Now, let's switch to Manis because this is a completely different category of update but it hits a very real pain point that basically everyone doing AI visuals has run into So, Manis launched design view and the best way to understand it is generation into an actual editable workflow instead of a lottery machine The typical AI image loop is painful You prompt You get an image that's close Then you notice one thing is wrong The lamp color is off The text is garbled The sofa shape is weird The logo placement is wrong You want to change one details but the only tool you have is regeneration So you prompt again and now the whole image changes Lighting shifts composition changes the vibe changes and you're stuck trying to recover the version you liked Design View attacks that problem directly You generate an image and then you edit it on a canvas using precise selection tools Manis calls it a mark tools Essentially, you highlight the region you want to change and you apply a local edit while the system tries to preserve the rest of the images characteristics That one idea sounds small but honestly it's the difference between AI as a generator and AI as a usable design tools The workflow Manis is pushing is generate first then refined not regenerate refined Now, Manis is powering this with Google's Nano Banana Pro for photorealistic interior design generation and related edits And Nano Banana Pro is generally positioned as a highfidelity image model the kind that can follow prompts closely and produce clean realistic outputs The key part is that Manis isn't just using it to create images It's using it to preserve consistency during edits That's hard because local edits tend to break coherence if the model doesn't handle context properly For example if you change an object color in a room a naive edit would also change lighting or reflections or textures in a way that feels disconnected from the rest of the scene Manis is trying to keep those elements aligned so the image doesn't look patched together That's what preserving original image characteristics really means in practice It's about maintaining the look of the original shot while still allowing targeted changes Another thing Manis highlights is text editing AI image generation is notoriously bad at embedded text It's one of the most common failure points Manis addresses this by allowing clean editable text overlays on the canvas which is a more practical approach than forcing the image model to perfectly render typography That's a smart design choice because in real workflows, you want editable text anyway Now, the second big part of the Manis update is slide editing specifically for slides generated in image generation mode using Nano Banana Pro. And this is a big deal because AI slide generation has had a nasty usability problem for a while A",
        "start": 519.279,
        "duration": 1334.3209999999995
    },
    {
        "text": "workflows, you want editable text anyway Now, the second big part of the Manis update is slide editing specifically for slides generated in image generation mode using Nano Banana Pro. And this is a big deal because AI slide generation has had a nasty usability problem for a while A They look nice but they're not editable Then you spot a typo or you want to adjust spacing and your only option is to regenerate the entire slide That's absurd in a real presentation workflow. Maya says those slides are now editable at the element level That means you can change text point and edit visual elements compare before and after and even run bulk edits across multiple selected areas Bulk edits matter a lot because presentations often require consistent changes across multiple slides like updating a product name swapping a color changing a tagline, or fixing formatting issues What's interesting here is that Manis is basically restoring the missing last mile control AI tools are good at generating drafts quickly The last mile is where people spend time making the output correct consistent and presentation ready Element level editing is exactly that It doesn't just make the tool nicer It makes it usable Manis also positions design view as more than just an image editor It's an interactive canvas where you can generate visuals refine them and keep everything in one workspace instead of bouncing between apply That's the core pitch Fewer exports fewer tool switches fewer prompt resets They also mention multivocal generation including images videos and 3D assets on the canvas though the clearest demonstrated value right now is photocell visuals and editing And the design update is available across web and mobile which is actually meaningful for teams A lot of tools claim mobile access but only for viewing Manis is pushing actual edits across devices which makes it easier to do quick revisions without being tied to one machine There's also a note about performance and speed Typical image generation is described as taking around 10 to 30 seconds with longer times for more complex research integrated layouts That's pretty normal for highquality image generation The main improvement here isn't raw speed it's iteration speed If you can fix a detail without regenerating you save minutes and sometimes hours across a project Manis also makes a clear statement about ownership and usage rights Basically, that users retain ownership of what they create and can use it for personal or commercial work For professional adoption clarity here matters Teams don't want vague licensing Now, how does Manis compare to other options people already know like Photoshop's Generative Fill, Canvas Magic Edit, or built-in AI tools and design platforms The main difference is workflow integration Photoshop is incredibly powerful but it's still Photoshop. It's a separate environment and it's not structured as an AI agent workflow. Canva is accessible and fast but access can depend on plan tier and the editing",
        "start": 676.64,
        "duration": 1666.801999999999
    },
    {
        "text": "Edit, or built-in AI tools and design platforms The main difference is workflow integration Photoshop is incredibly powerful but it's still Photoshop. It's a separate environment and it's not structured as an AI agent workflow. Canva is accessible and fast but access can depend on plan tier and the editing be an all-in-one flow where you generate edit and manage assets in the same agent-like environment that also handles research and context Whether Manis wins long term depends on how reliable those edits are at scale The promise is strong Local edits preserved layout editable text overlays and bulk changes for slides The real test is consistency across batches especially when you need brand alignment But even without promising perfection the direction is clear Manis is moving away from prompt roulette and toward an editor first loop All right that's everything for today If you enjoyed this breakdown hit like subscribe for more updates like this Thanks for watching and I'll catch you in the next one",
        "start": 844.32,
        "duration": 1755.5229999999992
    }
]