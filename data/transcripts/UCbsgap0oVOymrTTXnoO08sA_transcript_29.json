[
    {
        "text": " All right So, over the last few years we've all watched context windows get bigger and bigger 8,000 tokens turned into 32, then 100,000, then suddenly everyone's talking about million token models And on paper that sounds like the problem is basically solved Just stuff everything into the prompt and let the model figure it out Except that's not how it plays out in real used performance drops answers get fuzzy costs explode and at some point the model just kind of loses the plot That's where this new idea comes in And it's not a bigger model not a wider window not a clever compression trick It's a totally different way of thinking about what a language model should even see in the first place What MIT and later Prime Intellect are proposing with recursive language models or RLMs is a shift in mindset Instead of forcing the model to swallow a massive prompt all at once you treat that prompt like an external world the model can explore The model doesn't read everything It pokes around inspects pieces writes code to search through it and even calls smaller versions of itself to help That sounds abstract at first but once you break it down it's surprisingly intuitive Let's start with the core problem they're trying to solve Even the best frontier models today suffer from something researchers now openly call context rot As inputs get longer quality drops and the drop is faster when the task is more complex A simple search task like finding a specific phrase hidden somewhere in a huge document scales pretty well But tasks where the answer depends on many parts of the input or worse on relationships between many parts fall apart quickly This shows up very clearly in benchmarks like oblong and long pairs where models are asked to transform or compare large numbers of entries instead of just retrieving one fact In the MIT paper they visualize this with GPT5. As you increase input length from a few thousand tokens all the way up to hundreds of thousands GPT5's performance drops sharply especially on tasks with linear or quadratic complexity On long pairs which requires pairwise aggregation across the input GPT5 basically collapses F1 scores drop close to zero And this happens even before you hit the hard context limit So, the issue isn't just not enough tokens it's how the model processes them Instead of stuffing a huge prompt into the AI's brain all that text just sits outside the model like a giant document on a desk The AI doesn't read it all up front It looks at it only when it needs to The model gets a simple set of instructions on how to interact with that documents It can skim parts of it search for specific words pull out small sections take notes and even ask a smaller AI for help on just one music tiny piece So instead of drowning in information it moves step",
        "start": 2.639,
        "duration": 329.36
    },
    {
        "text": "simple set of instructions on how to interact with that documents It can skim parts of it search for specific words pull out small sections take notes and even ask a smaller AI for help on just one music tiny piece So instead of drowning in information it moves step can think of it like this The AI isn't memorizing the whole book anymore It's flipping pages highlighting lines and calling in an assistant to summarize a paragraph when needed Behind the scenes there's one main AI running the show That main AI is connected to a workspace where the full input lives It can poke around run quick searches break the big text into smaller chunks and hand those chunks to cheaper smaller AIs to process music Once it has everything it needs it puts the answer together and sends it back music to you It still feels like a normal chat You ask one question you get one answer What's powerful about this is that the AI stops thinking in music terms of how much can I fit in my memory and starts thinking in terms of how do I work through this information It's not about reading everything It's about navigating it And that changes everything when inputs get massive There's a benchmark where the AI gets up to 1,000 full documents at once That's millions of words No normal model can read all of that in one got It's not even close But with this setup the AI doesn't try to It just scans searches and zooms in on the parts that matter Everything else stays in the background untouched That's the real breakthrough here The size of the input stops being the main limit What matters instead is how smart the AI is at finding its way through information On that benchmark music the results are honestly hard to ignore When RLM is paired with GPT5, it reaches a little over 91% accuracy and the average cost per question comes in at just under $1. music To put that into perspective the old school approach where you force the model to read everything directly would cost somewhere between one and a half and nearly i per query And that's assuming the model could even handle that much data in the first place And the gap gets even more obvious on tougher tasks Take Code QA from LongBench V2. With GPT5 on its own accuracy is 24%. If you add a summarization agent on top that number jumps to 41.33%. which already looks like a solid improvement but once you switch to an RLM setup accuracy climbs to 62%. What's really interesting music is what happens when you strip things back even further There's an ablation where the model gets access to the ripple environment but no recursive sub calls at all And that version actually hits 66% accuracy That's higher than the full RLM in this case And that's a big",
        "start": 167.44,
        "duration": 608.4809999999999
    },
    {
        "text": "music is what happens when you strip things back even further There's an ablation where the model gets access to the ripple environment but no recursive sub calls at all And that version actually hits 66% accuracy That's higher than the full RLM in this case And that's a big context out of the model's head and into an external environment already makes a massive difference Even before recursion enters the picture the model just works better when it doesn't have to carry everything in memory at once Now look at long pairs the quadratic task This is where things get wild GPT5 by itself gets an F1 score of about 0.04. That's essentially useless Summarization agents hover near zero Kodak with retrieval gets to about 24.6. 67 the full RLM jumps to 58.00 00 and even the ripple only variant with no recursion hits around 43.93. For Quen 3 coders a massive open model the base scores stay below 0.1 F1, while the full RLM reaches 23.11. The ripple gives the model a place to push all that context so it's not overloaded The recursive sub calls give it a way to actually reason over that context music in manageable chunks The MIT paper also shows what these models actually do while they're working step by step And once you see it it makes a lot of sense First, the model just takes a quick look It glances at the beginning of the input to understand what kind of mess it's dealing with Is this a list a pile of documents logs code or something else entirely That first glance helps it decide what to do next After that it gets selective Instead of reading everything music it starts searching It looks for words patterns or lines that seem relevant and ignores the rest So the model is already cutting down the problem without actually loading everything into its head When things get more complicated it stops trying to handle the data all at once It breaks music the big input into smaller pieces like individual lines or documents Each piece gets handled separately sometimes by smaller helper models The main model stays in control collecting the useful bits and combining them into one answer Now when the final answer itself is very long RLMs use another simple trick They don't try to write the whole thing in one got They build it piece by piece saving parts as they go and then glue everything together at the end That's how they get around the usual output limits They're not talking nonstop They're assembling This is also why the word recursive actually matters here The model can go back ask music again refine something or check its own work using smaller focused calls Sometimes that helps catch mistakes that would normally happen when too much information gets mixed together Other times it just means extra work and higher cost And the paper doesn't hide",
        "start": 309.12,
        "duration": 901.2809999999997
    },
    {
        "text": "The model can go back ask music again refine something or check its own work using smaller focused calls Sometimes that helps catch mistakes that would normally happen when too much information gets mixed together Other times it just means extra work and higher cost And the paper doesn't hide Others wander around double-checking too much and getting expensive But the important part is that the model now has a way to work through information instead of being overwhelmed by it That brings us to cost and efficiency On average RLMs are surprisingly music competitive In many cases the median RLM run is cheaper than a single base model call that tries to handle everything directly But the variance is high Some runs are cheap and efficient Others wander around for a long time making many sub calls and get expensive The authors point out that all their implementations use synchronous blocking calls no parallelism no learned policies for when to stop There's a lot of lowhanging fruit here This is also where Prime Intellect comes in They took the MIT blueprint and turned it into a concrete system called RLMNV. The way they set it up is very intentional The main AI, the one making decisions only gets access to a simple workspace No web browsing no huge tool outputs no messy data flooding its memory All the heavy lifting things like web search or file access gets pushed to smaller helper models The main model stays focused on thinking not digging They also give it a way to send out many small tasks at once using something called LLM, batch So instead of doing everything one by one it can split work up and handle things faster music And there's a very strict rule at the end The model has to clearly write its final answer into a specific place and mark it as done No half-finish thoughts no wandering off That separation turns out to be really important Huge chunks of text never get dumped into the main model's memory They stay outside in the environment The main model only sees short summaries notes and intermediate results That keeps everything manageable and stops the system from choking on its own inputs Prime Intellect tested this setup across several very different scenarios One of them deep dive is all about web research with extremely long and noisy pages Another music math Python focuses on hard competition style math problems using a coding environment They also reused oblong directly and added verbatim copy which checks whether the system can reproduce complex data like JSON, CSV files or mixed code Exactly. Across all of these models like GPT 5 Mini and Prime Intellect's own Intellect 3 MOE became noticeably more reliable once they were wrapped in this RLM structure Something else really interesting shows up when you compare models Both GPT5 and Quen 3 coder get much better when used as RLMs, but they",
        "start": 457.84,
        "duration": 1196.561
    },
    {
        "text": "these models like GPT 5 Mini and Prime Intellect's own Intellect 3 MOE became noticeably more reliable once they were wrapped in this RLM structure Something else really interesting shows up when you compare models Both GPT5 and Quen 3 coder get much better when used as RLMs, but they Comp Plus, RLM with GPT5 almost solves the benchmark RLM with Quen 3 coder on the other hand struggles on about half of the tasks What's wild is that the system prompt is basically identical The only difference is one extra warning telling Quen 3 coder not to overuse helper calls That tiny change leads to very different behavior GPT5 tends to be cautious and selective Quen 3 coder is more aggressive and starts splitting things up line by line especially on oblong style tasks Same structure different instincts This points to something important RLMs are supposed to work with any model and in theory they do but in practice how well they perform depends a lot on how good the base model is at making judgment calls Right now these models haven't been trained specifically for that kind of decision-making. They're figuring it out on the fly The authors are pretty honest about the limits Current RLMs only go one level deep The helper calls are just normal language models not full RLMs themselves Everything runs sequentially not in parallel There's no reinforcement learning guiding when to split when to stop or how much checking is enough Sometimes the model overthinks, keeps verifying the same answer burns through budget and still ends up wrong But that's also where the upside is The paper argues that these RLM runs are basically a new kind of reasoning trace And reasoning traces can be trained If you combine this structure with reinforcement learning you could teach models how to explore huge inputs efficiently how deep to recourse and when it's time to stop Better models and better systems would stack on top of each other For a long time improvement meant training bigger models with more data and more computer RLMs add a new dimension at inference time The limit is no longer how much fits in the context window It's how well the model can navigate information that lives outside of it In a way this borrows ideas from classical computer science Out of core algorithms process data sets much larger than memory by carefully managing what gets loaded when RLMs are doing something similar for language models Small, fast working memory combined with symbolic access to a huge external store And the results are already hard to ignore handling 10 million music tokens solving tasks that completely break frontier models doing it at comparable or lower cost all without changing the underlying model architecture So, when people ask whether we'll get to agents that can handle massive code bases entire company knowledge graphs or months of logs without forgetting crucial details this is one of the most concrete answers",
        "start": 607.68,
        "duration": 1494.5590000000002
    },
    {
        "text": "models doing it at comparable or lower cost all without changing the underlying model architecture So, when people ask whether we'll get to agents that can handle massive code bases entire company knowledge graphs or months of logs without forgetting crucial details this is one of the most concrete answers of context windows even mattering Drop your thoughts in the comments And if this helped you understand what's really changing under the hood hit like subscribe for more deep dives like this and thanks for watching",
        "start": 758.959,
        "duration": 1518.8010000000002
    }
]