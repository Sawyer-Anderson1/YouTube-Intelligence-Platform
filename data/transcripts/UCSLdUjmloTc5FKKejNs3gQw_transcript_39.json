[
    {
        "text": " We all know LLMs require powerful graphics cards to run efficiently. But what if I told you that there is a model out there that you can actually run on your everyday computer and without sacrificing much of the accuracy? That model, my friends, is Microsoft's Bitnet. It's the first one bit large language model that performs almost on par with full precision models. In this video, we're going to unpack what BitNet is, why one bit per weight is such a big deal, and I'm actually going to run it on my M2 MacBook Pro, and see how it performs. It's going to be a lot of fun. So, let's dive into it. Bitnet is a new model architecture developed by Microsoft that stores each weight using just 1.58 bits on average instead of the usual 16 or 32 bits. You might be wondering how can a model actually work with so little information. Well, normally every weight in a neural network is a floating point number which means it needs at least 60 bits per memory. For example, for FP16, Bitnet takes those weights and quantizes them to just three possible values minus one, zero, and plus one. This is called a turnary quantization. And that's why they call it a onebit model. Well, technically it's 1.58 bits because you need this many bits to represent three possible states, but I guess a one bit model just sounds better. Okay, but why is this a big deal? Well, this might sound like a trivial compression trick, but the impact here is massive. This means we can have the following things. First of all, smaller size. You can cut model storage by up to 10 times compared to FP16. Second, lower memory bandwidth. less data needs to move between CPU and GPU and RAM, which is often the real bottleneck. Third thing, faster interference. Microsoft reports up to six times speedups on CPUs and huge energy savings, 55 to 82% lower power use. And fourth, better for edge devices. This makes running LLMs on laptops, desktops, or even phones a realistic option. And here's the best part. Accuracy barely drops. and it performs on par with much larger FP16 models when compared with popular benchmarks. The key innovation is that Bitnet doesn't just take an FP16 model and smash it into one bit weights after training. It's trained natively in one bit space from the very beginning. This lets the model train around the constraints of the turnary weights instead of losing precision after the fact. So, let's go ahead and try it on our own and see how well it actually performs. One thing to note here is if I go to the Microsoft's official repo and try to follow the instructions on how to run bit.cpp, you will probably encounter some issues that people have been complaining about. So, instead, the easiest way to do it is actually to go to their hugging face",
        "start": 0.16,
        "duration": 362.0
    },
    {
        "text": "note here is if I go to the Microsoft's official repo and try to follow the instructions on how to run bit.cpp, you will probably encounter some issues that people have been complaining about. So, instead, the easiest way to do it is actually to go to their hugging face it using their transformers library. I tried it and it works great on my MacBook Pro running only on a CPU. I even modified the code a bit to have a constant chat interface through the terminal and the response time was very fast considering it's running on a CPU. I was genuinely surprised how well this tiny model performs on my computer. I could even get it to write code. How well that code is, we'll see that in a second. Okay, so that's all cool, but if you don't want to download anything, you can just go ahead and try it out on their demo page, which I have to say is pretty neat. It even shows the generated tokens and the time taken to produce the response. So, let's use this demo interface to run some sample benchmarks to see if it's really as good as advertised. I'll choose some questions from the publicly available LLM logic test spreadsheet, which tracks how different models behave. I'll leave a link to it in the description if you're interested to check it out. I did similar tests in one of my previous videos testing Nvidia's Neotron. If you want to check out that video, just click the link over here. So, first let's try this question about colored boxes. This one got it correct with only 60 tokens generated. That's pretty cool. Let's go ahead and try this riddle. What has 13 hearts but no other organs? And yes, indeed, it is a deck of cards. That is the right answer. So good so far. Next is the four-legged llama question that many models seem to fail at. And let's try it on bitnet. And yes, unfortunately, Bitnet also fails to answer the llama question correctly. And lastly, let's try the more complex question about students and buses. And we see here that even this question Bitnet seemed to answer correctly. So on logic tests, I'd say it's within the average range of most language models, but it wouldn't be a real LLM test without trying the famous bouncing ball in a rotating hexagon challenge. So I increased the token limit on my local Bitnet model and prompted it to write the code for this simulation. And on the first try, it didn't work. It got some errors. So then I prompted to fix those. And on the second try, it still did not work. So I prompted it the third time to fix the remaining errors. And finally, we got something that actually compiled. But as you can clearly see, there's neither a ball, neither a hexagon. Neither of them are even spinning. And",
        "start": 180.8,
        "duration": 670.002
    },
    {
        "text": "And on the second try, it still did not work. So I prompted it the third time to fix the remaining errors. And finally, we got something that actually compiled. But as you can clearly see, there's neither a ball, neither a hexagon. Neither of them are even spinning. And because there just are none. Yeah. So Bitnet is clearly terrible at coding. So, please don't cancel your clawed code subscription just yet. So, if there's anything we've learned today is that one bit models are not good at coding. What they are good at, however, are logic tests, basic LLM functionality. So, I still believe BitNet is a powerful option if you want something super lightweight which you can run on low-end devices like a laptop, a phone, or maybe a robot. I don't know. Who knows? Maybe one day. So anyway, there you have it. That's BitNet. I would say the architecture behind Bitnet is super impressive and promising, but Bitnet itself is still not ready to compete with the big LLM giants. But it's super promising to see that companies like Microsoft are thinking about limitations and making their models smaller in size and more efficient. So I think we will see plenty of new exciting models in the near future. So what are your thoughts about BitNet? Have you tried it? Will you use it? Do you know of any interesting ideas what it could be used for? Let us know in the comments down below. And folks, as always, if you like these types of technical breakdowns, be sure to smash that like button underneath the video to let us know. And also, don't forget to subscribe to our channel. This has been Andress from Better Stack, and I will see you in the next videos. Heat. Heat. [Music]",
        "start": 337.6,
        "duration": 869.6229999999997
    }
]