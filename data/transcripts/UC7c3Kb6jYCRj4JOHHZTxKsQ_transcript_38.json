[
    {
        "text": " Anthropic just dropped Claude Sonnet 4.5 and it is not just an incremental change to what was already the best coding model on the planet. This is something different. This is a major step forward in coding ability. This model has been able to think for 30 plus hours independently, autonomously, and I'm going to break all of it down for you right now. And this video is brought to you by our fantastic partners at Vulture. More on them later in the video. First, here's the blog post. Claude Sonnet 4.5 is the best coding model in the world. First sentence right away. Let's get that out of the way. According to Anthropic, of course. But what really makes this model special is its ability to take a task and go think for a long period of time autonomously. That means this is really built for agents. And with this release, Anthropic is giving us a huge preview of what the future of software is going to look like. And they actually have a demo of what the future of operating systems is going to look like. And I'm going to show you that in a moment. Let's get right into the benchmarks first. So Claude Sonnet 4.5 is state-of-the-art on SWEBench verified evaluation. And here it is. Here's Opus 4.1, which was at 79.4. and a multiple percentage jump sonnet 4.5. And here's GPT5 Codeex and Gemini 2.5 Pro, which was my favorite coding model for a while. But this is almost 20 percentage points ahead on SweetBench verified. And let's look at some other benchmarks. Here it is on Terminal Bench, 50%, the best of all of these models. Here it is on aentic tool use. Some of the best scores again, computer use, by far the best score. high school math Amy 2025 with Python 100% just acing the test and you could see so on this model is just the best across the board. Now Michael Truel, CEO of Cursor has this to say. We're seeing state-of-the-art coding performance from Claude Sonnet 4.5 with significant improvements on longer horizon tasks. It reinforces why many developers using Cursor choose Claude for solving their most complex problems. Here's the chief product officer of GitHub. Claude Sonnet 4.5 amplifies GitHub Copilot's core strengths. Our initial eval show significant improvements in multi-step reasoning and code comprehension. So, let's spend a minute talking about long horizon tasks. That really seems to be the next frontier of AI evolution. Now, the research firm Meter put together this incredible blog post, this incredible website talking about the exponential increase in AI's ability to complete long horizon tasks. Now, as we're seeing these benchmarks get completely saturated, Amy 2025, MMLU, all of the benchmarks where all of the Frontier models are essentially acing them at this point, how are we really able to measure the incredible improvement we all know we're seeing in these models? Well, here it is. Look at",
        "start": 0.16,
        "duration": 365.918
    },
    {
        "text": "seeing these benchmarks get completely saturated, Amy 2025, MMLU, all of the benchmarks where all of the Frontier models are essentially acing them at this point, how are we really able to measure the incredible improvement we all know we're seeing in these models? Well, here it is. Look at release date. On the yaxis, this is task duration for humans. So, for example, here's find a fact on the web. Less than 10 minutes, it looks then less than an hour. Train classifier at about an hour 15, fix bugs in small Python libraries. And then all the way up here over 2 hours, exploit a buffer overflow bug. And look what we have here. GPT3 capable of task lengths of 9 seconds 3.5 36 seconds GPT4 five minutes and then all of a sudden look at this growth rate 01 was a major update this is when we really started seeing internal reasoning that is when the models would use chain of thought to think before outputting its response then that really set this curve in motion we have claude 3.7 Sonnet 03 Gro 4 GPT5 and now at 30 hours which would be way up here we have Claude sonnet 4.5 and here's the point the length of task AI can do is doubling every 7 months that is the new Moors law and if you don't remember Moore's law says that the number of transistors on a chip doubles every approximately 18 months And that's been the case for a while. And now we have this new scaling law of the amount of time an AI can run autonomously and successfully. So here we go. As you can see this curve 2x every 7 months. And honestly that actually might be increasing because now we're at 30 hours. And remember that 30 hours is not with any agentic framework around it. That is just the model itself. And according to this post, which was written just a few months ago, we should be at 30 hours, not until 2028. We are there now. And here they show it. Recently, the trend has accelerated. So, we're actually beating their initial predictions. Thanks to the sponsor of this video, Vulture. Check this out. Vulture is the world's largest independent cloud provider, and they've been a fantastic partner to us. So, I'm really excited to tell you about them again today. So, if you need to provision GPUs, whether you're just tinkering on your own AI project or you're scaling up to production, Vulture is the place to go. They offer the latest AMD and Nvidia GPUs, spanning 32 locations across six continents, so you're going to get the lowest latency. They also offer industry-leading price to performance with serious accessibility and reliability. So with Vulture's global fully composable cloud infrastructure, you move your applications closer to your users and frees you from vendor lock in which you know I've talked about quite a bit on",
        "start": 183.519,
        "duration": 709.5180000000005
    },
    {
        "text": "to get the lowest latency. They also offer industry-leading price to performance with serious accessibility and reliability. So with Vulture's global fully composable cloud infrastructure, you move your applications closer to your users and frees you from vendor lock in which you know I've talked about quite a bit on Kubernetes Engine which allows you to scale beyond just a single container. So if you're tired of waiting in line for other GPU providers, check out Vulture today. They're offering my viewers $300 in credits for your first 30 days when you visit getvulture.com/bman. And remember to use code Burman 300. Thanks again to Vulture. Back to the video. Being able to think for a long time is only one half of the equation. Just as importantly is task efficiency. If it thinks for 30 hours to figure out what 1 plus 1 equals, that's no good. What we really needed to do is be super task efficient and super token efficient. And somebody who's been talking a lot about that recently is Greg Camerit. So here he is talking about the ARK 3 prize. So cost performance is good, but interactive environments give us a new measure to grade action efficiency. So what I really am looking for is for the most efficient token usage. I wanted to accomplish the most in the shortest amount of time and then we expand that amount of time to do harder and harder tasks. And so Greg suggests the best measure of AI is intelligence per watt. And it's interesting because whenever you hear about a new data center, a new massive project like Stargate, you always hear about the total amount of electricity it's going to use. But the total amount of electricity isn't really the most important thing. It's how much intelligence is going to be generated per electricity given to it. And that is intelligence per watt. But that's kind of hard to measure and it's not equal across the board. And remember how I said anthropic is giving us a preview of the future of applications, the future of software. I've talked a ton about the future of SAS. SAS is dead. And what I really mean by that is it's all going to be agentic. It's all going to be generated. And I truly believe this. And now we have a vision of that future. This is called Claude. Imagine. So imagine with Claude. Let's go. We have what looks to be a desktop. It is fully interactive. And you're probably thinking, \"Okay, cool. I could probably build that with Claude code myself.\" And yeah, you're probably right. But just imagine this. This is now your desktop and you can generate apps on the fly, exactly what you want in the moment. So, let's try one of these sample ones. Create an email client from 3025. Okay. So, I simply type in what I want. It's going to start building it within this desktop",
        "start": 358.639,
        "duration": 1005.3580000000004
    },
    {
        "text": "now your desktop and you can generate apps on the fly, exactly what you want in the moment. So, let's try one of these sample ones. Create an email client from 3025. Okay. So, I simply type in what I want. It's going to start building it within this desktop have a fully functional email client. So, you can see right here, it's building it out. It has all the tabs on the left. It's starting to put in kind of fake emails. It's fine. Again, this is all demo, but this is really what's going to be possible in the very near future, especially when we have inference speeds approaching what Grock and Cerebras are capable of. And so, instead of it taking a minute or two to generate this, it literally takes a fraction of a second. And so, here we go. This is a fully functional application. So, if I click into it, okay, so it's starting to open it up. And again, it's just generating this on the fly. So, check this out. We have an email. It's all formatted properly. We have downloadable data and of course this data is not actually there yet. But if I download it, it will generate it and then download it for me. Here we can reply but when I click reply that functionality is not ready yet. So it is generating it on the fly. And so you can really start to envision what the future of software is going to be. So here we go. I'll just type hello neural send. I don't actually know what that means, but I think it just means that it's going to be using a neural network to actually send the data. And there we go. Quantum entangling message particles so on. Okay, this is all just demo. It's not really working, but the actual functionality could be working. So, let's try something else. Build me a calculator app. So, I'm going to leave this here. And now it should build a calculator app. And of course, very simple, but it's only going to get better from here. And so, here we go. But we can see it being built in real time in the window. And there it is. So I clicked eight. It actually has to generate that. Again, it's just generating the view until I'm done. Plus, it's generating that two equals. And there it is. 10. And again, yes, it is a little slow, but it is generating every single interaction on the fly. So let's try one more thing. Build me a to-do list. And there we go. Very fast, actually. Just a few seconds to generate what needs to be done. Let's say walk the dog. And of course, as you can see here, it is actually generating it on the fly. Here we go. Walk the dog. Let's say I completed it. I check it off. And",
        "start": 510.16,
        "duration": 1261.6790000000003
    },
    {
        "text": "fast, actually. Just a few seconds to generate what needs to be done. Let's say walk the dog. And of course, as you can see here, it is actually generating it on the fly. Here we go. Walk the dog. Let's say I completed it. I check it off. And to delete it again. It's just generating all of this on the fly. Now, let's say I want to browse the web. Create a web browser and navigate to google.com. Okay, so there it goes. It's generating the browser. It isn't actually navigating the web right now, but it is just generating what it thinks Google looks like. And there we go. So now let's see if I want to search for order collie which is a breed of dog. Okay. So again it's just generating all of this on the fly. There's Wikipedia akc.org. There are no ads because of course why would Claude put ads in it at least for now. So let's click on border Wikipedia. Let's see if it can generate the Wikipedia site. And there we go. It's generating a Wikipedia page for a Border Collie. All right. Now, let's see if it knows what reddit.com looks like. So, I type in reddit.com. You can see there it goes. It's still just generating it. All right. And there it is. There is a very basic version of Reddit. Now, it could have connected to the web. There's no reason it shouldn't, although they probably prevented it for security reasons. But that's it. It did work. And let me show you some of the industry reactions now. Of course, Ply the Liberator, Jailbroke, Anthropic 4.5, WOOI, this model is a real smarty pants. I had never seen recipes quite like this. And what is he talking about? Well, he got it to create some poisons, some narcotics, and I'm not going to show any of these, but yeah, he jailbroke it. And Matt Vid Pro, shout out, fellow AI creator said, \"Create a website that slowly falls apart comedically when you try to use it.\" Let's take a look. So, here we go. And And there we go. So, yeah, it's it's falling apart. And it is funny. And Aaron Levy from Box says, \"Box tested Claude Sonnet 4.5 for data extraction accuracy with Box AI on 40,000 fields across 1500 plus documents. Overall, the model performed four percentage points better than Sonnet 4 and saw major gains, especially in areas that required complex document and image understanding. So here we go. This is 4.5 versus four. And we can see four percentage points overall. And Ply did it again. He extracted the system prompt and it is a massive system prompt for CloudSonet 4.5, 80,000 characters and he dropped the full system prompt in GitHub. I will link this down below. So, a couple things about bias that I noticed. So, one, be as politically neutral as possible when referencing web",
        "start": 641.04,
        "duration": 1560.08
    },
    {
        "text": "the system prompt and it is a massive system prompt for CloudSonet 4.5, 80,000 characters and he dropped the full system prompt in GitHub. I will link this down below. So, a couple things about bias that I noticed. So, one, be as politically neutral as possible when referencing web any topic factually and objectively. Claude cares deeply about child safety. It does not provide information that could be used to make chemical or biological or nuclear weapons and does not write malicious code including malware. Obviously, this is all jailbreakable as proven by Ply and they specifically hardcoded Donald Trump is the current president of the United States and was inaugurated on January 20th, 2025. Donald Trump defeated Kla Harris in the 2024 elections. Do not mention this information unless it is relevant to the user's query. Why would they need to explicitly state that? I think because it is such a fiery topic that if anybody got Claude to say something like Donald Trump is not the president or anything like that, it would go absolutely viral in a very negative way and everybody would accuse Anthropic of being biased. And it's kind of interesting that they did this. I wonder what facts are going to be hardcoded into future models and other companies models. This is actually a very interesting point and I think we're probably going to see more and more of this. And if you're wondering how it compares to GPT5, check this out. So Flavio Adamo, who did it better? Same prompt, two completely different UIs. Here's one. And here's the other. And interestingly enough, this one is Claude 4.5. This one is GPT5. Which one do you like better? Let me know in the comments. And Daario, the CEO of Anthropic, said just a few days ago that Claude is already writing the majority of code for Claude. And this is what it's really all about. Extending that autonomous window is so important. We just spoke with one of the head researchers at OpenAI and he said the same thing. Being able to extend that window allows them to do so much more and complete harder tasks. So watch this clip from Daario right now. The vast majority of code that is used to support Claude and to design the next Claude is now written by Claude. It's it's the the just the vast majority of it within Enthropic and other fastmoving companies. The same is true. I don't know that it's fully diffused out into the world yet, but but this is already happening. So you can use it today. It's the same price as Sonnet 4. $3 per million input, $15 per million output. So still relatively expensive, but at least it's not an increased price. and they recommend upgrading immediately for all use cases. And once again, thank you to Vulture for sponsoring this video. You get $300 off your first month using",
        "start": 795.6,
        "duration": 1854.32
    },
    {
        "text": "Sonnet 4. $3 per million input, $15 per million output. So still relatively expensive, but at least it's not an increased price. and they recommend upgrading immediately for all use cases. And once again, thank you to Vulture for sponsoring this video. You get $300 off your first month using in the description below. Thanks again to Vulture. If you enjoyed this video, please consider giving a like and subscribe.",
        "start": 944.8,
        "duration": 1867.6
    }
]