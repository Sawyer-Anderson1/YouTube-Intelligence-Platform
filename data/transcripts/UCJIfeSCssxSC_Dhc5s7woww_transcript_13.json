[
    {
        "text": " We've had this fascinating conversation that started with restraining and mid-training. Let's get to post-training. A lot of fun stuff in post-training. So, what are some of the interesting ideas in post-training? The biggest one from 2025 is learning this reinforcement learning with verifiable rewards You can scale up the training there which means doing a lot of this kind of iterative generate grade loop and that lets the models learn both interesting behaviors on the tool use and software side This could be searching running commands on their own and seeing the outputs and then also that training enables this inference time scaling very nicely And it just turned out that this paradigm was very nicely linked in this where it's this kind of RL training enables inference time scaling but inference time scaling could have been found in different ways So it's kind of this perfect storm of the models change a lot in the way that they're trained is a major factor in doing so And this has changed how people approach portraying dramatically i Can you describe RVR popularized by Deepseek R1? Can you describe how it works i Yeah, fun fact Um, I was on the team that came up with the term RLVR, which is from our Tulu 3 work before DeepSeek, which is we don't take a lot of credit for the being the people to popularize the scaling RL, but as fun as what academics get as an aside is the ability to name and influence the discourse because the closed labs can only say so much that one of the things you can do as an academic is like you might not have the compute to train the model but you can frame things in a way that ends up being I describe it as like a community can come together around this RLVR term which is very fun and then Deep Seek is the people that did the training breakthrough which is they scaled the reinforcer learning which was you have the model generate answers and then grade the completion if it was right and then that accuracy is your reward for reinforcement learning So reinforcement learning is classically an agent that acts in an environment and the environment gives it a state and and a reward back and you try to maximize this reward In the case of language models the reward is normally accuracy on a set of verifiable tasks whether it's math problems coding tasks and it starts to get blurry with things like factual domains like that is also in some ways verifiable or constraints on your instruction like respond only with words that start with a Like all of these things are verifiable in some way And the core idea of this is you find a lot more of these problems that are verifiable and you let the model try it many times while taking these RL steps these RL gradient updates the",
        "start": 3.12,
        "duration": 328.96
    },
    {
        "text": "start with a Like all of these things are verifiable in some way And the core idea of this is you find a lot more of these problems that are verifiable and you let the model try it many times while taking these RL steps these RL gradient updates the reinforced learning from human feedback where in that era the score they were trying to optimize was a learned reward model of aggregate human preferences So you kind of change the problem domains and that let the optimization go on to much bigger scales which kind of kickstart a major change in what the models can do and how people use them What kind of domains is uh RLVR amendable to i Math and code are the famous ones And then there's a lot of work kind of on what is called the rubrics which is related to word people might have heard as LM as a judge which is like for each problem I'll have a set of problems in my training data set I'll then have another language model and ask it what would a good answer to this problem look like And then you could try the problem a bunch of times over and over again and assign a score based on this rubric So that's not necessarily verifiable like a math and code domain but this rubric's idea and other scientific problems that might be a little bit more vague is where a lot of the attention is where they're trying to push this set of methods into these kind of more openhanded domain so the models can learn a lot more i I think that's called reinforcement learning with AI feedback right i That's the older term from it that was coined in anthropic constitutional AI paper So it's like a lot of these things come in cycles also just one step back for the RLVR. So I think the interesting beautiful thing here is that you you ask the LM let's say a math question and then you know the correct answer and you let the LLM like you said figure it out But how it does it I mean you don't really constrain it much There are some constraints you can add like use the same language You don't switch between Spanish and English. But let's say you're pretty much hands off You only give the question and the answer and then the LM has to you know just the task to arrive at the right answer But the beautiful thing here is what happens in practice is that the LM will do a step-by-step description like you know like as a student or like as a yeah mathematician how you would derive the solution it will give you or it will use those steps and that helps actually the model to improve its own accuracy and then like you said the inference scaling So inference scaling loosely means basically spending more compute",
        "start": 167.68,
        "duration": 590.5589999999996
    },
    {
        "text": "or like as a yeah mathematician how you would derive the solution it will give you or it will use those steps and that helps actually the model to improve its own accuracy and then like you said the inference scaling So inference scaling loosely means basically spending more compute here the inference scaling is that the model would use more tokens and and also I think in the R1 paper they showed the longer they train the model the longer the responses are they they grow over time they use more tokens so it becomes more expensive becomes more expensive for simple tasks but these explanations they help the model with the accuracy there also interesting lot of papers showing what the model explains does not new does it have to be correct or maybe it's even unrelated to the answer but for some reason it still helps the model like this this the fact that it is um explaining and I think it's also again I don't want to anthropomorphize these LLMs but it's kind of like how we humans operate right if there's a complex math problem let's say in a math class you usually have a note paper and you do it step by step you cross out things and the model also self-corrects and that that was I think the aha moment in the R1 paper they called it aha moment because the model itself recognized that made a mistake and then said \"Ah, I did something wrong And so let me try And I think that's just so cool that this falls out of just giving it the correct answer and having it figure out how to do it that it kind of does in a sense what a human would do Although, LM don't think like humans It's kind of like an interesting coincidence And the the other nice side effect is it's great for us humans often to see these steps It builds trust but also we learn we can double check things There's a lot in here I think some of the debate there's been a lot of debate this year on if the language models like these aha I think the aha moments are kind of fake because in restraining you essentially have seen the whole internet So you have definitely seen people explaining their work even verbally like a transcript of a math lecture You try this Oh, I messed this up And what reinforce learning is this RLVR is very good at doing is amplifying these behaviors because they're very useful in enabling the model to think longer and to check its work And I agree that it is very beautiful that this training kind of the model learns to amplify this in a way that is just so useful at the final answers being better I can give you also a hanson example I was training the Gwen 3 base model with RLVR",
        "start": 299.6,
        "duration": 837.1989999999996
    },
    {
        "text": "I agree that it is very beautiful that this training kind of the model learns to amplify this in a way that is just so useful at the final answers being better I can give you also a hanson example I was training the Gwen 3 base model with RLVR accuracy of about 15%. Just 50 steps like in a few minutes with RLVR the model went from 15% to 50% accuracy And the more you can't tell me it's learning anything about fundamentally about math in i the Quinn example is weird because there's been two papers this years One of which I was on that talks about data contamination in Quinn and specifically that they trained on a lot of this special mid-training phase that we spent like a minute on That's weird because they train on problems that are almost identical to that i Exactly. And so you can see that basically the RL it's not teaching the model any new knowledge about math You can't do that in 50 steps So the knowledge is already there in the pre-training. You're just unlocking it I still disagree with the kind of premise because there's a lot of weird complexities that you can't prove because one of the things that points to weirdness is that if you take the queen 3 so-called base model and you can you could Google on the screen you could google like math data set hugging face and you could take a problem and what you do if you put it into queen 3 base the all these math problems have words so it would be like Alice has five apples and takes one and gives three to whoever and there are these word problems with these Quenb base models why people are suspicious of them is if you change the numbers but keep the words i Quen will produce like a very high without tools will produce a very high accuracy like decimal representation of the answer which means there's some like at some time it was shown problems that were almost identical to the test set and it was using tools to get a very high precision answer but a language model without tools will never actually have this So, it's kind of been this big debate in the research community is like how much of these reinforce learning papers that are training on Quen and measuring specifically on this like math benchmark where there's been multiple papers talking about contamination is like how much can you believe them And I think this is what caused the reputation of RLVR being about formatting because you can get these gains so quickly and therefore it must already be in the model But there's a lot of complexity here that we it's not really like controlled experimentation so we don't really know But if it weren't true um I would say distillation wouldn't work right I mean distillation can work to some",
        "start": 425.759,
        "duration": 1091.9999999999989
    },
    {
        "text": "quickly and therefore it must already be in the model But there's a lot of complexity here that we it's not really like controlled experimentation so we don't really know But if it weren't true um I would say distillation wouldn't work right I mean distillation can work to some the biggest problem in LM research this contamination because we don't know what's in the data It's unless you have a new data set it's really impossible And the same you mentioned um math the math data set which is a question and then answer and an explanation is given But then also even something simpler like uh MMLU which is a multiple choice benchmark if you just change the format slightly um like I don't know you use a dot instead of a parenthesis or something like that the model accuracy will vastly differ i I think that that could be like a model issue rather than a general issue i It's not even malicious by the developers of the LM like hey we want to cheat at that benchmark It's just it has seen something at some point and I think the only fair way to evaluate an LLM is to have a new benchmark that is after the cutoff date when the LM was deployed i Can we lay out what would be the sort of the recipe of all the things that will be going into post training and you mentioned our RLVR was a really exciting effective thing Maybe we should elaborate RHF still has a really important component to play What kind of other ideas are there on post training i I think you can kind of take this in order I think you could view it as what made 01 which was this first reasoning model possible or what will the latest model be and they actually have you're going to have similar interventions at these where you start with mid-training and the thing that is rumored to enable 01 and similar models is really careful data curation where you're providing a broad set of like what is called reasoning traces which is just the model generating words in a forward process that is reflecting like breaking down a problem into intermediate steps and trying to solve them So at mid-training you need to have data that is similar to this to make it so that when you move into post training primarily with this verifiable rewards it can learn and then what is happening today is you're figuring out which problems to give the model and how long you can train it for and like how much inference you can enable the model to use when solving these verifiable problems So as models get better certain problems are no longer like the model will solve them 100% of the time and therefore there's very little signal in this If we pull if we look at the GRPO equation this one",
        "start": 556.48,
        "duration": 1369.1999999999982
    },
    {
        "text": "enable the model to use when solving these verifiable problems So as models get better certain problems are no longer like the model will solve them 100% of the time and therefore there's very little signal in this If we pull if we look at the GRPO equation this one the reward given to the agent is based on how good a given action a action is a completion is relative to the other answers to that same problem So if all the problems get the same answer there's no signal in these types of algorithms So what they're doing is they're finding harder problems which is why you hear about things like scientific domains which is like that's so hard like getting anything right there If you have a lab or something it just generates so many tokens or much harder software problems So the frontier models are all pushing into these harder domains and they can train on more problems and the model will learn more skills at once The RHF link to this is kind of like RHF has been and still is kind of like the finishing touch on the models where it makes the models more useful by improving the organization or style or tone There's different things that resonates to different audiences Like some people like a really quirky model and RHF could be good at enabling that personality and some people hate this like markdown billeted list thing that the models do but it's actually really good for quickly parsing information in RHF this human feedback stage is really great for just give putting this into the model at the end of the day So it's what made chat so magical for people and that use has actually remained fairly stable This formatting can also help the models get better at math problems for example So it's like the border between style and formatting and like the method that you use to answer a problem is actually um they're all very closely linked in terms of when you're training these models which is why ROF can still say make a model better at math but these verifiable domains are a much more direct process to doing this because it's kind of makes more sense with the problem formulation which is why it kind of ends up all forming together but to summarize it's like mid-training is give the model the skills it needs to then learn RL and verifiable rewards is let the model try a lot of time So put a lot of compute into trial and error learning across hard problems And then RHF would be like finish the model make it easy to use and kind of just round the model out i Can you comment on the amount of compute required for RLVR? i It's only gone up and up So I think Grock 4 was famous for saying they use a similar amount of compute for",
        "start": 697.36,
        "duration": 1640.3189999999993
    },
    {
        "text": "model make it easy to use and kind of just round the model out i Can you comment on the amount of compute required for RLVR? i It's only gone up and up So I think Grock 4 was famous for saying they use a similar amount of compute for the scaling discussion they involve very different hardware for scaling Pre-training is very computebound, which is like this flops discussion which is just how many matrix multiplications can you get through in one time And because RL you're generating these answers you're trying the model in the real world environments it ends up being much more memory bound because you're generating long sequences and the attention mechanisms have this behavior where you get a quadratic increase in memory as you're getting to longer sequences So the compute becomes very different So you when in restraining we would talk about a model I think if we go back to like the Biden administration executive order it's like 10 to the with flops to train a model If you're using flops in post training it's a lot weirder because the reality is just like how many hours are you allocating how many GPUs for And I think in terms of time the RL compute is getting much closer because you just can't put it all into one system like restraining is so computationally dense where all the GPUs are talking to each other and it's extremely efficient where RL has all these moving parts and it can just take a long time to generate a sequence of 100,000 tokens like if you think about GBT 5.2 Pro taking an hour it's like what if your training run has a sample for an hour and you have to make it so that's handled efficiently So I think in GPU hours or just like wall clock hours the RL runs are probably approaching the number of days as pre-training, but they probably aren't using as many GPUs at the same time There's rules of thumb where in labs it's like you don't want your restraining runs to last more than like a month because they fail catastrophically And if you were planning a huge cluster to be held for 2 months and then it fails on day 50, the opportunity cost is just so big So you kind of don't want to just you people don't want to put all their eggs in one basket which is like GBT4 was like the ultimate solo run and nobody ever wanted to do it before where it took like 3 months to train and everybody was shocked that it worked where I think people are a little bit more cautious and incremental now So RL VR is more let's say unlimited how much you can train or get still benefit where RLHF because it's a preference tuning it you reach a certain point where it doesn't really make sense to spend more RL",
        "start": 835.839,
        "duration": 1892.2389999999984
    },
    {
        "text": "people are a little bit more cautious and incremental now So RL VR is more let's say unlimited how much you can train or get still benefit where RLHF because it's a preference tuning it you reach a certain point where it doesn't really make sense to spend more RL um preference tuning So there are multiple people that can give multiple let's say explanations for the same thing and they can both be correct but at some point you learn a certain style and it doesn't make sense to you know iterate on it My favorite example is like if relatives ask me what laptop they should buy I give them an explanation or ask them like yeah what is your um use case like they for example prioritize battery life and storage Other people like us for example we would prioritize RAM and compute and so but both both answers are correct but different people require different answers and with preference tuning well you're trying to average somehow like you are asking data labeler to give you the right well not the right the preferred answer and then you train on that but at some point yeah you learn that average preferred answer and there's no I think reason to keep training longer on it because you know it's just a style where with RLVR you literally give the model well you let the law model solve more and more complex difficult problems and so I think that it makes more sense to allocate more budget long-term to LRVR and also that right now we are in LRVR 1.0 blend where it's still like that simple thing where we have a question and answer but we don't do anything with the one stuff in between So there also I mean multiple research papers also by Google for example on process reward models that also give scores for the explanation how correct is the explanation and I think that will be the next thing let's say our LVR 2.0 O for this year focusing in between question and answer like how to leverage that information the explanation to improve the explanation and help it to get better accuracy but then uh so that that's one angle and there was a deepseek math version two paper where they also had interesting uh inference scaling there where first they had um developed models that grade themselves a separate model and I think that that will be one aspect and the other like Nathan mentioned it will be for LRVR are branching into other domains i The place where people are excited are value functions which is pretty similar So process reward models i are kind of like process reward models assign how good something is to each kind of intermediate step in a reasoning process where value functions apply value to every token the language model generates Both of these have been largely unproven in the language",
        "start": 963.6,
        "duration": 2180.7189999999973
    },
    {
        "text": "similar So process reward models i are kind of like process reward models assign how good something is to each kind of intermediate step in a reasoning process where value functions apply value to every token the language model generates Both of these have been largely unproven in the language people are more optimistic about value functions forever for whatever reason now I think process reward models were tried a lot more in this prep pre-ereasoning model era and a lot of people had a lot of headaches with them So I think a lot of it is the human nature of like value models have a very deep history in reinforcement learning They're one of the first things that were core to like deep reinforce learning existing is like training value models in this So right now the literature people are excited about trying value models but there's very little proof in it and there are negative examples in trying to scale up process reward models These things don't always hold in the future I think we came to this discussion by talking about scaling and a simple way to summarize what you're saying with like you don't want to do too much RHF which is essentially the signal scales is people have worked on RHF for language models for years especially in intense infra after chat and this the first release of a reasoning model trained with RLVR openis01 had a scaling plot where if you increase the training compute logarithmically you get a linear increase in evaluations and this has been reproduced multiple times I think deepseeek had plot like this but there's no scaling law for RLHF where if you log increase the compute you get some performance in fact the seminal scaling paper for RLHF is scaling laws for reward model over optimization so it's like that's a big line to draw with RLVR and the methods we have now and in the future like they will follow the scaling paradigm which is like the best runs you can let to run for an extra six and you get a few performance but you can't do this with RHF and that is just going to be field defining and how people approach them where I'm a shill for people academically to do RHF and that's a good way to describe it is like to do the best RHF you might not need the extra 10 or 100x of compute but to do the best snorts RLVR you do so I think there's a what I say is a seminal paper from what was a meta internship it's called it's like the art of scaling reinforce learning with language models they're what they describe as a framework as scale RL and their incremental experiment was like 10,000 B200 hours which is like thousands or tens of thousands of dollars per experiment And they do a lot of them which is just like this",
        "start": 1110.16,
        "duration": 2447.6789999999983
    },
    {
        "text": "of scaling reinforce learning with language models they're what they describe as a framework as scale RL and their incremental experiment was like 10,000 B200 hours which is like thousands or tens of thousands of dollars per experiment And they do a lot of them which is just like this academic which is a hard equilibrium where it's trying to figure out how to learn from each community",
        "start": 1245.76,
        "duration": 2462.638999999998
    }
]