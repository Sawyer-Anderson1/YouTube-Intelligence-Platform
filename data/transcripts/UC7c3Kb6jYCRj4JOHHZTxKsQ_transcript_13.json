[
    {
        "text": " This is an Aeros problem. It is one of the hardest open math problems on Earth. In all of history, there are only hundreds of people that can actually solve these problems. But this guy, Neil Smani, a quantitative researcher, has just solved it. And here's the interesting part. He solved it with the help of artificial intelligence, specifically GPT 5.2. And this video is brought to you by HPCAI. More on them later. The proof for Aeros problem 397 was prompted by a human, generated by AI, then was submitted and accepted by Terrence Tao, basically the best mathematician of all time. Listen to what Neil says at the end of this post. Many open problems are sitting there waiting for someone to prompt Chad GPT to solve them. This is a huge deal, but it doesn't just end there. Over the last two weeks, six of these problems have been solved with the use of artificial intelligence. AI is solving frontier math. And even crazier, listen to this. How long did it take 5.2 to solve? Around 15 minutes. That's it. So, why is it so important? Why is it so amazing that AI is solving some of the hardest math problems on Earth? Well, if you thought I was going to share this graph from the intelligence explosion paper, then you were right. Once again, let's look at this incredibly relevant chart. The intelligence explosion happens when artificial intelligence can self-improve. Right at this point, when it can do scientific discovery, mathematics discovery, then all of a sudden it can improve itself. It can find new efficient ways to run itself to run artificial intelligence apply it back to itself and then we have this recursive compounding effect of the intelligence explosion and then after that it is completely unbounded intelligence and this isn't the first time that AI and specifically Chachi PT has solved some of the hardest math problems on earth just a few months ago it got gold at the IMO which is the international National Math Olympiads, one of the most difficult coding competitions on Earth, where only about 9% of the human participants earn a gold medal. Here's another one. Alpha Evolve from Google was able to improve the matrix multiplication algorithm for the first time in 50 years. It was a small improvement, but it was still an improvement and one that was completely unlocked by artificial intelligence. And matrix multiplication is the heart of how artificial intelligence works today. So a small improvement scaled over the entirety of the artificial intelligence ecosystem actually results in massive gains and again it's self-improving. Every single matrix multiplication after that is now more efficient. Thus it can run more often and it compounds and allows it to discover new math and new science. Rinse and repeat. And in fact, that same AI system by Google, Alpha Evolve, was able to come up with improvements to the Google server architecture, the worldwide server",
        "start": 0.24,
        "duration": 408.399
    },
    {
        "text": "more efficient. Thus it can run more often and it compounds and allows it to discover new math and new science. Rinse and repeat. And in fact, that same AI system by Google, Alpha Evolve, was able to come up with improvements to the Google server architecture, the worldwide server circuit design to the scheduling system to how Gemini trains. So that's open AI, that's Google, but many other companies are seeing the same thing. Sakana AI out of Japan had the AI scientist that is AI that can do discovery in science and actually use that discovery to again self-improve and you can get open source models to solve some incredible math and you should do so with the sponsor of today's video. We all know that prompting isn't enough anymore. The entire industry is moving to post-training to get models to actually behave, but the infrastructure side is a nightmare. You basically have three bad options. Cloud GPUs where you burn through cache just debugging everything going on in your system. Slurm clusters where you're waiting in cues forever. Or managing bare metal and then you become a CIS admin. And that's an entire monster in itself. That's why you need to check out the fine-tuning SDK from the sponsor of today's video, hcai.com. They use function level management similar to the workflow that Tinker made famous. You still control the forward and backward steps in the code, but it all runs entirely on their fully managed cloud infrastructure. And the easiest part is the billing. They use a transparent tokenbased model for pricing. You only pay for the input tokens, the output tokens, and the training time that you actually use. So, you can think of this like local code, but at cloud scale. Check out hpc-ai.com. The first 100 users will receive $10 in free credits. Link down below. Let them know I sent you. Now, back to the video. And in fact, Terrence Tao, again, the best mathematician on earth, has been keeping a record of all of the open math problems that have been solved or partially solved or attempted to be solved by artificial intelligence. And by the way, I'll drop this link down below. So we have a lot of attempts from alpha evolved the AI from Google that I just mentioned. Here's ChachiBT and here's one of the Airdos problems that were just solved January 10th 2026. A mixture of Aristotle CHBT 5.2 and thinking we have green full solution verified validated with lean and again it seems like the combination of this Aristotle software Chad GPT 5.2 to pro and then of course a human prompting it. We get multiple full solutions and these are fully open problems. Now if we scroll down a little bit we see again what were thought to be open problems but subsequently found earlier human solutions but look at all of these passes right here using again Aristotle",
        "start": 204.64,
        "duration": 733.4400000000002
    },
    {
        "text": "human prompting it. We get multiple full solutions and these are fully open problems. Now if we scroll down a little bit we see again what were thought to be open problems but subsequently found earlier human solutions but look at all of these passes right here using again Aristotle here are a bunch of already solved problems that were again solved by artificial intelligence. And so tying this all together, what does it mean? I hinted at it at the beginning of this video. We are at this inflection point where we're starting to see certainly more frontier math problems get solved. We're building systems that can perform open-ended scientific discovery. This is obviously amazing for humans. This is where we really get excited about things like solving all disease, curing cancer. Essentially, we can just throw infinite intelligence at these problems and they're going to get solved. And then when they make discoveries in the realm of artificial intelligence, whether it's matrix multiplication or the successor to transformers, all of these things are then going to be applied back to AI and it becomes a self-improving system. This is really again when we have the intelligence explosion. There are a lot of people out there saying that the pace of AI improvement is slowing down, but it certainly doesn't seem so. The longer these models can think, the more crazy hard math problems they can solve, the more open-ended scientific discovery that they could accomplish just makes them more powerful. And this is all happening right now. And we are able to parallelize it all. AI doesn't get tired. A human can spend an hour or a few hours trying to solve some of these hardest math problems. AI can go 247 and then we just replicate that AI. Whether it's 10 or 100 or a million. At that point, the only constraint is how many GPUs do we have and how much energy can we throw at those GPUs. It is such an exciting time to be alive to see these things and we're just going to continue to see it accelerate. And another big thanks to HPCAI. Start fine-tuning with HPC AAI today. Link down below with $10 in free credits. If you enjoyed this video, please consider giving a like and subscribe.",
        "start": 369.12,
        "duration": 979.4379999999999
    }
]