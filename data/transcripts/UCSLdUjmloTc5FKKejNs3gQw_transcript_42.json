[
    {
        "text": " This is Pummel, an open-source markup language developed by Microsoft to help you write reliable and maintainable AI prompts. Why do I need a markup language to help me write prompts? I mean, isn't it just plain English? I thought the same thing at first, but after seeing it consistently add data from external files and extract text from websites, I was sold. So, let's go through just how cool Pummel is. And before we do, don't forget to hit that subscribe button. Before we continue, let's remember that Pommel was developed by prompt engineering researchers at Microsoft. Basically, people who write lots and lots of detailed prompts. So, keep that in mind when watching this video before commenting, \"This is too complicated. Why would anyone ever use this?\" Okay, with that out of the way, let's talk about Pummel. Pummel itself looks like HTML, but can get compiled into whatever format you want for your prompt. Unfortunately, for JavaScript developers, the TypeScript SDK is not yet stable. I tried it out and got some issues. So, for this video, we're going to be using the Python SDK. And here is a very basic example of it using the Pommel class and this string as an argument to generate an output that looks like this. So, here is another example using the pommel class. This time passing in a pummel file that looks like this. encapsulating everything in pummel tags with a ro tag to specify the role of the LLM, a task giving the LLM a task and a hint giving it a hint. All of this compiles to this markdown which we can format just to output the content. By default, Pommel files compile to markdown, but this can be changed to JSON which looks like this or HTML, YAML or even just XML. Yes, there is a Pummel VS Code plug-in to help you visualize the output, but I don't use VS Code. So, we're doing everything in the terminal. From here, we can get this prompt and pass it as an argument through a subprocess, which in this case is running a non-interactive codeex command. This could of course be claim. So, running the file will send the prompt to codeex which generates a response. This of course is one way to use pummel if you want to run a dententic tasks. But a more efficient way would be to directly use the open AI API for Python or any other API you want. So from here we can set up the open AI SDK and then change the format to open AAI chat. But of course there are other formats like raw message dict and paidantic. From here, let's just print our prompt to the console so that we can see it's in the open AAI format. And then we can just focus on the content before sending the prompt to open AAI using the GPT5 mini model. And",
        "start": 0.16,
        "duration": 339.60099999999983
    },
    {
        "text": "formats like raw message dict and paidantic. From here, let's just print our prompt to the console so that we can see it's in the open AAI format. And then we can just focus on the content before sending the prompt to open AAI using the GPT5 mini model. And some extra details. So when we run that, we see our prompt followed by the response from GPT5, which is not what I was expecting. Anyway, let's go back to the pummel files. These in pommel, so roll, task, and hint are called components. Yes, very confusing if you're coming from the world of react. But if you want to feel like you're more at home, you can make them all uppercase and everything will still work. There are basic components like bold code and captioned paragraph. Intentional components like example, hint or step-wise instructions which get rendered like this. And data components for adding documents, images, and tables to your prompt. This is actually really cool because it's getting a data file, in this case an Excel file, but it could be a Word document, getting just five records, and converting it from an Excel file to a CSV, which shows up correctly in your prompt, no matter if it's markdown, and even if I changed this to JSON and took off the conversion to CSV, everything still works. You can even add text from a specific part of a website. But Pommel has a template engine which you can use to do some really cool things like creating a variable with let and referencing it with double curies. Getting data from a JSON file and rendering its output here. Also notice that pummel works with two different pummel tags that both have two separate syntax values. Anyway, you can also pass data from code into the pummel file using context. So in this case we have a Python string variable and an object called people which we're referencing with variables and can print out in our prompt. We can also loop values using the four attribute use if conditions and include other pummel files. Yes, I know that you can include files in claude code custom prompts as well. But the benefit of pummel is that the prompts can be used anywhere in any model no matter the tool. Okay, now that we've got the basics out of the way, let's go through an actual use case. This is an agent MD file generated for a project I'm working on. And it's pretty bare bones. I mean, right now it's 25 lines and it doesn't really talk about the way I like to write code with some examples. So, to help it out, I've put some of my code styles in this folder containing my JavaScript, Nex.js, React, and TypeScript preferences. And if we take a look at it, yes, this was also generated by AI, but this is very close to the way",
        "start": 171.44,
        "duration": 647.841
    },
    {
        "text": "code with some examples. So, to help it out, I've put some of my code styles in this folder containing my JavaScript, Nex.js, React, and TypeScript preferences. And if we take a look at it, yes, this was also generated by AI, but this is very close to the way have created this Python file that's going to use OpenAI and some CLI arguments to create an agent MD file using this PML file. And what this does is it grabs the CLI variables and renders different documents based on their values. So if it's React, add this document. If it's TypeScript, add this document and so on. Then it adds to the rest of the prompt with suggestions on how to write a good agent MD file. This will send the prompt to OpenAI using the TPT5 nano model. Then we'll create an agent MD file and tell me when it's done. So right now, as you can see, there's no agent MD file here. So I'm going to ask it to run the script for a React and TypeScript project. And here we can see it's created an agent MD file without having to look through all of my code. In hindsight, my examples had Nex.js, so it's picked that up. Nevertheless, it's added some into the file, so it can see the way I like to write components. It's got the import order, my styling patterns, and so on, which is a really fast way of making an agent MD file without having an agent go through every single one of your code files. And if you want some more inspiration, there are plenty of examples on the Pummel repo, from blog post writing to math calculators and even code reviews. You can already see how useful Pummel can be when it comes to writing prompts. And there are a few features I didn't even go through, like using stylesheets to specify the shape or format of your data, being able to specify which tools to use, which can evaluate expressions using ZOD, and being able to capture traces from your prompt before it's sent to the LLM. If you want to know more about Tracy, check out the link in the description to watch our explainer video. Anyway, what do you think about Pummel, and are there any alternatives you prefer? Let me know in the comments. Again, don't forget to subscribe.",
        "start": 327.919,
        "duration": 875.8410000000005
    }
]