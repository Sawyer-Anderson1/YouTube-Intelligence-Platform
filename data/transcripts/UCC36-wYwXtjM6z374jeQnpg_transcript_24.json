[
    {
        "text": " Most of the public focus to date has been on the large public benchmarks for things like coding I think the problem is though i Matt Fitzpatrick, CEO of Invisible Technologies. i We're an infrastructure company What that infrastructure allows us to do is what I would call hyperpersonalized software at scale i It sounds like your position is we need thousands of of new narrow benchmarks to capture maybe every labor category every industry vertical That is an interesting second part of this which is i we're going to see the largest disruption ever in 2026 from companies that don't make this change i There are many sectors where the structure of what the industry does is going to change If you think about knowledge work the production of large amounts of documentation where these these technologies are very disruptive I think the question is which parts of your business can really change with AI? What are you seeing most companies get wrong on their mission to implement AI? i You've got two kind of different challenges One is i now that's a moonshot ladies and gentlemen i Everybody welcome to Moonshots. In today's episode we're going to be discussing why all companies need to become AI companies in 2026. How they do that what happens if they don't We'll discuss whether or not big legacy companies can even make such a dramatic change and how they can best do it We're going to be going over some fun and meaningful AI use cases So, listen up I think they're ones that are going to sort of get you excited about what you can do And we'll dive into some predictions from our guest for 2026. Today joining the moonshot mates is a friend of the pod Matt Fitzpatrick. Uh, who for more than a decade was at McKenzie, rising to the position of global head of Quantum Black Labs. I love that name Quantum Black. It's so cool Leading the firm's AI software development R&D, and global AI products A year ago Matt joined as the CEO of Invisible Technologies, a company started by a brilliant friend of mine Francis Padrazza. For those of you who don't know Invisible, uh the company is a modular AI software platform that uses AI training uh and provides AI training for most of the large language model providers out there Uh building custom workflows and agents for enterprise Uh they anchor their work in creating clean data and human in the loop delivery to ensure measurable business results Matt, welcome Good to have you here i Hey Matt, i thank you for having me We got DB2, AWG, Sem almost Happy holidays guys i It feels like it feels like we're on this pod you know every other day I think we should just move into a large you know sort of podcast house and i we will have fully documented the singularity i Oh, well you know I'm really looking",
        "start": 0.56,
        "duration": 336.7200000000001
    },
    {
        "text": "holidays guys i It feels like it feels like we're on this pod you know every other day I think we should just move into a large you know sort of podcast house and i we will have fully documented the singularity i Oh, well you know I'm really looking you know on Thursday, we have to do our predictions for next years i Yeah. i And Matt is going to give us a ton of insight today One of my predictions out of the gate is that enterprises are going to move super stupidly slowly compared to AI capabilities And Matt is the worldleading expert on the intersection between AI and enterprise So I cannot wait for this i You cannot you can't cheat this way Dave, and sort of use Matt's predictions as yours i I can't i No, you'll be there i Well, but everybody listening this pod will know that i All right Well, take good notes uh if nothing else Um you guys ready to jump in Matt, I'm going to kick it off with a question sort of a broad question for you i Um and uh here it is So within the past years you know we've heard from like every company out there and every CEO that we're going to be pivoting to become an AI company See, in the last pod you said something like we're going to see the largest disruption ever in 2026 from companies that don't make this change And I think Alex uh the term you used is they're going to be cooked if they don't i I said knowledge work is cooked Not knowledge workers not companies Knowledge work as we currently know it i Uhhuh. Okay. So you don't think that companies are going to be cooked that don't make the transition to to AI. i I think we're going to see many more companies over time and many more smaller companies as well i Okay. Well, uh we're going to dive into that So look in an earlier episode we we pointed out that you you know when you thought you had product market fit and you were scaling a SAS company you're like toast because everything needs to be rethought now given AI. So this is now applying to big companies also i All right So Matt, the question to kick this off is can every company truly become an AI company and how And then which companies and industries do you think need to disrupt themselves now before they become basically irrelevant Sort of that's a you know a softball question to kick us all off here i Peter saving the hard balls for me Matt i I I always i I think your second question relates to your first question in some ways which is I don't think and I think all the data that has come out on this so far that all industries are going to be",
        "start": 171.76,
        "duration": 601.36
    },
    {
        "text": "the hard balls for me Matt i I I always i I think your second question relates to your first question in some ways which is I don't think and I think all the data that has come out on this so far that all industries are going to be are some sectors and areas where you're going to see materially different impacts I think areas like media legal services um business process outsourcing there are many sectors where the structure of what the industry does is going to change If you think about knowledge work the production of large amounts of documentation where these these technologies are very disruptive I think where there's I think the hype has been a bit overblown is if you take a lot of sectors like oil and gas or real estate the function of what they do is going to stay pretty consistent and I think actually most of the good analytic of how you know job dynamics will change over the next couple years will get at this like the decision on which apartment building or which office building to buy is going to function pretty similar to what it did 5 six years ago and so I think the question is which parts of your business can really change with AI it's not all of them and and some sectors it'll be more or less And then the second part of your question which is also an interesting one which is can everyone actually become an AI company And that is an interesting second part of this which is not there are not that many people that know how to build these sorts of models or deploy these sorts of models well And so one of the big challenges is do you have the expertise unhouse to do this How do you think out adjusting the operating functions of your company to do it Is it the same team you have in your IT function doing it now And particularly Peter, I know like the the the set of folks that uh you and I have spoken with in the past like small businesses if you're a 50 person company it's hard to deploy a lot of this stuff at scale if you don't even have a CTO in house And so I think there's a mix of is your industry going to fundamentally change and then what are the actual core competency your company has to implement it i And so I think what you're going to end up finding Yeah. i Yeah. So, I mean do you end up bringing a chief AI officer into your company Are you going to bring that capability or are you basically renting it Uh, I mean part of the other thing that's going on right now we've talked about this on the pod a lot is your competition isn't really the large you know multinational It's the AI native",
        "start": 306.4,
        "duration": 847.5999999999998
    },
    {
        "text": "company Are you going to bring that capability or are you basically renting it Uh, I mean part of the other thing that's going on right now we've talked about this on the pod a lot is your competition isn't really the large you know multinational It's the AI native that's reinvented themselves from the ground up as an AI first company right i Yeah. i Yeah. Like down and dirty Matt. like which happens first I can get a mortgage by talking to an AI and get it done in under an hour or we're walking on Mars with our own two feet like like which which of those two things is going to happen in the real world first i Yeah. So the the way I've often heard the the question asked is do the um do the startups get distribution before the big companies build the technology And I do think that will be the tension in a lot of ways And look I think there's a lot of big established companies that are going to figure out how to do this really well Like I I think if you take a sector like legal services I do think the big law firms will figure out how to use a lot of this over time Um you know I think there are sectors where I think banking is a really interesting one to look at right now Uh you know if you look at the the the age of the application footprints in banking like most of the teach that exists in banking is north of 20 years old Um and so you do have a bunch of very fastmoving newer fingers that are approaching it different ways companies like Revolute. I I don't know how that plays out but I do think I do think that becomes the the question in a lot of ways is which moves faster the emerging entrance or the modernizations of the existing Um I think Peter to to hit on what you were asking also as a second part of that Do you buy or rent I I think that's something you've got to be really honest with yourself about as a company right And I I think the idea that everyone can buy everyone can um hire people to do this is challenging The challenge of trying to adapt an existing IT function to do this is many of the skill sets that people hire for even like do they know Python things like that um there are gaps in that and so I think the answer that most companies I've seen who don't have the resources in house who are being just directive about how to push that is they they are finding ways to rent or buy this externally and to partner with folks that can allow them to do it Every week my team and I study the top 10 technology meta trends",
        "start": 431.68,
        "duration": 1096.0810000000004
    },
    {
        "text": "resources in house who are being just directive about how to push that is they they are finding ways to rent or buy this externally and to partner with folks that can allow them to do it Every week my team and I study the top 10 technology meta trends decade ahead I cover trends ranging from humanoid robotics AGI, and quantum computing to transport energy longevity and more There's no fluffy only the most important stuff that matters that impacts our lives our companies and our careers If you want me to share these meta trends with you I write a newsletter twice a week sending it out as a short two-minute read via email And if you want to discover the most important meat trends 10 years before anyone else this report's for you Readers include founders and CEOs from the world's most disruptive companies and entrepreneurs building the world's most disruptive techs It's not for you if you don't want to be informed about what's coming why it matters and how you can benefit from it To subscribe for free go to dmmandis.com/tatrends to gain access to the trends 10 years before anyone else All right now back to this episode I think I think legal and accounting are really really cool case studies and I know you know more about this you know your McKenzie time quantum black you were like the guy understanding and parsing all of this but they're really cool because they can be replaced by a startup you know like Harvey i Dave two things I'd say about that I think one challenge of the implementation of genii in the enterprise setting is a statistically validatable baseline to compare against right and so as an example if you take something like mortgage underwriting has made huge progress like in a very positive way Actually, the percentage of mortgage underwriting that's now done by a very guardrail uh and very effective set of algorithms developed by the banks is pretty high because they can back test and say this is a correct credit decision that has no redlining anything else i But if you think about a a document like the the reason contact centers has been one of the cases where you've seen a lot of adoption of this is you do have a clear baseline right like time per call seesaw cost per call You you have a set of metrics you compare against something like let me generate an investment memo which is different in format at every firm It could be 10 pages versus 40 pages The content is different It's been harder for folks to build baselines Um I do think that's why legal services is an interesting one is there's certain areas of legal where um where those baselines are clear like you can look at at what documents are really good for like an ISA agreement But where I would I think you're going",
        "start": 557.44,
        "duration": 1365.5999999999995
    },
    {
        "text": "baselines Um I do think that's why legal services is an interesting one is there's certain areas of legal where um where those baselines are clear like you can look at at what documents are really good for like an ISA agreement But where I would I think you're going segments I think the high end of that market still persists in a really differentiated way which is if you're doing a large M&A transaction you're still going to want a really good lawyer's advice And where it changes I think is in the more very basic like produce an MDA type of of work and and the kind of basic and I think that's going to be one of the shifts again is is that really really good human guidance is going to persist forever It's the the basic commodity information that right now a lot of people are paid you know probably excessive amounts of money to do i Yeah. Well, you know the NDA is pretty extreme That that's a that's a But I'll tell you the the the venture funding that we do you know we do tons of these every years And the term sheets always say you the company that we're investing in will bear the costs of the legal uh capped at $50,000. And then the documents are freaking identical every single time There's like eight knobs and you could you could store all combinations on like the smallest thumb drive in the world And like how is this like a $50,000 and and it always runs up to $4,999.99. It's like whoa what a miracle So, I don't know that to me feels like you know that would be on sort of the mid to hard end of the scale yet it's still so doable You know an NDA is a no-brainer. Mortgages are no-brainers. i I I completely agree I think what's been interesting though is how slow the actual adoption curve has been in say contact centers because contact centers should have had I mean generally sees scores or people don't really like most contact center actions um the the the kind of general customer feedback you get is pretty unhappy Uh and that's been true for a decades So you would have expected i walk us through the whole CLA thing Actually, I know you're an expert on this Like the CLA thing has been really interesting to watch Wait, tell tell us the story i What is the CLA thing i By the way I was not involved in CLA in any way but I can say at least what I what I know from from from reading about it and what my hypothesis would be I mean basically CLA announced that they were going to move entirely towards um a fully endto-end Agentic contact center and then a couple months later they and by the interesting was at that time they were the most frequently cited example",
        "start": 694.32,
        "duration": 1648.1599999999994
    },
    {
        "text": "about it and what my hypothesis would be I mean basically CLA announced that they were going to move entirely towards um a fully endto-end Agentic contact center and then a couple months later they and by the interesting was at that time they were the most frequently cited example then about 8 to 12 months later they basically announced they were rolling the whole thing back and moving back entirely to human contact center agents and and I found the entire evolution kind of interesting because I if you think of how these systems should be defined like you know deployed like a mufti went system The way it should work is you'd have uh kind of an orchestration of what the types of calls are You'd have a set of validations on which calls could go well or badly and you'd have some sense of where you need escalations to human agents versus where you're going to say So you actually would never want to move and I think this is a theme invisible has throughever. You're never going to want to move to doing everything argentic are going to want humans in the loop in every in almost every industry in almost any topic because I think actually that's where a lot of the if if these models are trained off of precedent data and then you you can train them really well to then kind of continue that logic you're going to want humans for some of the things where you don't really have precedent data or you need them to work through complex things where you don't have enough historical information and so I found the entire structure of how the change happened quite confusing because you would always want to a contact center to be a mix of humans and and agents and then evolve the mix between those and I'll watch topics and so the whole movement from all all humans all agents back to all humans was confusing I think for i you are pregnant with a question i no no I just wanted to give out some details here so the CLA situation they rolled out an AI to do customer service calls and the claim was in the first month it did the work of 700 full-time agents handled 2.3 million calls a month and they projected that it would save them 40 million a years Um, and they were like really proudly saying this was like month one and it's only ever going to get better uh from here The when I saw that I was like okay I if I was doing that this sounds like a PR exercise more than anything real because you never put that out in the first months You'd wait a couple of months to see what exactly happened Um, and Matt, you may be able to give a little more color on why did they roll it back uh in",
        "start": 837.04,
        "duration": 1900.3209999999988
    },
    {
        "text": "a PR exercise more than anything real because you never put that out in the first months You'd wait a couple of months to see what exactly happened Um, and Matt, you may be able to give a little more color on why did they roll it back uh in were too many Uh the the exception handling was too much or was it a cultural backlash What was it exactly that had them undo the whole thing i I I I don't know in the sense I haven't worked with CLA, but um you know I think you hear a variety of different pieces of feedback on why folks have struggled in contact centers I think one one reason is um there there are cases in which humans just want to talk to another human And so uh I think some of the PR of saying we're moving to only agents has its challenges Uh I think two a lot of the challenges and where uh contact centers are most sensitive is non firstling call resolution topics So it's not something like check your balance It might be something like process a refund right Something that's pretty complex You have to write back to the source systems I would it was surprising to me how quickly they rolled that out and I wonder how hard how well it was able to roll to kind of deal with some of the more complex functionality in that example i right Go from level one to level two and three very quickly on those support calls and then and then you're you do not want an AI dealing with you i Can we get back to the main question here which is you've got you know 2026 is coming up i Um, if you're listening to this in 2026, it's here now Uh, so here's the question you're a medium-sized company or largest company and and your board of directors has just said i uh to Mr. CEO or CTO, uh guys what's your AI plan What are you doing I mean we're seeing that over and over and over again Uh, their first reaction is what typically and what should their what should they do I mean I want to just get some of the fundamentals here because I want to serve our our listener base in that fashion i Yeah. So, I I think if you're that CEO, you've got two kind of different challenges One is what are the things I should focus on And then two is who should do them uh and do I have those skills in house And so the first thing I would start with is making sure you know the first question And I do think this is a question of following the value So I'd go down a list I would not start with letting a thousand flowers bloom I would start with what are you know two three things that if",
        "start": 966.24,
        "duration": 2152.7209999999986
    },
    {
        "text": "with is making sure you know the first question And I do think this is a question of following the value So I'd go down a list I would not start with letting a thousand flowers bloom I would start with what are you know two three things that if needle for your business Maybe it's you know we're just talking about customer service Maybe that's one example Maybe it's uh forecasting um in your uh FPNA function um maybe it's um uh inventory management but there's there's definitely two to three things that almost any business on Earth, even as a small company has that are digital marketing probably is another one that you see pretty frequently And you focus on one or two of those and you make sure you get to a pilot stage in that one or two Meaning not a strategy documents I I do think the one thing that anyone who spent real time in the space will tell you is um well if you take the paradigm of how machine learning is deployed where you spend months and months building something and then it works and you can underwrite statistically that it works This is kind of the exact opposite paradigm and that you can get a prototype up and running in a month i but you have to do a lot of testing and validation to to make sure you can trust it And so it is really a a function of making sure you can get something up and running and testing and validating And you know Peter, the question I always ask is would you under would you bet your annual bonus that whatever use case you deploy works And and that's a complicated thing If it's like let's say generate a claims um a claims processing review and you have to do 10,000 of them Most companies don't know how to say that works or it doesn't And so what I would do is just to summarize make sure you have a list of two to three things that moves the needle Make sure you get to do a proof of concept in one of them And I would probably do that first use case as an RFP to a third party vendor that gets compensated based on results i Mhm. Yeah. i And I say that very specifically because I I think if you do it in-house, the odds are the unhouse team has not had a lot of exec experience with this And so you also can't hold them accountable in the same way of you get paid if it works And so I do think tying it to outcomes limits your risk i I mean that that is still the business model for Invisible, right You're paid by money saved i Correct. We outcomes Yeah. Outcomes in various ways Yeah. i Yeah. Alex, i I want to bring you into the game here",
        "start": 1094.08,
        "duration": 2402.721999999999
    },
    {
        "text": "do think tying it to outcomes limits your risk i I mean that that is still the business model for Invisible, right You're paid by money saved i Correct. We outcomes Yeah. Outcomes in various ways Yeah. i Yeah. Alex, i I want to bring you into the game here preliminary matter in interest to full disclosure I have no financial interest in Matt's company Invisible. I do have a number of questions thought First question maybe pulling the the thread on testing One of the things that we talk about here on the pod all the time is benchmarks The importance of benchmarking I'm I'm curious i We all talk about that constantly Alex. i That is all we talk about We talk about nothing else i That is all we talk about i Oh, wait Maybe that's you Okay. G given that's all we talk about as as Dave just mentioned and given that Invisible is is also in the business of training so many models what benchmarks do you think most need to be brought into existence in the world What what's most missing Top three benchmarks you'd like to see summoned into existence Yeah, look I I think and you've seen a bunch of of these start to get publicized in the past uh couple of months but most of the public focus to date has been on the large public benchmarks for things like coding and and I think those are very useful as metrics for are the models improving broadly Uh and I think you know that that is a way you've been able to see by any standard if you look the last three years the models have 50% to 100% improvement on most dimensions um that you can look at I think the problem is thought if you think about like enterprises or small businesses your benchmark for most cases is not a broad-based um occur kind of cognitive benchmark it's accuracy or human equivalence on a specific task And so what I think you're going to see more and more need for is kind of custom evils on highly specific topics So if you go back to the contact center example the benchmark you want to build if you're going to roll this out for a contact center is a series of expert agents that are in your contact center and how they perform and then how the AI agents perform similarly Same with claims processing But basically most businesses are going to have to get comfortable with doing what's called an evil or a custom benchmark for the tasks they're trying to modernize because an 80% accurate very smart deployment is not you know there's still too much risk in that fallout framework And so I think a lot of this is actually the way that we think about benchmarking will evolve from broad-based benchmarks to hyperspecific benchmarks I I freaking love that because I can immediately see",
        "start": 1220.96,
        "duration": 2678.3230000000017
    },
    {
        "text": "smart deployment is not you know there's still too much risk in that fallout framework And so I think a lot of this is actually the way that we think about benchmarking will evolve from broad-based benchmarks to hyperspecific benchmarks I I freaking love that because I can immediately see calling in life based on what you just said because because all this you know benchmarking within these domains is really really hard to figure out unless you know the like you know title insurance you know what's the benchmark for successful AI and title well somebody in that industry listening to this pod right now is going to like you know what I was an early adopter of AI and I know this space inside and out that's my benchmark to own and if you declare yourself the owner of it and then broadcast the benchmark the evidence so far is you become an instant start Like nobody's grabbing topic ownership in all these topics and if if you just get there first you become an instant start I i I completely agree with that and I have to be a serious geek like Alex type or Matt type or David. i I don't even agree with that In this era of post training as a commodity if you own the benchmark often it's the case I think that the benchmark is the hard part and you can leverage existing resources to post train an offtheshelf model I am curious thought Matt, maybe following up on this So it it sounds like your position is we need thousands of of new narrow benchmarks to capture maybe every labor category every industry vertical assuming that's the correct is that something that Invisible is working on can be working on should be working on i Yeah, we do spend quite a bit of time working on that In fact a lot of the time what we're building is customer specific benchmarks for an individual task So that is that is a lot of what we think about is actually how to test equivalents for a given task And you know I think one of the things that folks have not fully realized is let's say you take a really high performing LLM and you want to tailor it to your individual context That process of actually fine-tuning it off of your data So, an example I would give is um and I I think one of the challenges that that people were hoping that this would be a SAS um buyer paradigm meaning like I could just buy something that off the shelf would just solve everything I needed So, like I wanted to buy a a a a sales agent I wouldn't have to do anything I could just take in a sales agent that would sell well And the reality is that's pretty hard to do You need to actually train it up on your specific knowledge corpus your",
        "start": 1361.2,
        "duration": 2918.2430000000018
    },
    {
        "text": "I wanted to buy a a a a sales agent I wouldn't have to do anything I could just take in a sales agent that would sell well And the reality is that's pretty hard to do You need to actually train it up on your specific knowledge corpus your think about it is you take the LLM or you take an agent that's been trained for sales and then you fine-tune it off of your specific company information your products the way you sell your way of speaking and then you have to build an evil or a benchmark against that to say this is performing well or not on that Quick followup question if I may because there was the sort of infamous Bloomberg GPT moment where Bloomberg was sort of in quasi competition with the Frontier Labs. They had a wide variety of internal proprietary data sets Their original plan this is now sort of an infamous episode from 1 to two years ago Their plan was to offer their own proprietary frontier model basically and but trained critically restrained and or post-trained off of their internal data sets And the the plan was to achieve superb performance in financial domain because they had all the data or a lot of data that were not broadly available to the general public But what actually happened is the generalist models offered by the frontier labs that were training basically off of the internet and more or less publicly available data sets within a few months leapfrogged Bloomberg's GPT project And so I I guess the the the moral of that parable in in my mind is how far do you think we can really get with proprietary data sets proprietary benchmarks before the generalist models completely wipe the floor with them i Sorry to clarify I'm saying you use an LM. The process I'm describing of actually fine-tuning a model a a large language model for your specific context is basically adding more context You're seeing most of the LMS offer a paradigm where you can do this where you can add your knowledge corpus and train it to be more specific to your your individual context I I don't think the I don't think you'll see individual institutions building their own LLMs. I think that's a very compute intensive very difficult thing to do I think you'll see them tailoring the large language models to their context i Sure. Sure. To be clear I I i Oh, one more question if I may to to be clear wasn't asking whether you think every institution is going to get into the business of restraining their models I I was rather asking whether you think post-training, which is inclusive of supervised fine-tuning, reinforcement fine-tuning, a variety of other post-training, whether you think that has a long-term future or will maybe in 1 to two years we just use restrained plus post-trained generalist",
        "start": 1482.48,
        "duration": 3209.043000000001
    },
    {
        "text": "business of restraining their models I I was rather asking whether you think post-training, which is inclusive of supervised fine-tuning, reinforcement fine-tuning, a variety of other post-training, whether you think that has a long-term future or will maybe in 1 to two years we just use restrained plus post-trained generalist internal benchmarks and any internal data sets for post training Well, I think there are clearly going to be use cases where you are going to need to contact an individual company right Like if you just take the law firm example and just just take I mean just I'll answer it this way Um there are documents that company has on how they want um their dock their their future state documents for M&A agreement to look right And the the LLMs are not going to have that information So at some point you are going to have to see the post-processing layer happening at the enterprise And what we're seeing more and more is there's ways to design that layer so that you can as new models evolve kind of drop those in and we are seeing more and more folks experiment with that So they're using all the new teach that's being rolled out But i I think in fact what's going to happen is over time that edge in data is going to be the most valuable part of any company is that trade secret type of how do we do things Now at some point it may leak into the public models but for i like if you used open AI right frontier models connecting I remember we were talking to you know replete etch people are using it and then the the data is going straight into the cloud right and that's kind of dangerous we're going to have they're going to have to solve that layer in a very powerful way That's one of my predictions to forecast etch is is we're going to need to see a layer of protection between company data and the broader uh AI world i Um Matt, I want to make this a little more tangible Uh now I know you can't talk about the work you've done with the hyperscalers. Uh but you've identified I think five or six uh cases where you can speak publicly about it So, if you don't mind maybe we can toss a few of those in and then talk about them as concrete examples And and since Alex made his no financial involvement uh uh uh statement I will say I'm a proud uh adviser and am conflicted in a positive fashion uh uh supporting what Matt and Francis are doing So, but uh do you want to pick one of those I I I loved the example on uh on the basketball court Can you can you speak to that one i Yeah, sure So, um we worked with the Charlotte Hornets um on fine-tuning",
        "start": 1629.76,
        "duration": 3482.002000000002
    },
    {
        "text": "Matt and Francis are doing So, but uh do you want to pick one of those I I I loved the example on uh on the basketball court Can you can you speak to that one i Yeah, sure So, um we worked with the Charlotte Hornets um on fine-tuning prep for them So, uh in their case they wanted to look at the spatial movement patterns of players uh on a very broad scale across single point cameras across a whole host of different um uh uh college universities and and um international locations And so we fine-tuned a customer computer vision model um to specifically look at movement patterns they were interested in before uh before the draft And so that was a big part of uh their i this in English. You basically took the video and you were able to use models to evaluate every player uh based upon the video to see how well they performed at every different I'm not like a sports guy so it's the big round i that's becoming clearer actually i Yeah. Yeah. for me i Yeah, sure So, think of um if you take typical NBA stats they're things like points rebounds what's called plus minus is one ratio that's often used which is like the the amount you score versus give up when you're in the game but they're mostly stats that are um kind of transactional stats What they don't look at is the movement patterns of the players who create space uh who like where people are positioned at any point in time And that's actually a lot of the most interesting data if you go back to like you know some of the original baseball uh analytic that Billy Bean did for the A's. It's the movement patterns of players and who is in the best spacing right And there are companies that do this on on very consistent formats like on the same court Uh but we've been able to do is to do that over many different camera angles many different um stadiums very very quickly Uh and that is using custom computer vision models So we effectively are able to take a single point camera and understand the movement patterns of players uh in many different environments i and and so the Hornets use this how for see team selection player selection i right draft selection so to understand which players fit certain characteristics they were looking at i Fascinating. Yeah, it's a complicated problem too because chemistry between like it's not just about finding the best player The chemistry between players matters too It gets it gets infinitely complex and it's a cool little case study But you know Gavin Baker was saying recently that in fantasy football leagues all over the country which I used to love before I ran out of time Now, now I have to spend all my time keeping up with that",
        "start": 1768.159,
        "duration": 3753.7620000000006
    },
    {
        "text": "infinitely complex and it's a cool little case study But you know Gavin Baker was saying recently that in fantasy football leagues all over the country which I used to love before I ran out of time Now, now I have to spend all my time keeping up with that and having fun i But that's exactly No, not Now we're now we're obsoleting human obsoleting human sports leagues replacing them with robot sports leagues and sports i Yes. i Very just century not twinset i I'm betting on T800 again Yes, that's right i Yeah, but people are losing their leagues All you know great great fantasy football people are losing all over the place because the AI agent is tracking a huge amount of more detailed data And you know if if you look at the video footage you know somebody's like making it up and down court very slowly nobody's going to notice that but the AI will notice it in a heartbeat And then that just goes into the great model It's it's really a cool little case study Since we've asked a little bit about kind of if if a traditional business were we're thinking about how to do this I'll give a I'll give a slightly different one which is um lifespan MD which actually Peter I think this one will resonate you in particular which is i I know I know Chris who runs it Yeah. i Yeah. So, so so um Lifespan is a uh conciser medicine business and you can think of it as they have a network of practices both internationally in the United States, which all have very different sets of data on their patients um kind of practice information And so the the thing I always start with with any AI use case is you have to get the data right before you can even start with AI, you have to make sure that you have the structured and unstructured data together that you want So the first um the first thing that we're doing for them is on our data platform Neuron, we're creating a a HIPACO compliant multi-tenant cloud instance where we bring in together all the patient and provider uh data that's of interest And we start to bring both a 360\u00b0ree view of both the patient and the practice And so you can start to think of things like if you wanted to understand what longevity focused tests male patients 35 to 50 are using most frequently You can start to think about things like that on patient outcomes that are really interesting If you want to understand practice performance if you want to understand um where you have certain patients that are not compliant or not as interactive It's effectively just a control tower to understand everything that's going on across that footprints of practices And then I think the the area where generative AI has become more important for that is",
        "start": 1907.039,
        "duration": 4016.162999999999
    },
    {
        "text": "to understand um where you have certain patients that are not compliant or not as interactive It's effectively just a control tower to understand everything that's going on across that footprints of practices And then I think the the area where generative AI has become more important for that is people can ask questions knowledge management systems and and really interrogate and ask questions of all the key data from all of those practices One of the key things that's challenging about that is uh obviously in healthcare you have to be extremely careful about which data is stored locally at the practice versus how that's uh brought centrally And so the HIPPA compliant multi-tenant cloud is one of the key components of that is is actually making sure that no patient data leaves the premise of the individual practices and um doctors are able to access certain things and then certain practice metrics are are are organized centrally i I I heard the coolest thing this week Uh it's a a QA company that has invented talk to your defect It's just the coolest concept The defect actually has a personality and you can ask it questions about itself like where did you originated I I can totally imagine what you just said in healthcare being talk to your illness like have a conversation with it Where did you come from How do I treat are you are you getting better or worse if I do this thing And it's talking back to you with a personality It's just the coolest idea ever isn't it i It's amazing i It's one thing with the defect is a little awkward when you say \"Uh, here's this bacteria you're talking to You're like \"I don't know The defect is real I talk to your illness Maybe maybe gets a little weird I don't know what voice you would give it Voldemort voice or something i How do I kill you How do I dispatch you i Well, Dave, one thing I'd note there too is I think there's a question and I get asked this often of like how do you Peter, you asked earlier how do sectors evolved Like I think actually the question of does decisioning of visual patient care change with Genai is a much murkier question I think the easier place to start and I think where you know in many ways be very interesting is the US as an example spends about 13 to 14,000 per cap per patient per capita on healthcare right compared to 2500 to 3,000 per capita in say Germany or Canada i something like 30 to 40% of that is admit cost i and that is not admit costs that anyone wants to bear and so this is something where I actually think the idea that if is pursuing is not to change the stand actually to make the the position even more empowered but to take all of the",
        "start": 2038.32,
        "duration": 4281.043000000001
    },
    {
        "text": "admit cost i and that is not admit costs that anyone wants to bear and so this is something where I actually think the idea that if is pursuing is not to change the stand actually to make the the position even more empowered but to take all of the make that part i and AI should do a huge amount of damage in those areas i Exactly. i What are you seeing most companies get wrong on their mission to implement AI? i Yeah, I I think it's a couple different things I think um the first one is uh a lack of focus on data as the starting point So I do think it I do think the challenge if you just tried to um if you tried to build an AI agent on fragmented customer and product data it's going to break by definition right And so I think you do have to be in a place where the data you're going to feed feed into the models is is clear and working So I think that's been one one major challenge Um I think i do you think do do you think I mean if you had to look at companies as a whole in the medium and large size do they have clean data How long does it take a company to sort of get its data into a format uh and a level of fidelity that's useful I mean is this a is this a hard lift or an easy lift i It depends If you take the paradigm of I'm going to put everything in a data lake and get everything right which can take five years And the reality is most big companies have spent a decade trying to get all their major data schemas in order But I think if you start with the question of like what data do I need for this specific use case like um you know if you take um uh let's take credit underwriting like to do that well you need one uh you need a set of data around the credit itself the market you can probably have five to six kind of core data variables you need um uh kind of the core financial of the business um the the security of the credit all those kinds of core pieces of core information but you don't need every piece of data across the entire commercial bank to be right You need the core elements for that use case And so I think I think companies that are focused on the exact data they need to get right I think they've done pretty well But I do think trying to get all data like I mean you've also seen the enterprise for a long time Peter, if you asked any Fortune 1000 company to look at their full data repository and how much of it is accurate and working",
        "start": 2173.2,
        "duration": 4549.122
    },
    {
        "text": "think they've done pretty well But I do think trying to get all data like I mean you've also seen the enterprise for a long time Peter, if you asked any Fortune 1000 company to look at their full data repository and how much of it is accurate and working very few companies have that So I do think being very tactical about what data you need The other thing I think for Genai in particular is that a lot of the most important data is non system of record unstructured data So it's things like images videos text files It's just not things that people have tried to master historically And so I think the first step in this is saying what is the thing I'm trying to solve and how do I make sure I have that data ready i Yeah. i Yeah. One thing I see a lot of uh you know had a long board meeting this morning uh company that's very AI forward a portfolio accounting company called Vesmark and the data you know for account reconciliation for example the data is abundant but it doesn't tell you what the person actually does you know it just tells you how it was reconciled So now the path to success is first the AI assistant which helps accelerate you through the day but also knows what you're actually doing then that accumulates then that becomes the RHF or the training or tuning data because what you're trying to do is is like what are you doing guys and that's not really represented in the data but a lot of times you go talk to a bank or an insurance company and they're like our data is our advantage go ahead bomb it into the neural net and train it like what the I don't even know what that means you like I'm just going to throw terabytes of spreadsheet data in and see what happens Like that's going to go CLO on you like i well you have all sorts of other issues as well I was talking to the CIO of one of the biggest banks in the world and they have 300 different customer databases Okay. 300 one for mortgages one for loans one for this because the mortgage people don't want to tell the loans people about their customer data so they guard it jealously It's a total disaster for the poor CIO. i Oh, fascinating Alex. i Yeah, I I I think these are all very interesting points I I'd like to if I may be so bold jump up several levels uh and and maybe speak a little bit more about the the business model uh of Invisible. i My understanding correct me if I'm wrong Matt, is there's an element of the business I think it's called Meridial, that is sort of a marketplace for ML freelancers if if I understand correctly And I'm I'm I'm curious I I",
        "start": 2310.0,
        "duration": 4811.924
    },
    {
        "text": "about the the business model uh of Invisible. i My understanding correct me if I'm wrong Matt, is there's an element of the business I think it's called Meridial, that is sort of a marketplace for ML freelancers if if I understand correctly And I'm I'm I'm curious I I elephants in the room in this conversation is that we're arguably on the edge of recursive self-improvement. All of the frontier labs more or less I think would agree with the assertion that we're nearing the point where you could have an AI researcher where you just turn over compute resources to the AI researcher and the AI researcher does as good if not a better job than the human AI researchers who work for the frontier labs If if that is indeed the case uh surely one of the the the several elephants in this room but given limited time focus on this one I is that the need for a marketplace of ML freelance researchers to train models Doesn't that evaporate entirely as as we start to reach the point where AI researchers can can build custom models off of custom data sets and custom benchmarks for each client i Yeah. So, so to as you said we have two sides of our business One side meridian which um we train all the large language models and then on the on the enterprise side we build basically custom applications for enterprises Look I think um there has been a year evolution where I think consistently folks have said at some point you will not need reinforce learning human feedback to validate and test models And I think the the challenge with that logic is a couple different things one the the spectrum of expertise that if you take language multimodality um extreme expertise on things like computational biology and then the fact that a lot of these are reasoning tasks you do need and there's a whole host of studies on this that actually pairing synthetic and human data together is stronger but you do need human feedback on almost every different sort of agent you want to roll out and so I think the nature of RHF is changing so I think you're moving more towards things like RL gyms controlled environments simulations I think you're starting to see things like um much more of the expert work now is PhDs, masters So, it's less what I'd call commodity cat dog cat dog labeling But if you say tomorrow you're going to train a model to figure out um you know different evolutions in with century French architecture in French, you are going to want RHF to do that to validate it And I think that you're seeing that over and over is actually as as the models move more and more into very specific areas there is more and more RLHF needed for them i That's interesting It it it I maybe I",
        "start": 2444.64,
        "duration": 5109.204000000001
    },
    {
        "text": "want RHF to do that to validate it And I think that you're seeing that over and over is actually as as the models move more and more into very specific areas there is more and more RLHF needed for them i That's interesting It it it I maybe I be curious to to hear what you're you're seeing in in your version of the ground truth My my intuition my impression is that we're seeing greater and greater data efficiency in in pardon I mean our LHF was obviously sort of very fashionable over the past 3 years Maybe it went through sort of peak fashion if you will And then we saw the rise of reinforcement fine-tuning, alternative mechanisms that that maybe are far more data efficient and maybe even more human time efficient If you have to just build an RL environment arguably that's per human hour involved probably a lot more time efficient than staffing out to to to some so-called developing country folks to as you say cat dog cat dog do supervised fine-tuning or or or some other RHF type mechanism Surely and I'm I'm projecting my intuition is that you'd see more data efficiency not less and and therefore the amount of time effort money expended on RHF or or any sort of e even if we we buy your uh your assertion that we're seeing sort of hyper parochialization of lots of different tasks and each of them is going to need artisan annotation Surely there is a competing force which is increasing data efficiency from algorithmic efficiencies like like reinforcement finetuning. What are you seeing i Yeah, I mean people have been arguing that for 5 years but I think I think at least what I've seen on the ground is the the accuracy that you want um in the if you think about a reasoning task that involves a se a several step leap and you think about the risk of lose nations it is more useful to have human feedback involved in that in some form Right. And so I think I don't think that that means if you think in some ways RHF happens after all the restraining compute cost um it's a pretty small percentage of the total cost in training and it is some of the most valuable feedback and as you as you see more and more specific agents being trained for specific tasks like take legal services as an example if you train if you get a new legal services data set which is interesting and you want to train a model off of that you are going to want to see some sort of comparable equivalence whether it's an associate or or an M&A lawyer equivalent where you actually test if it works Now is it possible that at some point 10 15 years from now you run out of things to train on Possibly. But actually I mean",
        "start": 2595.44,
        "duration": 5402.1640000000025
    },
    {
        "text": "to see some sort of comparable equivalence whether it's an associate or or an M&A lawyer equivalent where you actually test if it works Now is it possible that at some point 10 15 years from now you run out of things to train on Possibly. But actually I mean modalities robotics is probably the next frontier of this in some ways RL gyms contact centers there's a lot I we are as a company fully a believer I can talk about this in the enterprise side too that human the loop is going to be a feature not a bug for a long long time And I think the the entire red herring of the enterprise for example is that autonomous agents will do all of this with no human salute I actually think you're going to need more and more humans at every step i Alex, you're saying that the level of intelligence of these agents as we pass through AGI and get to ASI is such that they'll figure it all out as good as any human and be and replace that human in the loop What's your timing on that i That was exactly my question Peter. So, so my timeline I if I had to spitball of course this is not the predictions episode so don't hold me to it Hold me to my my predictions in the next episode Um, my timeline are approximately 2 to 3 years for uh as a conservative outbound for for some element of recursive self-improvement where we get our AI researcher that's as good if not stronger than than the human researchers for building ML models as a conservative outerbound. i Not 10 to 15 years two to three years max Yeah, that's the outer outer edge i But I also believe Matt's totally right that that 2026 is going to be the year of recursive self-improvement and capabilities growing crazy exponentially and corporations moving at a snail's pace compared to what they could be doing And it's all going to be stuck and bottleneck and logjammed and it's going to frustrate the hell out of Google and open AAI and and companies like Invisible are the the lubricant that's going to actually get it from point A to point B. that Clark use case is a really good like in our tests for for contact centers 80% of the people massively prefer the AI but the 20% that don't like it more than torture the whole thing to death and make it better to repeal the entire thing there are probably eight ways to fix that quickly i y i but it's not going to come from Google and it's not going to come from open AI and it's going to involve data that isn't in the natural data set you know and and it It's right now if you told me two years ago that everyone in the world",
        "start": 2743.28,
        "duration": 5663.924000000005
    },
    {
        "text": "but it's not going to come from Google and it's not going to come from open AI and it's going to involve data that isn't in the natural data set you know and and it It's right now if you told me two years ago that everyone in the world will be three people who are multimillionaires who build RLHF companies walking around be like that's not even a thing Oh wait now it's not only a thing it's massive in scale There will be new terminology in 2026 for many many of these other bottlenecks that yeah the AI can do it but for whatever reason the bank is not doing it the contact center is not doing it And those bottlenecks are going to like they're going to be so lucrative for companies like Invisible to just plow them down I I can't answer the specific question of whether your workforce is going to involve the distributed workforce that you just described What was it called Alex or Matt? i Uh it's called Meridial. Meridial. i Meridial. Yeah. So there is a really healthy debate on whether is Meridial a key part of this or a network of even more agents a key part of this or is it you know is 2026 the transition year between the two It's going to be a really interesting foot race between those two different approaches But I but i that's really what I'm that that really is I I think you put your finger on it Dave. That that really is what I'm asking which I think is a distinct question from is there value in supervised fine-tuning or reinforcement learning with human feedback going forward Of course there is What I'm really asking is how much of that can come from AI sort of bootstrapping it in the near-term future versus needing human inputs A i and what I'm saying is think about a balance between generalizability and hyper specificity And I agree with you on on generalizability. I don't I actually don't think RHF is important even now for that But where where it gets more complicated is when you want to train off a specific task So let's take um let's take the insurance claim example that I I mentioned earlier right you're going to you're going to generate a 10 person page insurance claim And you could apply this to any enterprise use case many consumer use cases but in that world you build you know an an LLM is producing an outcome It's fine-tuned off a specific company's data but you need a way to actually say at that point does this produce a comparable output to what that claim did to what to what a human doing his task before was doing And so when I mentioned earlier custom benchmarks that is the process by which you do that is you actually do need human equivalence testing You need a human to provide a",
        "start": 2875.76,
        "duration": 5931.604000000007
    },
    {
        "text": "output to what that claim did to what to what a human doing his task before was doing And so when I mentioned earlier custom benchmarks that is the process by which you do that is you actually do need human equivalence testing You need a human to provide a looks good or it doesn't and you just don't have precedent data to train that off of in in um in any CLM because there's the human input is not now again that's going to keep going down more and more specific tasks If you take legal services take it by language take it by topic take it by document type there's human feedback required for all of that i I I I almost I mean not to put too fine a point on it but I I want to make sure that those in this uh in this episode who want to to drink the bitter pill with the bitter glass of water for the bitter lesson are are are so drinking I'm I'm curious Matt, to to understand how you see this Surely there's a wave of generalist that is over time and may maybe we can uh sort of finesse what the appropriate time scale is Sounds like maybe your your so-called timeline are are a little bit longer than perhaps mine but would you at least agree with the premise that over time even the specialized skills end up getting subsumed by generalist models or or do you think that's just never going to happen will always or by always I mean like on time scales of 10 to 15 years which is pretty long time scale We're just going to have generalist models that are always sort of like specially fine-tuned. I i I don't think all expertise all specialized expertise go right away No, I mean again if you think about a lot of that a lot of the information that specific experts have uh there's no training data available for that like it's stuff that sits in people's head it's experience like take I mean again I'm aware of many of the narratives that human expertise uh becomes less important again we are a company that actually thinks the human touch elements become more and more important but take you know take sales for example um many of the best selling patterns many of the people who've done that the best like there is no information you can train off of what they do they they live you know human interaction actually in in a world where you know there are 500 companies selling email-based SDRs I think human beings things become more important in that world So I I don't actually think special I actually think that the shifts are expertise becomes more and more important in many different areas I think human loop stays really important but I think the I mean if you take a contact center and",
        "start": 3011.92,
        "duration": 6204.804000000007
    },
    {
        "text": "become more important in that world So I I don't actually think special I actually think that the shifts are expertise becomes more and more important in many different areas I think human loop stays really important but I think the I mean if you take a contact center and what you're saying but like we're four to five years into this and if you look at the number of US contact centers that have migrated to using agents it's pretty small percentage C i can I ask you actually the the Jane Street question is really burning a hole in my pocket So, so it's really clear that stock picking is moving to AI at warp speed i And the reason is because there are no barriers like you're just placing a trade that's already automated So, it's like and that's the other i and there's a great benchmark i More money i More money has i Yeah. And also almost all of the volume on public equities markets has long since been dominated by ALGO. So, this happened decades ago i Yeah. Well, it's it started with rapid trading So, the quarts were already there So, now that it's moving to fundamental analysis it's the same mindset So, it's that's one of the reasons it's just taking off But, uh but you know like Peter said you know you're making more money Okay, let's just keep going then You know there's no there's nobody who's saying \"But I'm going to lose my job It's like \"No, we'll just pay you more Let's just got So, it's a really interesting you know bellwether But you know within that world they're struggling because uh the the data is so proprietary i and it's looking more and more likely that these self-improving massive foundation models are going to get to you know superhuman IQ this years This year being 2026 but the prompt window is getting massive and the recursive uh chain of thought reasoning is getting really really good So you can you can actually feed it data without having to retrain it and have it achieve the job So if I take that mindset you know from Jane Street and I move it over now I'm a mechanic and I'm trying to fix a car and trying to diagnose what's wrong with it and I have audio and I have you know sensor data Great, easy use case But am I going to then put that data into the LLM API and transmit it to OpenAI where they can accumulate it and then if they decide later they want to be a garage they they have all my data or am I going to run some kind of a walled off model and you know garage mechanics maybe not the best example that's why I chose Jane Street because they're never going to take their proprietary data and give it to OpenAI",
        "start": 3150.88,
        "duration": 6443.764000000009
    },
    {
        "text": "garage they they have all my data or am I going to run some kind of a walled off model and you know garage mechanics maybe not the best example that's why I chose Jane Street because they're never going to take their proprietary data and give it to OpenAI insurance companies you know hospitals like are how are they going to deal with this like it's easy now like sometime in 2026 it becomes easy but the data is is proper that's my only reason for having a competitive advantage I don't want to give it like over the to the API i yeah look I think you're seeing there are definitely sectors many of which you just named banking healthcare where um people are deciding to keep their data on premise or they're using things like small language models for those sorts of reasons and I think you may continue to see that as a trend um I think One mistake folks often make is not all data is proprietary So you can have you take the James Street case maybe their trading data is is proprietary but their you know back office kind of forecasting data might not be and so or back office uh finance data might not be and so I think one thing is being clear about the data that you don't that you need to keep proprietary and that you do want to take more uh parameters of of security around and then what data you say look this is actually I'm going to be very uh careful as a company but this is data that is not as as uh proprietary I think that sort of balance I think the whole you know similar to what we discussed with contact centers the idea of I will not give anything to the LM, but I'll keep it all unhouse I don't think that makes sense either but I do think that's a paradigm you're seeing more and more I think that yeah i um I want to kind of change the tack a bit if that's okay Um when I I actually do agree that we'll automated but I think we'll automate in a way that's different from this discussion So, let me give an example Let's say I'm Canon printers and I'm selling home printers right Right now I have a bunch of people doing marketing and content development brand management then salespeople to sell to the Best Buys and so on Uh and online folks then you have uh postpurchase getting the customer to try and register the dang printer And then you've got all the repair support technical staff and then you've got your accounting folks in the company financial management right you've got pockets of people doing different functions across the board Um, if I was going to build an AI native uh printer sales company then I might think about having all of those things automated completely with",
        "start": 3271.68,
        "duration": 6717.364000000009
    },
    {
        "text": "then you've got your accounting folks in the company financial management right you've got pockets of people doing different functions across the board Um, if I was going to build an AI native uh printer sales company then I might think about having all of those things automated completely with but you're functionentric across those The printer could report when it's you know running out of ink and you ship it a new thing You uh it tells you when there's a problem with it or there's a problem coming up you you alert your uh your repair staff saying \"Hey, this guy may maybe we can upwell him printer a a a day and you essentially automate all the functionality with AI and you leave the human 90% out of the loop almost completely because you've automated the core functionality And right now what I'm seeing is what I used to call radio over TV, right When you first had television we took radio announcers put them on TV to read radio scripts Okay? We didn't adapt for the medium And I think what I'm seeing right now is we're automating right now what the human being is doing at each of those functions but surely over time we're going to automate the functional flow and then get rid of the human beings completely i AI native AI first right i Not to mention getting rid of the printers i Well, that's a separate question that example Who's going to be doing any of the printing Let's leave that part aside just for the moment i I I think you're absolutely right See. I mean this is where a young AI native company imagines an entire field uh and has zero legacy and zero friction uh in coming forward The question as Matt said in the beginning is do they have the distribution right uh but this is where a large company canon in this case should actually be uh investing in entrepreneurs I mean one of the things that you and I talk about a lot of times is if I'm a large company and I don't know what to do uh I would basically hold a competition ask young AI entrepreneurs around the world to come forward and how would you disrupt my company uh you know give me a pitch and then I would pick the best five of them and I would fund them and I would say you know we're going to fund you to disrupt us and then you know we're going to give you access to our data to everything we have and then ultimately we're going to buy you or buy a majority stake in you and we're going to make you our new company right this is the innovation on the edge the displacement of the core et however you want to call it i this episode is brought to you by Blitzy autonomous software development with",
        "start": 3409.839,
        "duration": 6982.965000000007
    },
    {
        "text": "you or buy a majority stake in you and we're going to make you our new company right this is the innovation on the edge the displacement of the core et however you want to call it i this episode is brought to you by Blitzy autonomous software development with thousands of specialized AI agents that think for hours to understand enterprise scale code bases with millions of lines of code Engineers start every development sprint with the Blitzy platform bringing in their development requirements The Blitzy platform provides a plan then generates and pre-ompiles code for each task Blitzy delivers 80% or more of the development work autonomously while providing a guide for the final 20% of human development work required to complete the sprint Enterprises are achieving a ex engineering velocity increase when incorporating Blitzy as their preIDE development tools pairing it with their coding copilot of choice to bring an AI native SDLC into their orgy Ready to ex your engineering velocity Visit blitzy.com to schedule a demo and start building with Blitzy today i You're a medium-size or a largest company I'm not going to focus on the on the startup right now i And what do you do in 2026 cut you're going to have to do something You're going to have pressure from your board from your shareholders from from just competition i So, you got to do something And what I heard you say so far uh Matt, is number one you got to get clean data You need to make sure you understand what your data situation is Number two you should pick two or three uh if you would areas call them benchmarks where you're going to run experiments on Uh, and it's not a proposal it's not a you know an idea It's actually uh run it Actually do you know uh run an experiment uh to see how it works um what else Um and then pour scale pour money on the things that do work and then have an expanding sort of uh increasing circumference around the the company's major revenue engines How do you think about that Walk us through a few more steps Yeah. So I I think one of the things which has been a lot of the the topic of conversation here was given given all the improvements in the models given um you know when SL was walking through on the potential to clean sheet and design a company from scratch why has you know there was this MIT report that came out that i of enterprise models make right now make it to production right so I think there's a starting question of given all this teach excitement why has that been so much harder and it's not the technical challenges we've talked about it's the data it's the focus on which priorities to look that I think the other two big ones though are the organizational structure by which you",
        "start": 3545.2,
        "duration": 7290.801000000008
    },
    {
        "text": "question of given all this teach excitement why has that been so much harder and it's not the technical challenges we've talked about it's the data it's the focus on which priorities to look that I think the other two big ones though are the organizational structure by which you particularly the advice I give everyone is do not locate this in your technology organization take your best operator your best ops person give them an operational KPI and track it to that and make sure it's a really clear operational KPI so we talked a bunch about contact centers you should have an of an operational person there lead it around you know uh um siestas score time per call whatever the core metrics you're looking at and and that should be your guide If you want to take something like inventory forecasting you should do it around inventory days stockouts, all those kind of metrics But I think if you have a clear sense of which operational person is leading it and how they're marshalling resources around it and you have a clear KPI, you're going to make progress if you focused on a couple different things I think the the failure mode on that has been you let a thousand flowers bloom none of them have an operational metric and you kind of end up with a science project dynamic i Yes. i Exactly. That's exactly right It if if you walk in the thousand flowers boom you walk in and you say I am going to give you a million genius level people for free do something It fails i Yeah. It's like it's like here's a million people for free and they're all geniuses i and it it fails for that same reason It's like I didn't think of an idea so I said a thousand flowers just go bloom I couldn't think of anything so maybe you will like how's that going to work I've seen that You're exactly right It's it just it's just so sad You know i we go we go even further We basically say not just take the operator and put a put but put them outside the organization and let them build something from scratch at the edge because otherwise you get encumbered by all the internal rules and bureaucracies and that gets slowed down for a huge amount then it fails for for legacy reasons i Yeah. So the lock skunk works it's the Apple MacBook team i or Apple is actually a master at this If you think what Apple will do is they will form a small team that's very disruptive They will put them at the edge of the company They'll keep them secret and stealth and they'll say to them \"Go disrupt another industry right Whether it's watches or retail or whatever At last count I think they have 18 teams looking at different industries thinking about and when they",
        "start": 3704.799,
        "duration": 7551.0410000000065
    },
    {
        "text": "will put them at the edge of the company They'll keep them secret and stealth and they'll say to them \"Go disrupt another industry right Whether it's watches or retail or whatever At last count I think they have 18 teams looking at different industries thinking about and when they into it and they patiently iterated right Um, the Apple Watch for example Um, so this is the model I think we're going to see many other companies take on where you you do this and you because if you think of any operational company the insights they have on all sorts of adjacent industries is incredible Very hard to disrupt in their own industry because they're probably pretty optimized for it unless you come with the AI startup but they can really disrupt a lot of the edge case a lot of the industries around them So I expect them to launch AI native startups that go into adjacent industries and go attack some of their neighbors Nice. Matt, before we get to a few of your 2026 predictions can you just share a couple more of the uh the use cases here just because they're they're fun i So, we worked with um SEIC Vanter and the US Navy on building um intelligence for underwater drone swarm for under unmanned underwater vehicles So, think of that as if you have a series of drones and you have enormous numbers of sensors on each of those drones and you need to understand the movement patterns of those different drones Um, and in each case you see a differ you know you see an object underwater what do you do Do you engaged Do you step back Do you move with other drones That whole movement pattern and decisioning for underwater unmanned vehicles Um, that's what we worked on fine-tuning a model to do that training it looking at all the movement pattern data And again this is one of the most interesting things about drones is they are autonomous And so thinking about how those movement patterns evolve uh in complex environments is very very trick hard to do But you also have lots and lots of interesting sensor data to do that Um I think one that maybe anchors more on uh the human decisioning side is uh Swiss gear So like Swiss Army the luggage brand you know similarly and I actually think this is Peter one that um a lot of folks in the audience may relate to in some form which is you know they had enormous mix of different data tables around products customers etch that they couldn't really bring together for inventory forecasting And so we uh we used our data platform Neuron to bring together 750 tables really quickly and then optimize the forecasting to look at both minimizing knockouts and um optimizing which inventory to hold which you know if you get inventory forecasting right it's probably one of",
        "start": 3836.799,
        "duration": 7812.880000000008
    },
    {
        "text": "inventory forecasting And so we uh we used our data platform Neuron to bring together 750 tables really quickly and then optimize the forecasting to look at both minimizing knockouts and um optimizing which inventory to hold which you know if you get inventory forecasting right it's probably one of for most big and small businesses is you minimize loss revenue um you make sure that you don't hold lots of excess inventory It's one of the hardest things to do particularly if you've got a six to eighteenth order cycle time And so that was that was something we partnered with them on and I think was uh was a great outcome i We ended up expanding their overall inventory coverage by about i 30% and um basically ex the numbers of skis with a reliable prediction and again that was done in about in a couple of months i All right So uh later this week my moonshot mates and I are recording our 2026 predictions We'll have Emmod back and we'll be talking Each of us will provide two predictions for 2026. Uh we'll have our top 10 from the Moonshots podcast It's going to be fun Uh it's going to be a battle We're going to ask our listeners to vote on which predictions they like best I mean of course they're all going to vote for Alex's, but hey uh Matt, uh talk to us about what you see coming in 2026. Yeah, I think I I'll call out a couple and we've we've just done a bunch of research on kind of our 20 26 predictions so I won't I won't say all of them but I'll call out a couple Um, i I think one of the first ones I would I would anchor on is mufti went teams So, I think one of the challenges and it's inherent a lot of what we discussed about discussed here is if you're a large enterprise or medium-sized company implementing a use case you won't ne you won't necessarily have one decisioning agent that does everything you'll train task specific agents for individual tasks usually orchestrated by an LLM. And what that allows you to do is to pinpoint the accuracy on those specific tasks and then use the uh broader logic set of the LM to make sure they all work together properly And I think that's been an architecture that's been discussed pretty broadly for a while but I I don't I think that uh we're just starting to see the green sheets of more and more folks having success with that Uh contact centers being a good example Um so I think that's a big one that I would call out I think the other the second one I'll call out is the multivocal leap I think more and more video images audio are going to become a bigger and bigger part of how people engage with with these",
        "start": 3969.28,
        "duration": 8060.559000000008
    },
    {
        "text": "so I think that's a big one that I would call out I think the other the second one I'll call out is the multivocal leap I think more and more video images audio are going to become a bigger and bigger part of how people engage with with these the most interesting Um and so I I do think the the way you'll be able to speak to them interact with them visualize them is going to be a really interesting moment for 2026. And I don't think that will all be text based like it has predominantly historically And then maybe maybe one other Sorry, go ahead i No, I was I was gonna ask Alex for feedback Go ahead But but finish up man i Yeah. So, one the third one I'll call out because we've talked about it a couple bit on the on this episode so I'll is um kind of what we call either the mirror world or RL gyms So, I don't actually think that's a well understood concept for many folks in the audience but think of that as actually created him creating simulated environments to or digital twins for tasks you might want to test right So maybe that's a coding environment maybe that's a contact center as we've used that a couple times Uh but it allows you to actually simulate a series of function calls tasks or environments by which if you're going to train a model or a task you can actually test how it's going to work like a manufacturing environment before you roll it out to your actual physical world And I think that's more and more in both model builders in the enterprise What we're seeing is a a very interesting topic I want to go around to the mates one second maybe ask some uh some final final questions of Matt. Alex, you want to kick us off i Yeah, I I think the most interesting crux of what we're discussing here is what is the future of human expertise For for that matter does human expertise have a future And assuming it does what's the halftime of the it's cooked What what's what's the halftime uh of the value of human expertise And so so to put that in question format what do you think uh of of all of the the forms of human expertise of all of the labor categories and job roles that exist in the economy today What do you think will be the last three of those job roles or forms of expertise that will disappear or or ultimately succumb to AI? What are the last three to i last expert standing Okay, i that's right i I mean I I'll go back to where I started the episode I think a lot of the commentary on um mass shifts ignores the actual function of jobs in society today So if I go let's let's take",
        "start": 4095.119,
        "duration": 8326.400000000005
    },
    {
        "text": "three to i last expert standing Okay, i that's right i I mean I I'll go back to where I started the episode I think a lot of the commentary on um mass shifts ignores the actual function of jobs in society today So if I go let's let's take a lot of the functional expertise um you know geocismic if you look at seismic engineers people on oil and gas sites drilling that that is a human function like you do need so I I think uh real estate as another example like you know humans actually help select which You can go down a whole list of different areas I think there are sectors where you're going to see uh more disruption near-term. I called out a couple of them BPOS's, legal services I think media is a fastchanging area but I'm also not exactly sure that those are lead to negative like meaning have negative employment consequences Like if you take media it's a really interesting one You know five six years ago eight years ago I I think media as a category really struggled in a lot of ways for paid media as an example right And you've actually now seen in the last couple of years post DLM era Substack Medium all these blogs become much more interesting You have way more media entrepreneurs and so you've changed the function of society and like where the money is coming from changes but it has not changed total employment You know look I I understand a lot of skepticism that says that you know AI is going to radically change everything but I think if you look at the an American society for the last hundred years it's something like 25% of every high school class uh goes into a field that did not exist when they were in high school And and and the the reason that persists is people go into the working world understanding the tools they have thinking out about what they can create from that And you know one of my favorite statistics that's the the Wall Street Journal reported last last like a couple weeks ago is 20% of uh US employment right now is digital ecosystem jobs And something like i i of um US citizens are full-time social media influences which is mindbogglingly to me But but i yeah i but you know again these this is the changing nature of work And so I think that pattern will persist I think that the core of what will change is the process of looking up information across multiple systems and documents is you're going to race that that is going to become less uh valuable But I think all the jobs that involve human interaction physical work physical like I actually think one of the most interesting things over the next couple years is the job ecosystem around data centers electricians etch is going to become",
        "start": 4230.08,
        "duration": 8611.840999999999
    },
    {
        "text": "that that is going to become less uh valuable But I think all the jobs that involve human interaction physical work physical like I actually think one of the most interesting things over the next couple years is the job ecosystem around data centers electricians etch is going to become sitting I was on the panel Oh yeah I was i I was on a panel with with um a recruiting someone who runs a recruiting company They were saying that job profile they think will will two three ex over the next couple years And so that will have pretty interesting implications for the education system and everything else But I I don't I think we will see an evolution i I meant the human the humanoid robot electrician and and plumber Alex uh very quickly what are your three last standing uh human roles here i or your last i interesting so I I'll present multiple competing hypotheses uh so hypothesis one to i briefly uh one hypothesis is it's the politician because they they help to make the laws i another another hypothesis is that it's the greatest intellects the physicists or mathematicians even though as we talk on the pod math and the sciences are are all getting solved On the one hands there's still perhaps to the extent that that represents the the culmination of human intellectual accomplishment maybe the greatest intellects will be the the last to be automated There there's another school of thought that says not it's it's the roles that involve the greatest need for human authenticity because even though it's not actually a capabilities question people nonetheless demand human contact or or something to that effect And so it's going to be the highest touch job roles where people just want to know that there's a human counterpart on the other side of the interaction So that that's a set of three hypotheses i Taste makers will will dominate i that That's authenticity bucket number i That's what Mike Sailor said word for word actually i Yeah, we had on his on his boat we had that enjoyable sunset conversation i Thank you Alex. See, do you want to go next on a on a closing question for for Matt? Um I think you covered some of it on the industries that are um kind of going after you've had some you guys have done some government work Where in government functionality do you see the biggest opportunity for AI uh automation uh efficiency etch i Yeah. Um everywhere i Look I I actually think this could be one i I think this could be one of the really positive trends for society So um I saw a study recently that AI assisted permitting could cut energy and um data center project implementation timeline by 50%. Um think about uh housing like one of the biggest challenges right now for housing development in the US is",
        "start": 4376.0,
        "duration": 8910.240999999998
    },
    {
        "text": "of the really positive trends for society So um I saw a study recently that AI assisted permitting could cut energy and um data center project implementation timeline by 50%. Um think about uh housing like one of the biggest challenges right now for housing development in the US is to build housing because of the myriad of different regulations and zoning contracts by location Right? or even saying I the OECD came out with this thing that or came out with this report that um AI could shrink public sector process cycle timeline by 70% on licensing benefits approvals compliance and basically accelerating infrastructure deployments So to me the simplest thing that AI can do is project management and timeline related to all spending and infrastructure deployments is would be a really positive thing for society in my mind Amazing. Good question Sim Dave, why don't you close us out on the questions here i Oh, I got so many but I'll pick I'll pick the best First, Matt, uh how many hours of video footage will there be of you one year from today compared to one year ago Because I know we saw each other in Riad a few weeks ago and and I know that you you are the thought leader in this whole bottleneck of AI getting into the enterprise It feels like what we're doing right now you know the footage of you that's out there right now is all this CNBC Bloomberg type you know five minute format But here we're getting your real thoughts It's just so much better But how many hours can we count on a year from today i Well, look I think as of 12 months ago I had done almost no interviews of any kind So, this job has been fun in that front And uh look I I what I enjoy about the COD podcast format is it does allow you to talk about um some of the more complex topics And so I you know particularly a podcast like this that's really interesting So hopefully many more uh in in the year to come i Well, I'm hoping for at least a six on that And then my followup question to that is uh the Avatar version of you that's also out there talking is that a 2026 thing you think or when i Yeah, that's probably happens in 2026. I don't think it be that hard to train an avatar off of uh my public statement So I, you know I think that's be an interesting We are actually working um in the sports space actually on the topic of avatar training And I think it is actually an interesting space where um you could imagine a lot of different areas where rather than a chariot interaction people want to speak to people they know via an avatar that might get I actually think that will become a more natural part of society",
        "start": 4527.92,
        "duration": 9164.96
    },
    {
        "text": "I think it is actually an interesting space where um you could imagine a lot of different areas where rather than a chariot interaction people want to speak to people they know via an avatar that might get I actually think that will become a more natural part of society i I totally agree I just the timeline is could be you know as soon as two months as far as I'm concerned i Think it's not an avatar we're speaking to you right now Dave, i that's a good question i That seems very human actually I don't know The best ones are i the green orbs behind you kind of give it away i Yeah, they are pretty they are pretty strange i That's not real i Matt, where do people find you Where do people find Invisible? Uh who should go to Invisible to check out what what you do and how you do it i Sure. So, we have uh seven offices now Uh New York, San Francisco, Austin, Texas, DC, London, Poland, and uh Paris. Um I'm the easiest to find probably We have an office right off of uh Union Square. um uh which is where I'm at least half the time when I'm not on the road And and look I think in in terms of who who should come to us and from the listener base in particular any madcap um or or or enterprise company that is that knows there is potential in their business that knows that AI can transform in a positive way and is struggling to bring all the pieces together I think that is the the main thing I would say is there is no doubt you know everything's al Alex is asking the technology has made an enormous step change over the last couple years The hard thing is actually the change management the operationalization, the metric tracking the evaluation It's it's kind of bringing together like you know I think it's the difference between the our founder Francis has a uh an idea of you have all the components to build a cake but you don't have a cake like what we do is we actually bake the cake in the end We build you something that works We make AI work and we use all the modern tools to do that i Amazing. And uh the website i uh invisibletech.ai. All right Thank you Matt, Salem, Dave, uh AWG. I'm going to see you guys in a couple of days for our 2026 predictions Um, make them brilliant It's going to be It's going to be fun i Uh, all right i I want to benchmark for trapping tracking benchmarks i That's your All right i All right i No, that's not the one I'm going to talk about i Okay. i All right guys Have a great day Every week my team and I study the top 10 technology meta trends that will",
        "start": 4657.679,
        "duration": 9422.399999999998
    },
    {
        "text": "to benchmark for trapping tracking benchmarks i That's your All right i All right i No, that's not the one I'm going to talk about i Okay. i All right guys Have a great day Every week my team and I study the top 10 technology meta trends that will ahead I cover trends ranging from humanoid robotics AGI, and quantum computing to transport energy longevity and more There's no fluffy only the most important stuff that matters that impacts our lives our companies and our careers If you want me to share these meta trends with you I write a newsletter twice a week sending it out as a short two-minute read via email And if you want to discover the most important meat trends 10 years before anyone else this report's for you Readers include founders and CEOs from the world's most disruptive companies and entrepreneurs building the world's most disruptive techs It's not for you if you don't want to be informed about what's coming why it matters and how you can benefit from it To subscribe for free go to dmmandis.com/tatrends to gain access to the trends 10 years before anyone else All right now back to this episode",
        "start": 4788.719,
        "duration": 9523.121
    }
]