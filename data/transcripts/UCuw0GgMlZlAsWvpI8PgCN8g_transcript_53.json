[
    {
        "text": " [Music] Welcome to Zcats everyone. I'm CS Garval from ZK Research and today we're going to be unpacking how AI data centers are evolving and the technology that's enabling them. While I specifically be looking at networking and while networking makes up several critical components, this discussion will focus on the network's role in creating what Nvidia CEO Jensen Wong often refers to as the new unit of compute. We're going to dig into new optics, the ability to scale across AI data centers, partnerships, and the need to to be open as well as how to accelerate adoption. And so today joining me will be Bob La Liberty, principal analyst at the cube and as my co-host as well as a special guest Galad Shiner, SVP in networking for Nvidia. So welcome gentlemen. U Bob, how you doing? And a little intro on yourself. Do doing great Zas. Thanks so much to having me here. Really looking forward to this conversation and Golad thanks for joining us as well. and GL's on time so we don't really need much of an intro to him but you talk about your role at Nvidia networking Nvidia really interesting set of products there so what's your role and and what do you do there thank you for inviting me and and happy to be here I've joined Melanox many many many years ago and started actually as a ASIC designer and was involved in the design of the first Infiniban adapter that was created. later on I participated in multiple developments both hardware infrastructure software and then few years back with the acquisition of Merlox join Nvidia and focusing on building the infrastructure the infrastructure that actually create the AI data centers. now you mentioned networking and and networking is so much you know it's like a large last century word to describe what we do. it used to be networking because essentially in the past u the networking devices or their their mission was to move data from one side to another side. today it's it's it's not really networking anymore. It's actually a computing infrastructure. It's something that takes or or t takes those compute A6 and help to form a supercomputer. So it's not just moving data, it's it participate in the computing activities. It's actually drive elements that new and were not being part of the infrastructure before. So we don't really call it networking but it's actually computing infrastructure. and this is what we're doing. Yeah. And well, you've certainly got networking in your blood or compute infrastructure if you want to refer to it that way. before we we get into some of the networking changes though, I'm curious about just the overall state of AI and as AI becomes mainstream and we evolve all everyone's going to have to evolve their data centers. Can you talk about some of the limitations of networking and why it needs to evolve in this way?",
        "start": 0.04,
        "duration": 427.07899999999995
    },
    {
        "text": "the networking changes though, I'm curious about just the overall state of AI and as AI becomes mainstream and we evolve all everyone's going to have to evolve their data centers. Can you talk about some of the limitations of networking and why it needs to evolve in this way? talk about AI we are talking about distributed computing workloads and that's something different than a traditional cloud workloads for example. In in traditional cloud workloads, most of the workloads are actually a single core workload, a single CPU workload, you know, single server workloads and therefore those cloud hypers scale cloud systems are system that packs a lot of servers and actually have a network that its network mission is to connect the end users to the servers in order for the servers to run that mission or to run the workload. When we talk about AI and we talk about distributed computing, it's a multi-server workload and it's not just few servers, you know, we can talk about hundreds of thousands of servers of compute engines that need to work as a single unit. And therefore, building an AI data center is actually building a supercomput. And a supercomputing mission is to combine multiple compute A6 and make them work as a single entity as a single unit. And therefore an AI supercomputer has been designed completely different than a traditional supercomput. And actually it requires multiple infrastructures that surrounding those computing AS6 in order to build that supercomputer and we talk about a scale up computing infrastructure that its mission is to connect those compute A6 and to form one GPU right that's why we talk about a Rex scale GPU right it's like the REC is a single GPU and it's all because of the scale up infrastructure and then when you form that single rack scale GPU you need to connect multiple of those together and they all all need to work in a in a very symmetrical way and they need to be completely synchronized between themselves because essentially it's running one job across everything and this is where you bring a skill out infrastructure that its mission is different is to take those wrecks and actually create a large data center or one unit of computing and then because of physical and power limitations you may need to connect multiple data centers together to form a gigawatt you know a gigas scale supercomput and this is where another infrastructure being created and this scale across it's different elements that do that and you need to have an access network and so forth. So in a traditional data center single server workload there's actually one network connects to the servers and that's how it's built. It's kind of closer to a server farm, an AI data center. It's building a supercomput and that supercomput can be actually located in multiple places, but it needs to be work or behave like a single unit.",
        "start": 214.64,
        "duration": 857.9590000000005
    },
    {
        "text": "actually one network connects to the servers and that's how it's built. It's kind of closer to a server farm, an AI data center. It's building a supercomput and that supercomput can be actually located in multiple places, but it needs to be work or behave like a single unit. because you touched on I think some some things where Nvidia has really helped is helping people scale within the rack, right? Scale up, scale out within across the data center. And then you introduced this concept of scale across and can you talk about a customer scenario where they would have to where scale across is the really their only viable path? Yeah. definitely. So when when you build a data center you have two given elements. One of them is how much power you can utilize in that location. And sometimes it's al also a physical constraint. You know what's what's the space that you have and those two elements will determine how much compute you can bring or build in that data center. how many GPUs you can build or what's the size of an AI supercomputer. Now with with the growth in the requirements from both AI training and inferencing, we see the scale that goes beyond what you can build in a single what you can put in a single building, what you can build in a single location. Which means is that in order to scale your workload, you will need actually to connect multiple buildings together, multiple sites together. Those sides may be far away and you know it could be buildings within a campus, it could be between campuses, it could be between cities, it could be between countries but you do need to run across everything because you need the combined computing capabilities of all those data centers to support a single workload. And in order to do that we designed a new infrastructure which is scale across that actually makes the data center work as a single unit. You can think also about other cases you know there is entity that would like to use for example renewable energy and therefore they're going to build data center where they can have renewal energy. So that's another example where you're going to have data centers in remote locations, but you do need them to work as a single unit. And as we scale and the demands from the workloads increases, you will see more and more entities and more and more of our customers actually leveraging scale across and leverage the the spectrum XGS internet for that scale across infrastructure to actually form a data center to data center connectivity and larger mega scale AI factories. and Golad that makes so much sense and obviously adding that now instead of just scaling up and scaling out now that concept of that ability to scale across especially for organizations looking to take advantage of sustainable energy sources right renewable energy sources",
        "start": 432.479,
        "duration": 1219.3180000000007
    },
    {
        "text": "and larger mega scale AI factories. and Golad that makes so much sense and obviously adding that now instead of just scaling up and scaling out now that concept of that ability to scale across especially for organizations looking to take advantage of sustainable energy sources right renewable energy sources other. So having that is certainly going to add another dimension and it also you know it's great to bring out now for organizations who are starting to scale their environments to be thinking about that ahead of time and where they want to be and give them options for where their AI data centers can go. and you know obviously like I said a lot of this driven by energy. So I wanted to touch upon another area that's rapidly evolving and that's around the the co-acked optics optics or CPO. How does that CPO perspective, how does that change the economics and energy profile versus today's pluggables, especially in those large GPU fabrics? Yeah. So, one of the things that you are aiming to do or you want to do when you design a data center is to build it in the most efficient way. So, you want to improve its power efficiency. it wants to improve its energy efficiency. The more power that you can save, the more computing that you can bring in and therefore you want to leverage all of the technologies that enable you to reduce power consumption. One example and before touching co-op package optics is usage of copper. Copper is zero power. So as much as you use copper, as long as you can use copper, you can save power and you want to use copper as much as you can. And what we are focusing in the scale up infrastructure in NVLink is to pack as much compute as we can inside the rack so we can focus the scale up or envy link infrastructure on copper. And that's why for example we building systems building our computing in Rex with liquid cooling so we can increase the the compute density. We're working with our ecosystems and our system design team to be able to make the rack more efficient and actually increase the compute density and that increase the capabilities of anvilic and so forth. So this is one area of focusing on util utilizing copper because it does gives you the best power performance that you can think about. Now once you maximize the wreck and and maximize the scale out infrastructure then you need to go to scale out and scale out need to drive longer distances and this is where corporate cannot be used and essentially using optics. Now in the traditional data centers most of the work was inside a server. So the connectivity between the rack was not that much. So there is optical connections but that optical connections that consume more power than copper obviously was not a big factor",
        "start": 615.519,
        "duration": 1606.4380000000006
    },
    {
        "text": "used and essentially using optics. Now in the traditional data centers most of the work was inside a server. So the connectivity between the rack was not that much. So there is optical connections but that optical connections that consume more power than copper obviously was not a big factor there is actually a lot of bandwidth that needs to be driven in the scale out infrastructure and as GPUs progress from generation to generation and the infrastructure progress or or advance from generation to generation there is more bandwidth that is needed on a scale out. The amount of power that is consumed by the optical connections in a scale out infrastructure can reach almost 10% of compute which actually it's a big number. So if we can save power on that scale out infrastructure we can increase the compute cap capability of that data center. And in order to do that, we've introducing cop package optics and we announced the spectrum x Ethernet copage optics and we announced the quantum x infinib cop package optics and and the motivation or the idea is very simple. When you when you have an optical network, you have switches connected to transceivers and then transceivers connect to fiber that goes between them. there is a dense distance between the transceiver and the switch for example. So you need to invest power in order to transition an optical signal to electric signal and go through multiple transitions of cages and PCBs and and and packages and so forth until it gets to the switch. This is where this is where you can optimize. So with spectrum maxer and photonics for example on on with copage optics we've moved the optical engine and put it in the package with the switch reduce the distances reduce the power that you need to use for driving the signal and with that we are able to reduce the power consumption of the scale out optical infrastructure by 3.5x. So for the same ISO power of the scale out network now you you can actually connect 3x more GPUs it's a huge number. Now there is other great side effects quote unquote because the design that we have done with a ecosystem of partners and there is a huge amount of innovations and work with multiple partners TSMC's on packaging and other vendors on laser sources and optical arrays and so forth. First, we created an optical engine that is very efficient unlike traditional attempts out there that were focusing on MCM designs which results in a large optical engines that will limit the radics of the switch. We design an optical engine which is very very small. It's micro modulator design. It's very small so it can actually support large radic switches which are important for the scale out infrastructure. we work with TSMC on packaging for example we work with partners around laser sources and working with them we",
        "start": 811.36,
        "duration": 2029.3970000000008
    },
    {
        "text": "optical engine which is very very small. It's micro modulator design. It's very small so it can actually support large radic switches which are important for the scale out infrastructure. we work with TSMC on packaging for example we work with partners around laser sources and working with them we compared to traditional aspect by 4x. So it's power reduction mean it's reductions the number of components that you need in order to build a data center. So the resiliency is 10x improved the power consumption is 3.5x less in that sense and this is another aspect of improving performance or power performance performance per watt and be able to bring more computing into data centers and to make them more efficient. Awesome. That was that was great, Glad. Thanks so much for that. And certainly highlights a significant number of benefits going with those co-ackaged optics optics. So hopefully everyone who's listening is going to be able to make sure they're starting to factor that in as they start to scale out their environments as well. Yeah. Glad and you use the you used the two terms you talked about power and energy and I find often in this industry people you know conflate those two terms and there is a difference between power efficiency and energy efficiency and so can you talk about how you define the differences and what KPIs operators should be prioritizing between those two? Yeah. Well, first first both are important. Okay. Let's start from from that side. power efficiency is measured by performance per watt. Okay. So you want to improve the performance per the power that you consume. And for example, co- package optics is a technology that enables you to improve performance per watt. using copper for our env scale up it's improving performance per watt it's improving the the the power efficiency of the system liquid cooling does the same thing right so those are example now when we talk about energy efficiency we're talking about what is the total energy we need to invest in order to complete the task when you're running training when you're running inferencing it it takes time you know it can takes days, it can take weeks. The question is what is the energy that you need to invest in order to complete your task. Your data center can be very power efficient, but it may take that data center twice the time to run a job and therefore it will be very have very poor energy efficiency. Okay, you want to improve on both. So for example, the reason that we created Spectrumx Ethernet is because offtheshelf Ethernet infrastructures that existed were not built for distributed computing workloads. Okay, they were built for traditional clouds, single server workloads. They they were built or designed for heavy virtualized enterprise. They were designed for service providers. They were not designed for distributed computing. So if you were using an offtheshelf",
        "start": 1024.799,
        "duration": 2412.6779999999994
    },
    {
        "text": "infrastructures that existed were not built for distributed computing workloads. Okay, they were built for traditional clouds, single server workloads. They they were built or designed for heavy virtualized enterprise. They were designed for service providers. They were not designed for distributed computing. So if you were using an offtheshelf could easily take you 40% 50% 60% more time to complete a job. And that means that your data center energy effic efficiency will be very poor. with Spectrum X Ethernet that actually is designed to support distributed computing. It's designed to eliminate jeter. It's designed to to have very highly effective bandwidth. It designed to make sure that all of the computing engines works in simultaneously work in in and get data at the same time. that can save a lot of time in running the workloads. So we demonstrate for example reductions almost 40% of running training when you're using Spectrumx Ethernet versus using offtheshelf internet. And with Spectrumax Ethernet it takes you less time to run a job. It takes you less time to run training or running inferencing which means we're optimizing the energy. So both are important. One is performance per watt. The second one is energy per job and we want to optimize them both. Okay, thanks for that. That makes sense. Yeah, Golad, I wanted to touch base on something else. There's been some chatter recently about proprietary AI fabrics and so forth. I'm wondering if you could highlight how Nvidia supports multi- vendor interoperability in practice and and maybe discuss any of the deployments that you you have today. Yeah. Well, let's let's look on that. So, first when we look on the scale out infrastructure, we have QuantumX Infiniban and Spectrum X Ethernet. Both of them are fully industry standard. Quantum X is built according to the Infiniban specification that are managed by the IBTA consortium. And then Spectrum X Ethernet, it's Ethernet. So it's it's a full internet connectivity. It's interoperable with any internet device that exists. So that's from an interface perspective. now this even the software that runs on top of that the operating system that runs on top of that scale out infrastructure it's also open source. So you can actually run the Sonic open-source operating system on top of the SpectrumX iterate infrastructure and it's important and it's important for us because with that open-source capabilities it means that our customers can innovate on top of that they can bring their secret sauce into the software that runs on top of our infrastructure and customize it to the workloads that they want to run. When we look on on scale up infrastructure, we announce envels with an industry standard interfaces to any accelerator to any CPU that exist and therefore any accelerator or any CPU can connect to any scaleup infrastructure including Enveling Fusion and be and with that we're enabling people actually to utilize the best infrastructure for their usage.",
        "start": 1220.4,
        "duration": 2833.4770000000008
    },
    {
        "text": "infrastructure, we announce envels with an industry standard interfaces to any accelerator to any CPU that exist and therefore any accelerator or any CPU can connect to any scaleup infrastructure including Enveling Fusion and be and with that we're enabling people actually to utilize the best infrastructure for their usage. Ethernet and Quantum X is Infiniban both are industry standards and have open source running on top of that. Envy link which is an amazing infrastructure and it it's it's actually it's not easy to build a scaleup infrastructure because it has it needs to support huge amount of bandwidth and loaded store operations and take those GPUs and make them one and very low latency and high message rate. There is so much innovations that needs to be brought into a scale up infrastructure and then with enabling fusion we actually enabling anyone to use that amazing scale up and they they can bring their own accelerator their own CPU their own combination of accelerator and CPU. So the the interfaces are standard. There's lot of open-source software that runs on top of that and we enabling our partners and customers to completely innovate on every aspect of what we build. Yeah, I think that go I think yeah know I think one of the the big proof points though for the openness of what you're doing is the is the ecosystem that you have around you and specifically can you talk about that but specifically around Cisco and the partnership there I know you know when people think of networking Cisco's got more domain knowledge than you know maybe anybody in networking has ever had and that's become a very important partner for you so can you talk about that, you know, the ecosystem approach, but then also the relationship with Cisco and how you're leveraging their you networking and software to you to expand the reach and adoption of of what Nvidia is doing. Yeah. Well, you know, we we we love the ecosystem. We love the ecosystem. Working with the ecosystem enable us to run very very fast. Okay. AI is being progressed very very quickly. You see the progress of technology you see the scale of the data centers and and working with the ecosystem enable you to actually run very very quickly and progress very quickly. Cisco is a great partner. You know, Cisco invested so many years in building infrastructure for the enterprise and they have great software capabilities that they have built to support enterprise clients. and and as we see that AI is going to enterprise, the combination of Nvidia and Cisco is like match made in heaven. Okay, if I'm if I'm going that path, , by working with Cisco and bringing our SpectrumX Ethernet infrastructure into Cisco, Cisco can leverage and offer the performance of SpectrumX Ethernet for enterprise AI on one side. On the other side, they're running their entire enterprise software on top of it.",
        "start": 1439.039,
        "duration": 3223.3160000000007
    },
    {
        "text": "if I'm if I'm going that path, , by working with Cisco and bringing our SpectrumX Ethernet infrastructure into Cisco, Cisco can leverage and offer the performance of SpectrumX Ethernet for enterprise AI on one side. On the other side, they're running their entire enterprise software on top of it. use the same Cisco software, the same enterprise capabilities of Cisco they are used to and they enjoy and now it's actually accelerated with the spectrum X internet underneath for the AI enterprise data centers. So it's a great partnership. It's Cisco bring their enterprise expertise. We bringing the spectrum X performance and together we're building an amazing solution for the enterprise AI bill. I just wanted to go back to something that we had talked about earlier. You had mentioned the scale across now being available. If I'm a customer and I want to get started, what's the most pragmatic path for them to do? Is there reference architectures they should be looking at? Is this something that they have to design from the get-go or can it be integrated into existing locations that they have in order to scale across to a new one? Yeah. So, first the way that we design things is that we design a single unit. Okay. The the the unit of computing today is the data center, right? It used to be a CPU. It used to be a server. Now in the world of AI, the unit is a data center. And therefore, we design a data center. And when we design a data center, we design it in the best way that we think that we will want to consume it ourselves. Okay. The idea is that we're building a data center for our own usage. And that's why you want to build the best thing you can build. And after we do that, we offer that as a reference architecture and our customers can take the entire design. Our customer can take pieces of that design. So they can take the GPU and they can take some other infrastructure. They can take our envelope. They can take pieces of that if they want to and combine it with other. So it's it's there is all the options that exist here on the table. Now, Scalic cross is as you mentioned it's a new infrastructure and it's also part of that reference architecture that we have created and if you want to leverage that reference architecture and and enjoy the capabilities of Spectrum XGX Ethernet then you can take the reference architecture and actually you will be able to enjoy from this the same performance level that we're enjoying now spectrum XGX Ethernet connects remote data centers together. So it is supported by the variety of long reach options that exist. If you have built a connections, a dark fiber connections between remote data centers, you can have the Spectrum X Ethernet on both sides. Spectrum XGS Ethernet runs",
        "start": 1636.799,
        "duration": 3599.9560000000006
    },
    {
        "text": "now spectrum XGX Ethernet connects remote data centers together. So it is supported by the variety of long reach options that exist. If you have built a connections, a dark fiber connections between remote data centers, you can have the Spectrum X Ethernet on both sides. Spectrum XGS Ethernet runs new algorithms for the long reach congestion control and long adaptive routing and and distribution and you can use that if you are if you have a DWDM infrastructure that has multiple devices in in between you can also have the spectrum XGS on the endpoints on both sides and that will run across a different or DWM infrastructure. So you can use it on existing infrastructure that you have. You can use it on on a new infrastructure that you built and it's going to work and optimize itself according to the distance and according to the medium that exists between those remote data centers. Excellent. Yeah, that sound that sounds great. I'm really looking forward to hearing about from the, you know, some of the early adopters and not only how they're doing it, how far they're going, things like that, especially if they're they're leveraging the DWDM infrastructure. Cool. Thank you. Yeah. Well, well, we we did announce Core Weave by the way is the first customer of us that is going to use Spectrogs and there is many others that will will announce when it time. Yeah. Yeah. No, and thanks a lot. I think you know what's interesting here is that we've seen AI really you know redefine almost every part of the IT stack right and I think the we're just starting to see the evolution of the network now and it's great to see the innovation that Nvidia has done so I think that's probably a good place to wrap up we talked about co-ackaged optics you know the data center and scale across and what the whole definition of the new unit of compute as well as some partnerships and openness and so glad is there anything else you want to No, first thank you for the time today. it was really an interesting discussion. you know the cadence of innovation is fast. Okay. When you when you look ahead we're an annual cadence of technology innovation. Every year there is a new infrastructure being created a new data centers that is being built. , so it will be great to talk with you and continue and discussing new innovations and I hope that we'll do it soon because with the cadence of innovation tomorrow they're going to be new device, new infrastructure that will be created. Yeah. Well, you can already, you know, scale up, scale out, and scale across. You got nowhere else to scale now. Oh, we'll see. Yeah, I guess scale down something like that. So, all right, Glad. , really appreciate your time. Bob, you as well. Thanks for joining me today. , I think",
        "start": 1831.76,
        "duration": 3928.115999999999
    },
    {
        "text": "can already, you know, scale up, scale out, and scale across. You got nowhere else to scale now. Oh, we'll see. Yeah, I guess scale down something like that. So, all right, Glad. , really appreciate your time. Bob, you as well. Thanks for joining me today. , I think know, how AI is evolving data centers and and networking. And, , I'll also include , if there's a link that you have to those , blueprints, I can include them in the YouTube description below. So, on behalf of Gage Shiner from Nvidia, Bob Liberty from the Cube Research, I'm Zas Caraval from ZK Research. Thanks for watching. , give us a like and also hit that subscribe button. I'll see you next time on my next episode of ZCAST. [Music]",
        "start": 1998.72,
        "duration": 3989.2349999999988
    }
]