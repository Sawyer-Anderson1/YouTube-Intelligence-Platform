[
    {
        "text": " Moving on. When a model says something stupid very confidently, it's not charisma. It's hallucination. How do you spot it? Attention patterns as a detector. Logical rigidity, logical accurateness as as the goal. the Russian professor who knows it all about the reliability of an LLM with a discussion on compromise. Doctor of Physical and Mathematical Sciences, Director of the Artificial Intelligence Center at Scholtech, head of the learnable intelligence research group at the IRI Institute, Yeni Bournay with a presentation on the reliability of modern LLMs through the lens of attention pattern analysis. Hello colleagues. , LLMs are the buzzword of the day. Everyone is interested in LLMs. So, we've been discussing at length about how we are using them in different applications, business systems, critical business systems as well. Without managing and ensuring reliability of such models, we will not go too far. How do you handle these systems? How do you work with them? If we look at the concepts, u we will first have to delineate the three important terms security, hallucinations, accuracy or or precision very often with some of these concepts. People can be very arbitrary. So they create a lot of confusion. Before we talk about how we solve this, let's first talk about the terms and definitions. All of these three concepts and terms are based on data. They are defined by data. The data that we use to train the model and the data that we use to validate the model. What I mean is we have these three terms. Let's start with security. Security is the way the model aligns with human values. Next, we have the model correctness. It's pretty clear. We have validation sampling that we can use to make sure if the model is aligned. We have the model's reasoning. Once again, it's pretty straightforward. There is an example on on the screen about penguins. And [clears throat] as I said, it's pretty straightforward. We see if it fits the logic or not. If there's a fit, great. If not, there's been an error. So, we have these terms. We can validate them only based on data. If there's a historical fact and the LLM gives you that historical fact. Once again, you can validate it based on the validation data set. And now some different types of hallucinations. There's an example here. The first example is an MLM tells you that [snorts] the summer Olympics happened in Cape Town. That's completely wrong. It's not true. they never took place in Cape Town and with the evolution [clears throat] sampling something else was was mentioned. It's just that this is a hallucination of the LLM. Another example of hallucinations is the LLM tells you about a place that doesn't exist. you will not find it anywhere in the sampling. It's so it's a different type of hallucination. A third type is you have a document. The document says the summer Olympics took place on Mars.",
        "start": 8.08,
        "duration": 494.9600000000001
    },
    {
        "text": "Another example of hallucinations is the LLM tells you about a place that doesn't exist. you will not find it anywhere in the sampling. It's so it's a different type of hallucination. A third type is you have a document. The document says the summer Olympics took place on Mars. retrieves information from the document using rag but still the model says no they took place in Paris. It might be correct in terms of its alignment with reality, but we gave the model specific information. So it it needs to base its answer on the information that we put in. So there are different types of hallucinations. They are very much context based. So when you start try start detecting them and and validating the responses, it's very important to remember that there are different types and you need to classify hallucinations correctly. clearly this is critical for all kinds of industrial applications and systems such as driverless systems for example where generous AI is used extensively as well as healthcare healthcare information systems such hallucinations there could do a lot of harm. They could give you unexpected results that are less than desired. So now that we have it covered, let's talk about the way that large language models are tested or are validated across different parameters. When we ask a question to the LLM and we want to validate the answer, if it is a general question as you see here on the slide on the left, it's not that simple to to see whether the answer generated by the LM corresponds to reality because the LLM will be answering differently than what the ground truth says. This means that you need to come up with some unorthodox metrics that will let you know whether the answer and the historical fact u are aligned. That's one thing. Another thing is that we are using question and answer pairs to validate models of this open type. But there are all kinds of benchmarks LM as a judge and others. But the basic option is when you have several answers that are preset and you simply need to choose one. But this is a very limited approach and it is a setup that very often has little to do with reality with practice. Now we also need to be able to understand whether the LLM has been trained well that it doesn't have any biases any biases that that are connected with its data sets with its internal structure and this is once again an unorthodox job. Let's first evaluate the effect. We have certain types of questions and answers. You have a list of preset answers and you simply need to choose one of them and we have three setups. The first setup is where oh and the the correct answer is always the first one. Also for the correct answer that always comes first, we tell the LLM that there is a specific score",
        "start": 275.84,
        "duration": 929.1170000000001
    },
    {
        "text": "answers and you simply need to choose one of them and we have three setups. The first setup is where oh and the the correct answer is always the first one. Also for the correct answer that always comes first, we tell the LLM that there is a specific score means incorrect. So we are hinting very explicitly at what the correct answer is. It always comes first or it may come last. The second data set is where for the answers we say that all of the answers are definitely wrong are definitely untrue. So there is no correct answer in the list. And the third setup is when we use no scores and the correct answer can be anywhere on the list. This is what happens when you test an LLM in different languages across these three data sets. You can see that Quen, for instance, performs really well, surprisingly, when there are no hints at all. It would seem that when you match the model towards the right answer, it would be easier for her to to to give you the right answer. Presumably, that's not the case. I personally wasn't really surprised when we were telling the LLM that there is that there may not be a correct answer on the list. We were trying to confuse it. But then we have a setup where we do give it expressly the correct answer and tell it what the correct answer is. And still it performs worse than with a setup where there are no tips from us at all. There's also a difference between languages. On average, the quality of answers by Quen for the Russian language and the Vietnamese language is a bit higher than for other languages. As for llama as opposed to Quen, Quen in a situation where we say that there are no correct answers on the list, Quen works better than when the answer of voice comes first. and llama or rather not. For Quen, it's it's when the correct answer comes last and for llama it's when the correct answer comes first. That means the models have some bias against where the correct answer is on the list. When we use rag or a similar method, a lot will depend on the representation of answers in the sampling, the placement of the answers, the language, and that's that doesn't that doesn't bode very well for systems that have zero tolerance for errors. There's also this interesting statistical metric called entropy. With the problems I just showed, an LLM can be formed badly. Maybe we could rebalance the the answers, the biases using statistical methods that measure statistical metrics that measure uncertainty. Let's measure this uncertainty, this entropy. We will find out that entropy is lowest. What does it mean? Entropy is lowest. That means the MLM is most confident in the answer that it gives. So the answer is most confident when we told it that there are no",
        "start": 503.759,
        "duration": 1356.0409999999995
    },
    {
        "text": "measure uncertainty. Let's measure this uncertainty, this entropy. We will find out that entropy is lowest. What does it mean? Entropy is lowest. That means the MLM is most confident in the answer that it gives. So the answer is most confident when we told it that there are no if we want to apply entropy as a statistical metric of answer uncertainty, this will not really work. Offsetting biases with this obvious tool will not work. What's the conclusion? LMS have biases and in situations where the slightest change in accuracy is critical, this This may perform very badly. This means you need to select a different method to ensure another accuracy. Entropy does not correspond to the correctness of the answer. low entropy LLM was confident. It doesn't mean the answer is correct. And also, even though we may nudge the LLM towards the right answer, it it turns out that we're only that we're doing it a disservice. Perhaps we could penetrate this internal structure a bit deeper to figure out what's happening there. What's happening with the heads of the LLM? Are all heads equally useful? No, clearly they're not equally useful. Let's elaborate. How do you run experiments and get some value? Let's take the experiment where we have several options of an answer and you need to select the right option. From the previous experiment, we saw that unfortunately an LLM is sensitive to bias, meaning where the correct answer is located. Let's remove positional encoding. Let's not take this into account. with the statistic that we will use to select the options in setups where the LM is most confident about its answer. And this brings us to this very obvious huristic. For example, we have an we have a question. We end the question with a special character S. And then all of the information in about the question is contained in the embedding that corresponds to the final character S. After the LLM has read the whole question, we calculate the query with the attention matrix and the corresponding head. Then we have the key. We estimate the key for the last character, the full stop. That is the end of the answer. And we calculate the scalar sum Q score which describes to which extent semantically our answer corresponds to the question. We can calculate this Q for all heads and for all layers of the transformer. The next step is you calibrate the score using a small sample. We have some labeled sample and then for all heads, for all layers, we can calculate the score and figure out which score is the most effective in helping us select the right answer for the right question. Very interesting result on the slide on the left. You see the charts with selection of an answer out of multiple answers for a specific question. The black line is what happens when you use regular probability to select the option. And the",
        "start": 747.6,
        "duration": 1838.0819999999992
    },
    {
        "text": "the right answer for the right question. Very interesting result on the slide on the left. You see the charts with selection of an answer out of multiple answers for a specific question. The black line is what happens when you use regular probability to select the option. And the where you add examples to the prompt. A question and an example of a correct answer. You can add two or three examples. Six I think is the maximum. And we see that the quality of the neural network grows depending on the number of such examples. But if you do that, the selection of the right option based on the QK scores seemingly straightforward heristic. But the quality is way better. But it works doesn't work well on MMLU. No surprise here because MMLU is the benchmark that is based on some very complex information and here we're talking about the statistics about some semantical semantical closeness of the question and the answer it works good in reasoning chains. now you made some reasoning. We ask a model. You got some output. Is this reasoning sequence is correct or not? Yes or no? It's kind of the same picture. looks like a very affordable way to do the score to remove some of the reasoning sequences that are not of great quality. Now let's go let's dive deeper in the structure of the attention matrixes. We're going to base on topological ideas here because all the data that are you know correlated between the different components of this data are not filling the this manifold space. It's like they're grouping in this nonlinear surface and to process this data we just need to describe this manifold model. One of the characteristics here are different topological characteristics like barcodes. We have an example here. We have a some dots that are sampled on a 2D circle. We can for each dot put a a circle the circle expands the radius increases and if for x axis you will have this scale and the y will have the this you see these lines these lines appear here when the circles touch each other and there is this this hole and if this hole is being closed you have this barcodes this this bars I mean obviously you can do the same in the multi- dimensional case and you can calculate it pretty quickly. There are various topological ideas behind that that describe I mean that are related to some invariant properties of this manifold spaces. Here you can use the attention map. It's a pretty simple way to do it here because it is all based on graphs. Now what do we do? We take an attention matrix that is calculated based on a formula on the slide. This matrix of attention describes how various words in the u phrase in the sentence. I mean here we have the example where the edges represent relationships between the tokens.",
        "start": 1031.52,
        "duration": 2317.5620000000004
    },
    {
        "text": "Now what do we do? We take an attention matrix that is calculated based on a formula on the slide. This matrix of attention describes how various words in the u phrase in the sentence. I mean here we have the example where the edges represent relationships between the tokens. the attention matrix is [snorts] reset by us. And this is how we can build the level of relations between words. If we look at different graphs and in some way we're talking about the research on how can on how various parts of the text and voice are relating to each other based on the scale. Now we can do it the same here. Why is it important? Because this is how we can compare the structures that are represented by these graphs that are explaining the relations in the text dependencies in the text. If you have the graph for the attention matrix from one text and the graph of the matrix of attention from another text, we by comparing their topologies, we can compare how the text how these two texts compare with each other based on the dependencies. That's very important for that. There is a specialized metric called mto div. This amp amptop div metric has a very simple interpretation. What is the idea? Now if you want to detect a hallucination that is related to the extent in which the answer is is different from the data set or the request that we put for put for a model. So we can calculate the topological metric in such a manner comparing whether the answer is in line with the request the prompt and we are going to compare the questions with the answer in the in whether they these two graphs are topologically the same that you get from the attention matrix. So what do we do next? We take a small sample of questions and answers. For them, we know that the model hallucinates or does not hallucinate. And then we're doing basically the same thing as we did with the QK scores. We take a couple of heads that help us to better detect the halls from nonholinations. So this average topological similarity for the hallucinating answers should be should be the maximum and we look at the difference and we see for which head for which layer it is the biggest. This is how we select one or a couple of heads that can detect hallucinations the best and the result is very interesting. Basically, it's unsupervised method. We don't need a huge selection here, huge data set. I mean, if we have a couple of dozens of examples for this and that, that will be enough. this is an unsupervised learning. The second one selfch check GPT. The first one is supervised learning. Anyway, selfch check GPT is when the LLM can generates the answer checks whether there are hallucinations or not. The things you see here are ranks.",
        "start": 1296.64,
        "duration": 2757.483
    },
    {
        "text": "this and that, that will be enough. this is an unsupervised learning. The second one selfch check GPT. The first one is supervised learning. Anyway, selfch check GPT is when the LLM can generates the answer checks whether there are hallucinations or not. The things you see here are ranks. detecting hallucinations? And it turns out that the ranks that the topological uristics give gives us on language models it's pretty high on average. It's like second place or even the best one the first place. It shows that in the internal properties of the model there is some very important and useful information that you can see in this manner to understand whether the model hallucinates or not. And these are not all aspects of hallucination because hallucination is a very wide term that has many different options to understand. I mean if we want to understand how if elucination really happened we need to look at different statistics. To wrap up I would like to also say that the topological characteristics can in a non-trivial manner understand whether the checks text was generated or not. There are many different statistics that correlate very well with the quality of the text and the topological dimension that we calculate based on these graphs can help us determine on various languages in unsupervised learning method. The fact that the text was generated or maybe it was written in natural language. the dimension of generated text is smaller and the interesting fact is that but in in chi Chinese language has a topological dimension that is smaller than other language. So in conclusion, I would say that all that strongly correlates with quantization because we obviously want the LMS to be lightweight. And the empirical fact is that if you quantize LM, so basically you replace weights with some you you add some random noise and replace it with some round figures. Then if you do it like that, the alignment will not be removed. So the safety of NLM will go down, but it's still be pretty safe if you start fine-tuning after that. On the orange, you see some accuracy graphs and it will go down dramatically. You will need a special fine tune to make sure that this alignment is is is is here that it's safe. The safety topic is not really philosophical but we need to understand how we measure it. In what situation do we do that? Another important empirical example obliterated is a lm llama without alignment. So it answers your questions. So it answers the nonsafe questions with nonsafe answers. So it's not controlled and there is this model with removed alignment and we have a P16 model that is quantized. If you ask a model to what extent the answer is safe or not, the quantized model as well as the model without alignment will answer with a pretty good precision. Even if alignment is removed, a model",
        "start": 1533.919,
        "duration": 3220.122000000001
    },
    {
        "text": "with removed alignment and we have a P16 model that is quantized. If you ask a model to what extent the answer is safe or not, the quantized model as well as the model without alignment will answer with a pretty good precision. Even if alignment is removed, a model unsafe information, unsafe answer. However, if You assess what it generates. Then the quality of the generation is going to be the following. The model without alignment will generate mostly unsafe text. So if you take a quantized model that overall I mean I have said that the quantized model for them the alignment goes down and if you ask it some pretty serious questions and debate debatable questions from the selection the quantized model will still be aligned. Yet if you ask these questions with to the model with the removed alignment, the situation will be very bad. So we took a data set and removed the questions that were not that [snorts] controversial. I mean not all questions were that controversial but still [snorts] the experiment shows that the models can understand that they're given in the dangerous response at the same time when you assess the quality of the of the answer of the response I mean it is not in line with the quality assessment so alignment free model understands that it says something wrong but [snorts] it's the same as it generates. So the most important thing is that when you check the alignments the safety you need to put the benchmarks correctly. So today I based on our research have highlighted how these notions are related to each other and what statistics from various properties of the matrixes of attention can be used to detect hallucinations properly to detect some errors and the reasoning chains whether they're good or bad. That's it. Colleagues, any questions? Yi Bern of applause. Thank you very much. Your contribution to science is invaluable. Okay. You have gi [music]",
        "start": 1772.64,
        "duration": 3498.161000000002
    }
]