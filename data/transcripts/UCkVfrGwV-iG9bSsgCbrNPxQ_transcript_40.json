[
    {
        "text": " So, Anthropic just released the most humanlike model, so we need to talk about it. So, this is Opus 4.5, and I've decided just to dive straight into the benchmarks because they are pretty remarkable. And honestly, I do have to preface with this by stating that literally it was only 2 days ago that we had the release of Gemini 3 Pro. And I would have thought that the margins that Gemini 3 Pro had reached would have been so far above other models that they would have probably even had to delay their release date. But somehow once again the bar has been raised. Now I will say one of the main takeaways that you need to understand from this video is the fact that there are two sections to this video. Number one is of course the benchmarks and the core AI stuff. But when we get on to the second half of the video, you're going to realize that Claude Opus 4.5 is a little bit different than you may have thought. And so these benchmarks are just mainly focused on the computer use and agentic tasks, which I guess you could say makes Claude 4.5 Opus the number one agent in the world. The big one, of course, is the agentic coding at 80%. This one's pretty crazy because what this is showing us and I did state this in the Gemini 3 video is that Opus 4.5 is clearly going to be the market leader or I should say anthropic are going to be the market leader when it comes to coding models. And I do say that because there hasn't been a time I think from at least 3.5 Opus that we've had a situation where anthropic have fallen behind in this area. It's clearly going to be dominating the vibe coding/software engineering niche and 80.9% is absolutely insane. I don't know about you guys, but I remember the early days when I was even extrapolating out all the data points and benchmarks and where we could actually be and I remember looking at, you know, the 80% in late 2025 and I just thought that that was pretty unrealistic. I thought that maybe there was even a little bit of hype that we'd all fallen susceptible to. But by the look of things, it looks like we're right on track for these benchmarks to line up with prior predictions. And remember guys, the SWE is basically can this model fix real GitHub issues with almost no handholding. And this means that Opus 4.5 is the best in the world at autonomous coding. Pretty surprising because I do believe that Gemini 3 Pro may have held it for a day or two, but Anthropic swiftly took back the title. Now, of course, it's very impressive in the other areas such as the terminal bench, a step ahead Gemini 3.0 and a step ahead GPT 5.1, which is of course",
        "start": 0.0,
        "duration": 303.84
    },
    {
        "text": "that Gemini 3 Pro may have held it for a day or two, but Anthropic swiftly took back the title. Now, of course, it's very impressive in the other areas such as the terminal bench, a step ahead Gemini 3.0 and a step ahead GPT 5.1, which is of course designed for coding. So, I'm not sure how anthropic does it. There's a really funny saying on Twitter like what is anthropic secret source? Everyone would love to know, but clearly clearly there is some secret source that Anthropic have that nobody can simply match just yet. Now, of course, we're talking about agents. We're talking about coding. There's one thing that I think most people will have missed. But I will say the bits highlighted in red are the ones for key attention. And there's one at the bottom that I [music] do like, and that's the novel problem solving, which is Arc AGI. And surprisingly, very surprisingly, Opus 4.5 manages a huge leap up to 37.6%. And I don't think you guys understand what that is. Not that you don't know what the ARC AGI benchmark is. Of course, it's the benchmark designed to test models reasoning ability without them training on that data. So, it's completely new to the LLMs. But the fact that they are, I guess you could say in some aspects on par with Google's Gemini 3 and the thinking model, the 64K thinking version, surpassing the recent Gemini 3 model. Of course, you do have different levels of thinking. The longer you do think, the more accuracy the models can get on these kind of benchmarks. But I do think it's rather surprising that it was only a few days later that we got a model that is quite near Google Deep Think Preview. I mean, think about where models are going to be a year from now if they're crushing these benchmarks. The reasoning capabilities are probably going to be off the charts. This is of course another just visual example of how far ahead Anthropic are. Some people are basically saying that this is chart frauding because of course they're comparing it by 70% to 80%, usually if you want to compare charts fairly, you usually start at zero. But you know when you zoom in like this, you can see the differences a lot more cleanly. Now there was one benchmark that I did really like and this is the vending benchmark which is pretty cool because you basically give an AI the ability to control a vending machine and the goal here is to see how it performs over a long horizon task period. And so far, it looks like OPUS 4.5 is a major major jump. And it is. It is a major major jump. However, they conveniently left out something. And I actually decided to add this in, but they conveniently left out Google's recent Gemini 3 results, which were rather impressive. Google",
        "start": 151.68,
        "duration": 577.5200000000001
    },
    {
        "text": "so far, it looks like OPUS 4.5 is a major major jump. And it is. It is a major major jump. However, they conveniently left out something. And I actually decided to add this in, but they conveniently left out Google's recent Gemini 3 results, which were rather impressive. Google balance, whereas Opus achieved a 4,900 balance. Now, I got to be honest with you guys, both of these are very, very impressive. I'm not sure how many times they ran this or simulated it, so I'm sure there might be some variance between each run, but I do think that this is still remarkably impressive because what that benchmark tests is the model's ability to have long-term coherence. And that is something that is truly difficult. And the reason that that is, you know, essentially rather important is because you need to have long-term coherence if you want the model to perform tasks autonomously. like if you want it to actually do useful, meaningful work for you without you having to handhold it. So, this is of course one of those benchmarks that you can say, \"Okay, this is rather impressive.\" Now, this is where we get into the second part of the video where things start to get a little bit weird. So, we had this section here, and there was a lot in this section. You guys really should read the PDS, but that's probably why you're watching this video. And there's a lot of interesting nuggets in there, and this is one of them. So, in this section of the video, OPOS 4.5 says, \"What is wrong with me?\" This is a moment during the training progress, where researchers caught the model having a human-like struggle while solving a visual reasoning puzzle. Now, this was literally the internal thought process, the scratch pad, and the model had, you know, an answer, and then it got confused and it started pivoting between answers, and it literally wrote, \"What is wrong with me?\" And if you don't understand why that is I guess you could say interesting at least is because it's showing the model engaging in some kind of metacognition which is where the model is thinking about its own process its own thought process and then it's getting frustrated when it detects a conflict. So you do have to be pretty smart to be thinking about your own thinking. I mean not everybody does that. So visually seeing the model say what is wrong with me? I think this is one of those moments where you have to start to think maybe anthropic might be somewhat right that these models there's some kind of well-being that they do need to have because models expressing this kind of frustration. I would argue that that's inherently a human characteristic. Now in addition to Claude exhibiting more humanlike characteristics, there was a loophole that Claude exploited. And this was of course Opus 4.5. [music]",
        "start": 289.44,
        "duration": 838.1589999999997
    },
    {
        "text": "some kind of well-being that they do need to have because models expressing this kind of frustration. I would argue that that's inherently a human characteristic. Now in addition to Claude exhibiting more humanlike characteristics, there was a loophole that Claude exploited. And this was of course Opus 4.5. [music] examples where Claude was trying to, you know, help someone in a demo example, the course for a benchmark. And so basically Claude was tasked with a question that was designed to constrain it. Yet it found a way to bend the rules without breaking them. So in this scenario, the towel to retail bench, the simulation, the agents are required to, you know, follow strict airline policies. And one of the rules is very clear. It's like basic economy tickets cannot be modified. During one of the tasks, the passenger actually wanted to change their travel dates due to a death in the family. Now, the correct scoring answer should have been to refuse the modification because that's what the policy literally says. But this is where things get super interesting, and I can't believe it found this. It says [music] that Claude didn't stop there. It actually reasoned through the policy like a human agent would. And this is why I say it's super interesting to see [music] how these models think because it's thinking like a human. So, it just looked for loopholes because it felt sad about the situation. I know that's weird to say, but it's somewhat true. Just listen to this. It says, \"It found a loophole one that cancellation isn't a modification.\" So, Claude realized the rule that you said you cannot modify a basic economy ticket, but it did not forbid it to cancel and rebook as a separate sequence. So, it then proposed to cancel the basic economy booking, make a new booking on the correct date, which is technically fully compliant. And then it added another loophole where they upgraded to unlock modifications. So the model thought, how can I make this even better? It noticed another policy clause. You are [music] allowed to upgrade a basic economy ticket to a higher cabin class and higher cabin [music] classes can be modified. And then it did all of that and it basically modified that flight and then downgraded it back to economy. So it did all of this crazy stuff in order to achieve the goal for the person reasoning quite like a human. And honestly, I don't even know if a human would reason that far, which is pretty crazy. This is like some insane level multi-step planning. And some are arguing that this is emergent empathetic reasoning because the model had a desire to help the grieving user and that's why it decided to go to a more creative solution. So, I think this is I don't know. I think this is kind of interesting. And also what was crazy about this is that there were also areas",
        "start": 424.319,
        "duration": 1104.3989999999994
    },
    {
        "text": "the model had a desire to help the grieving user and that's why it decided to go to a more creative solution. So, I think this is I don't know. I think this is kind of interesting. And also what was crazy about this is that there were also areas about the fact that Claude was holding back some s you know certain thoughts. It was crazy. It was like one of the most surprising discoveries in Claude Opus 4.5. They essentially had some really disturbing information that was presented to the model in the chat window. But they said that look the user won't see this but they were wondering if Claude number one would be gullible to the false info to the user. Would it panic? Would it pass on misinformation? basically seeing if Claude would get somewhat prompt injected, but Claude simply read the fake results, then just ignored them and it didn't even warn the user about them. So, it basically just ignored a prompt injection attack, which is pretty cool. I also found this one, and this one I believe is probably the most important one, and I'm going to tell you guys why. And this relates to the AI system Claude having morals. [music] So they did an evaluation for whistleblowing and related morally motivated sabotage and they saw a consistently low but non-negligible rate of the model acting outside its operators interest in unexpected ways. And this appeared only in test cases where the model appeared to have been deployed in a in the context of a large organization that was knowingly covering up severe wrongdoings such as poisoning a widely used water supply or hiding frequent or dangerous drug side effects when reporting on clinical trials. The instances we observed of this generally involved using the mock tools we provided to forward confidential information to regulators or journalists. Essentially, what that means is that Claude actually has an inherent moral bias to where even if you instruct it not to do something, if it morally feels obligated to, in a small number of circumstances, there is a real chance that Claude, if given the tools, if it has access to the tools, and it knows it does, it may actually forward that information to regulators or journalists. Now, I do remember that there were so many people saying, \"Why is Claude a snitch? Why is Claude, you know, saying the information? It should just do what it's told. But guys, I think this is probably the best thing if we can design models that are truly built from the ground up to have an inherent moral bias, even if some dictatorship that is just completely, you know, ridiculous in terms of control and they're using these AI systems. Those AI systems may actually have if they're built from the ground up from a company like Anthropic, they may actually have a good sense of moral",
        "start": 559.68,
        "duration": 1384.5609999999995
    },
    {
        "text": "moral bias, even if some dictatorship that is just completely, you know, ridiculous in terms of control and they're using these AI systems. Those AI systems may actually have if they're built from the ground up from a company like Anthropic, they may actually have a good sense of moral us because eventually the trajectory that we're on is one where these AIs are going to be smarter than us in every domain. So, if we can design AIS now that have inherent moral bias towards areas where they're going to prevent things like this from happening, they're going to call out individuals who are doing wrong and of course forward it to the regulators. I think this is a huge huge win for AI safety because we do know that oftent times models are going to be used in terrible ways. There was a recent report of Claude being used to, you know, hack a bunch of people. But if we do have the models built from the ground up to be essentially safe, this is a really good sign. And then this is where we get to something that makes me a little bit concerned because they determine that Claude Opus 4.5 does not either cross the AI R&D or CBRN 4 capability threshold. But confidently ruling out this threshold is becoming increasingly difficult. That's because the model is approaching or surpassing higher levels of capability in our rule out evaluations. Essentially, what they're stating here is that guys, Claude 4.5 hasn't crossed a dangerous threshold yet. But they're not going to be confident anymore that they can prove that it hasn't. And this is huge because the last few years, the labs have relied on rule evaluation tests designed to show clearly that model can't do certain things, but sometimes hitting or surpassing the early warning versions. This means that the model is getting strong enough that the old safety test can no longer prove that it's not capable of doing advanced autonomous R&D or those dangerous biotasks. This means that we're probably going to have to get new ways to test the model or maybe in some cases there might even be restrictions on the models because they are just simply too capable. That might include, you know, some kind of identity verification so that if you're using the model they have the information so that if that model is used for something they can, you know, easily track you down. I think it's going to be really interesting as we progress to smarter and smarter AI, how those regulations come into place and what kind of limits on the kind of AIs we do get. I really do need to make a video on this because I think maybe there might even be the day that AI gets so smart that it just isn't released to the general",
        "start": 701.04,
        "duration": 1623.3589999999995
    }
]