[
    {
        "text": " This week in AI, Open AI reportedly hit a code red after Google's Gemini 3 climbed the charts. Leaks surfaced about a secret OpenAI model called Garlic. Apple quietly changed how AI searches long documents. Microsoft pushed near instant AI voice responses. Chinese labs showed live avatars that can run for hours. Tencent released a fast video model that works on consumer GPUs. And Google revealed a new memorybased system built for massive context. Lux crossed a line by letting AI control real computer screens. JIPU released GLM 4.6V as a fully open- source multimodal model. Mistrol shipped Devstrol 2 for real world coding. A Tokyo startup claimed it built an AGI capable system. Disney officially licensed its characters to OpenAI. The US military rolled out its own generative AI platform. And in the middle of all that, OpenAI released GPT 5.2, two, a major technical upgrade that somehow triggered skepticism instead of excitement. So, let's talk about it. All right. First, Open AAI quietly hit a breaking point this week after Google's Gemini 3 climbed to the top of the LM Arena charts. Sam Alman reportedly declared an internal code red, signaling real competitive pressure inside the company. Almost immediately, leaks followed about a new model OpenAI has been building in secret, internally called Garlic. According to internal comments from chief research officer Mark Chen, Garlic is already outperforming Gemini 3 and Anthropics Opus 4.5 in reasoning and coding during internal evaluations. The interesting part is why it works. OpenAI went back to the earliest stage of pre-training and rebuilt it to focus on broad conceptual structure first, then layer in fine detail later. That change unlocked higher performance in smaller, cheaper models. A clear response to rivals like DeepSeek, Mistl, and several Chinese labs that have been pushing surprisingly capable lightweight systems. Garlic runs in parallel with other internal model lines, showing how aggressively OpenAI is iterating. Timing is unclear, though as soon as possible likely means early next year. While OpenAI was in scramble mode, Apple quietly released one of the most technically impressive research systems of the year, Clara. Anyone who has used AI to search long documents knows the problem. Modern systems grab huge chunks of text, shove them into the context window, and hope the model figures it out. That approach works, yet it gets slow, expensive, and messy as documents grow. Apple threw that entire idea out. Clara compresses documents into extremely small sets of memory tokens that preserve meaning while stripping away redundancy. Instead of pulling thousands of words for every query, the model operates directly on these dense representations. What makes Clara stand out is how it was trained. Apple jointly trained the retriever and the generator as a single system. So both learn to reason inside the same compressed space. Most rag pipelines keep those parts separate. Apple trained Clara on around 2 million Wikipedia passages using synthetic multihop question and answer and repeated verification loops to clean",
        "start": 2.56,
        "duration": 355.68000000000006
    },
    {
        "text": "Apple jointly trained the retriever and the generator as a single system. So both learn to reason inside the same compressed space. Most rag pipelines keep those parts separate. Apple trained Clara on around 2 million Wikipedia passages using synthetic multihop question and answer and repeated verification loops to clean performs exceptionally well even at high compression levels, sometimes beating full text retrieval pipelines, while using far less input. Apple also released multiple versions and the full training pipeline, which quietly signals much bigger ambitions in the LLM space. Now, Microsoft focused on something completely different, latency. Its new Viveoice real-time 0.5B model tackles one of the most noticeable problems in AI assistance, the awkward pause before speech begins. This system can start talking in roughly 300 milliseconds, which makes interactions feel immediate instead of robotic. The key idea is simple and smart. Speech generation begins the moment the language model starts producing text rather than waiting for a full sentence or chunk to finish. The model is designed to run side by side with an LLM, streaming audio as tokens arrive. Despite being relatively small, it stays stable over long conversations and maintains voice consistency without drifting. quality lands in the same league as much larger voice models. Yet, the system remains light enough for real-time deployment, making it ideal for assistants, agents, and live conversational systems where timing matters more than anything else. On the visual side, Alibaba and several major Chinese universities unveiled live avatar, and this one surprised almost everyone. It's a real-time video avatar system that can generate expressive, highquality facial animation at over 20 frames per second, responding instantly to live audio input. This is not a short demo trick either. The system can stream for hours without losing identity, expression quality, or motion coherence. That long-term stability is what makes Live avatar stand out. Most video generation systems slowly break down as time passes with faces drifting, colors shifting, or motion becoming unnatural. Live avatar avoids that through heavy distillation and carefully designed attention mechanisms that keep the model anchored to a consistent identity. The result feels less like a research prototype and more like an actual product ready system. Now Tencent released Huan Video 1.5, one of the most practical video generators to appear this year. With just 8.3 billion parameters, it delivers highquality results while running fast enough on consumer-grade GPUs. Unlike many massive video models that feel locked behind data centers, this one is clearly designed to be usable by regular creators and developers. Speed is the big win here. Thanks to step distillation and an efficient architecture, Hunuan Video 1.5 can generate videos dramatically faster than previous versions while keeping motion smooth, prompts accurate, and visuals stable. Before we jump deeper into the story, there's something I keep seeing in the comments. People asking how we managed to produce so much content so fast. Look, in 2025 alone, this channel pulled in 32 million views. That's not",
        "start": 180.64,
        "duration": 685.1190000000003
    },
    {
        "text": "versions while keeping motion smooth, prompts accurate, and visuals stable. Before we jump deeper into the story, there's something I keep seeing in the comments. People asking how we managed to produce so much content so fast. Look, in 2025 alone, this channel pulled in 32 million views. That's not because every time a new AI breakthrough drops, we plug it straight into our workflow. Most people watch AI news and move on. We use it immediately. So, we decided to release something we've never shared before. The 2026 AI playbook. 1,000 prompts to dominate the AI era. This is how you go from just consuming AI content to actually using AI to build real unfair advantages for yourself. Get your proposals done in 20 minutes instead of 4 hours. Launch that side business you keep putting off. Become the person in your company who gets twice as much done in half the time. Founding member access opens soon. Join the wait list in the description. All right, back to the video. All right, now Google research finally pulled back the curtain on Titans, a system that directly challenges the limits of standard transformer models. The core issue Titans tackles is long context. Transformers become painfully expensive and unstable as context windows grow, while state space models stay efficient but lose detail by compressing everything into a single state. Titans blends both approaches. It uses windowed attention for short-term precision and adds a long-term memory module that updates while the model is running. What makes this different is how memory is handled. Titan stores information based on surprise and forgets intelligently instead of discarding data blindly. One variant, memory as context, handled sequences over 2 million tokens and dominated benchmarks like baby long and needle in a hay stack. A relatively small 760M parameter model outperformed GPT4 and much larger llama setups. If these results hold in real world use, Titan signals a shift toward models that adapt during inference instead of remaining frozen after training. Google is also preparing Nano Banana 2 flash, a lowerc cost version of its Pro image model that performs at nearly the same quality. This fits Google's playbook perfectly. Pro models establish performance leadership while flash models bring that quality to scale without exploding costs. Early signal suggests flash will be ideal for highfrequency image generation and batch workloads. The timing matters. Nano Banana has become one of the biggest drivers of Gemini engagement, especially among younger users who treat it as a creative engine rather than a productivity tool. A cheaper flash variant widens access and encourages daily use. Combined with Google's Android distribution advantage, this move is clearly about volume. It's not just about having the best model. It's about making sure the most people use it constantly inside Google's ecosystem. Then we have Lux released by the Open AGI Foundation. It actually feels like a turning point for AI agents. Instead of relying on APIs, Lux directly operates",
        "start": 347.039,
        "duration": 996.0780000000003
    },
    {
        "text": "volume. It's not just about having the best model. It's about making sure the most people use it constantly inside Google's ecosystem. Then we have Lux released by the Open AGI Foundation. It actually feels like a turning point for AI agents. Instead of relying on APIs, Lux directly operates screens, understands layouts, and performs clicks, scrolls, and keystrokes across browsers, spreadsheets, editors, and full operating systems. This makes it far closer to real automation infrastructure than a chat model with tools. On the Mind2 web benchmark, Lux posted an 83.6 6 score well ahead of Gemini CUA OpenAI operator and Claude. Its strength comes from both design and training. Lux offers three execution modes ranging from fast singlestep actions to fully scripted deterministic workflows. More importantly, it's trained through agentic active pre-training where it learns by acting inside thousands of live OS environments. That experience makes it robust, adaptable, and significantly cheaper to run, reshaping expectations for what agents can realistically do. All right. Now, GLM 4.6V exploded overnight. And once you look at what Zepu AI actually shipped, it's obvious why. This is the first open-source multimodal model that treats images, videos, screenshots, and even full web pages as real inputs for tool calling, not as something that has to be flattened into text first. Visuals flow directly into reasoning and action. That alone changes how agents work because the model doesn't just describe what it sees. It uses visuals as part of its decision loop. The open source aspect is what shocked people. Until now, this level of multimodal capability lived behind closed APIs. GLM 4.6V is MIT licensed, downloadable, and runnable locally. It also stretches to a 128k context window, which means it can process massive mixed inputs, long documents, slide decks, or even an hour of video in one continuous pass without fragile pipelines. JEIPU released two versions, a 106b flagship for cloud use, and a 9B flash model tuned for local low latency workloads. The flash version is free and the pricing on the large model lands around $1.20 $20 per million tokens total, massively undercutting GPT 5.1, Gemini 3 Pro, and Claude Opus. Despite that, GLM 4.6V beats or matches much larger models on long context reasoning, video understanding, and multimodal benchmarks. The real breakthrough is native multimodal tool calling. Screenshots, PDFs, video frames, and web results passed directly into tools and back into the model as visuals, not text summaries. that closes the loop between perception, reasoning, and action. The missing piece for real multimodal agents, between the licensing, pricing, long context, and genuine visual execution, GLM 4.6V doesn't feel like an incremental upgrade. It feels like a shift in what open-source multimodal systems are capable of, and that's why the reaction was instant. Now, here is an interesting one. A Tokyo-based startup called Integral AI says it has built the world's first AGI capable model. And that wording matters because AGI capable effectively means AGI capability isn't",
        "start": 505.199,
        "duration": 1352.397
    },
    {
        "text": "shift in what open-source multimodal systems are capable of, and that's why the reaction was instant. Now, here is an interesting one. A Tokyo-based startup called Integral AI says it has built the world's first AGI capable model. And that wording matters because AGI capable effectively means AGI capability isn't human level or beyond, then it's already there in practice. Integral AI was founded by Jad Terafi, a former Google AI researcher who spent years working on early generative systems before relocating to Japan to focus on robotics. According to Tarifi, this system is not another scaled up language model. It's built from scratch to mirror how human intelligence actually works and can learn new skills on its own without data sets, labels, or human supervision. What caught attention is how integral AI defines AGI. They use three concrete criteria. First, autonomous skill learning, the ability to acquire entirely new skills in unfamiliar domains without examples. Second, safe and reliable mastery, meaning the system learns without catastrophic failures. Third, energy efficiency, where learning a task consumes energy comparable to the human brain. That last point sets a biological benchmark most AI systems don't even attempt to meet. The architecture is modeled after the human neoortex and combines perception, abstraction, planning, and action into a single loop. Integral AI claims it has already tested this system in real world robotics where machines learned new behaviors independently. Whether the claim holds up remains to be seen, but it lands at a moment when leaders like Deep Minds Demis Hassabis openly say AGI is approaching fast. Between world model research, embodied agents, and growing ethical attention, even from institutions like the Vatican, the AGI conversation has clearly shifted. This no longer feels theoretical. Okay, now OpenAI just released GPT 5.2. And on paper, this should have been a clear victory lap. The benchmarks are strong across the board. Professional task performance jumped. Coding improved. Long context reasoning finally holds up at massive scales. Vision is more reliable. And Agentic tool calling looks genuinely productionready. By every metric OpenAI highlighted, GPT 5.2 is a real step forward from GPT 5.1. And yet, the reaction online felt strangely cold. Instead of excitement, the response was skepticism. People questioned the benchmarks, joked about reasoning modes, and said they'd believe it only after feeling the improvements in daily use. What makes this interesting is that most critics actually understand the data. They know the gains are real. That's exactly why the backlash matters. GPT 5.2 genuinely delivers. On GDP Val, it beats or matches human professionals on more than 70% of real workplace tasks. On SWEBench Pro, it sets a new state-of-the-art. On ARGI2, the jump isn't incremental. It's a slope change. Long context reasoning works across hundreds of thousands of tokens and vision and tool use show meaningful gains. This is likely the strongest general purpose model OpenAI has released. So why the distrust? First, benchmark fatigue. Users have seen too many charts that didn't translate",
        "start": 685.279,
        "duration": 1688.7160000000006
    },
    {
        "text": "incremental. It's a slope change. Long context reasoning works across hundreds of thousands of tokens and vision and tool use show meaningful gains. This is likely the strongest general purpose model OpenAI has released. So why the distrust? First, benchmark fatigue. Users have seen too many charts that didn't translate Numbers still matter, but they no longer persuade emotionally. Second, trust erosion. Past releases trained users to expect throttling, behavior shifts, or quiet roll backs. Improvements now feel temporary by default. And finally, focus. GPT 5.2 is clearly optimized for enterprise work, spreadsheets, coding, agents, long documents. It's better at productivity, but colder in tone. The reaction shows something important. Intelligence alone no longer defines success. Trust and comfort matter just as much. All right. Now, in other AI news this week, OpenAI and Disney announced a landmark three-year partnership that makes Disney the first major studio to officially license its IP to an AI video platform, backed by a $1 billion equity investment and additional warrants. The deal gives Sora and Chat GPT images controlled access to over 200 characters, costumes, and locations across Disney, Marvel, Pixar, and Star Wars. Starting in early 2026, users will be able to generate short fanprompted videos and images with curated content even appearing on Disney Plus. Importantly, the agreement excludes actor likenesses and voices, enforces age- based access rules, and includes strict safeguards against harmful or illegal outputs. Disney will also integrate chat GPT and Sora APIs into its internal tools and Disney Plus, turning one of the world's largest IP libraries into a licensed AI native ecosystem. This deal is widely seen as a test case for how generative AI and major rights holders can coexist without constant legal conflict. Meanwhile, Mistral AI doubled down on open source with the release of Devstrol 2, a new family of coding focused models. The flagship Devstral 2 weighs in at 123 billion parameters with a massive 256k context window. While Devstrol Small 2 offers a 24B parameter alternative that can run locally on consumer hardware. Devstrol 2 scored 72.2% on swbench verified putting it among the top performing openweight coding models available while remaining significantly more costefficient than proprietary competitors. The models are designed for realworld software engineering tasks like multifile refactoring, dependency tracking, and automated retries. Mistral also introduced the Vibe CLI, a command line tool for code automation and projectaware orchestration, reinforcing the company's strategy of building transparent developer first AI systems rather than closed platforms. Finally, in a move that surprised many, the US Department of War officially rolled out genai.mill, a secure generative AI platform now available to all military personnel, civilians, and contractors on the non-classified network. The system is powered initially by Gemini for government, a specialized version of Google's AI approved for handling controlled unclassified information. According to Secretary of War Pete Hegsth, the goal is mass AI adoption across daily workflows from document creation and research to data analysis, satellite imagery interpretation, and",
        "start": 855.839,
        "duration": 2047.6750000000002
    },
    {
        "text": "network. The system is powered initially by Gemini for government, a specialized version of Google's AI approved for handling controlled unclassified information. According to Secretary of War Pete Hegsth, the goal is mass AI adoption across daily workflows from document creation and research to data analysis, satellite imagery interpretation, and verified personnel, and users are reminded to validate outputs carefully. Officials framed the roll out as a strategic move in the global AI race, emphasizing speed, scale, and integration rather than experimentation. So, here's the question. Is AI moving fast because the tech is ready or because companies are rushing before anyone else wins? Drop your take in the comments. Like and subscribe. Thanks for watching and I'll catch you in the next one.",
        "start": 1037.12,
        "duration": 2098.9570000000003
    }
]