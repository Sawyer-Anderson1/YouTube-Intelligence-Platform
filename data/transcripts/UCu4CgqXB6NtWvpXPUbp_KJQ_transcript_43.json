[
    {
        "text": " We do have breaking news. It's regarding NVIDIA. We'll get to Jon Fortt now, who can fill us in with more. Jon, what do we know? Scott Walker, thank you. I am here at NVIDIA headquarters in Santa Clara with the CEO of the world's most valuable company and the CEO and president of the world's most valuable private company, Jensen Huang of NVIDIA, Sam Altman and Greg Brockman of OpenAI. So let's dive right in to the news. Jensen, NVIDIA is making a hundred billion dollar investment in OpenAI and working together to build out, I think you're saying, 10 gigawatts of capacity over several years. The investment is going to come with the gigawatts, one at a time. You're telling me, you guys, as quickly as you guys can get it done. Jensen, first of all, set the stage. Why? This is the biggest AI infrastructure project in history. This is the largest computing project in history. Well, the reason for that is because computing demand is going through the roof for OpenAI. You know, ChatGPT is the single most revolutionary AI project in history. It's being used everywhere, every industry, every country, every person practically that I know uses ChatGPT. The computing demand is going through the roof. And so this partnership is about building an AI infrastructure that enables AI to go from the labs into the world. This is about the AI industrial revolution arriving. It's a very big deal. A hundred billion dollars, a lot of money. Sam, Greg, you guys are used to dealing with a lot of money and big projects. So Sam, I think it was just eight months and a day ago, the initial Stargate announcement talking about the overall move that OpenAI is making in building out this capacity. Where does this fit? So as Jensen said, building this infrastructure is critical to everything we want to do. Without doing this, we cannot deliver the services people want. We can't keep making better models. And now that we really see what's on the near term horizon of how good the models are getting, the new use cases that are being enabled, what people want to do. This is like the fuel that we need to drive improvement, to drive better models, to drive revenue, everything. So this is helping us get to a world, along with our partners at Stargate, Microsoft, Oracle, where we can build out increasing amounts of infrastructure to deliver on what the world is demanding out of these services. There's like no partner but NVIDIA that could do this at this kind of scale, at this kind of speed. It's really quite incredible. But this will expand on the Stargate ambitions and let us push further and further. We have found every step along the way that we did not quite set our sights big enough, given the market demand. So this will help us push towards that next level.",
        "start": 0.0,
        "duration": 153.69100000000006
    },
    {
        "text": "really quite incredible. But this will expand on the Stargate ambitions and let us push further and further. We have found every step along the way that we did not quite set our sights big enough, given the market demand. So this will help us push towards that next level. We're so limited right now in the services we can offer. There's so much more demand than what we can do. And as we look forward another year or two years, if you have, you know, let's say it takes 10 gigawatts of compute or 5 gigawatts of compute, you could choose one of two things. You could choose to cure cancer by doing a bunch of, having AI do a bunch of research, or you could choose to offer free education to everybody on Earth. No one wants to make that choice. And so increasingly, as we see this, the answer is just much more capacity so that we can serve the massive needs and opportunity with this. Greg, one way of measuring that demand is the number of users that open AI has. ChatGPT says somewhere between 700 and 800 million. How quickly are you growing? What's the technology that you guys need to bring to bear from NVIDIA included in order to satisfy that? Yeah, so 700 million growing extremely quickly. I think that ChatGPT has grown from nothing to that scale and continues to grow faster than any product in history. And really, the reason we were able to make this breakthrough and serve it to the extent that we have is by leveraging NVIDIA's platform. We've worked together very closely since Jensen actually hand-delivered a server to us back in 2016. And we were just doing some math earlier. I think that this deal is really for a billion times more computational power than that initial server. And so we're able to actually create new breakthroughs, new models to be able to be able to actually solve problems like create cures for diseases and to actually be able to empower every individual in business because we'll be able to reach the next level of scale. Jensen, step back for me. I mean, a few days. You're all over the place, literally. I mean, you're in the UK. I saw the white bowtie and all that. The tux is very nice. But you're also doing a lot of investments. The Intel investment announced last week, quite a bit smaller than this one, but seems significant also because it's weaving NVIDIA technology in the PC and data center level in a way that perhaps it wasn't before. Where do these kinds of investments fit in? How do you think about the value of the ecosystem to NVIDIA? The Intel partnership is about recognizing that accelerated computing and AI's day has arrived. Remember, general purpose computing was invented practically 60 years ago.",
        "start": 166.99,
        "duration": 278.331
    },
    {
        "text": "way that perhaps it wasn't before. Where do these kinds of investments fit in? How do you think about the value of the ecosystem to NVIDIA? The Intel partnership is about recognizing that accelerated computing and AI's day has arrived. Remember, general purpose computing was invented practically 60 years ago. And so all of a sudden, accelerated computing time has come and we're fusing, if you will, the Intel architecture with the NVIDIA architecture to bring them into the world of accelerated computing and AI. So that's what that partnership's about. This partnership, I mean, this is, you know, monumental in size. There's never been an engineering project, a technical project of this complexity and this scale ever. And it really just says that AI was in the early adopter phase in the labs. And finally, it's breaking out into just about every single industry, every single use case we can imagine. It is very soon where every single word, every single interaction, every single image, video that we experience on, you know, in through computers will somehow have been reasoned through or referenced by or generated by AI. It's going to be touched by AI somehow. So all of our computing experiences throughout the day, everywhere in every industry will be powered by AI. This is the first 10 gigawatts. It surely it sounds like an enormous, enormous undertaking. But there's no question that AI is transformational for every industry. But the important thing is the AI infrastructure will be everywhere and will power computing experiences for everyone every day. And it's going to be just everywhere. Sam, it seems like the two most valuable companies in the world, NVIDIA at number one, over four trillion, about four and a quarter trillion. Last time I checked today, Microsoft, just under four trillion, are going to be major investors in OpenAI. How should we understand the governance, the influence that these companies have over opening where those lines are and even, you know, the investment size wise where that's going to net out? We're thrilled to have them both as partners. They're they're passive investors. You know, our nonprofit and board are in control. But the ecosystem is really important to us. NVIDIA and Microsoft are two of our most critical partners and have been from the very front from the very beginning. And having them so aligned with our success is, I think, great for us. Hopefully great for them, too. Greg, where does NVIDIA fit along with all of the other infrastructure providers? Oracle, you guys kind of touched Oracle and the stock went up, even though the name wasn't attached to it. When they reported earnings and their guidance, a lot of people were looking over at OpenAI. Where do these different technology infrastructure players fit in terms of importance in the amount of capacity that you need?",
        "start": 306.75,
        "duration": 425.51099999999997
    },
    {
        "text": "Oracle and the stock went up, even though the name wasn't attached to it. When they reported earnings and their guidance, a lot of people were looking over at OpenAI. Where do these different technology infrastructure players fit in terms of importance in the amount of capacity that you need? It's much larger than the Apollo program, for example. And so we're going as big as we can with the biggest partners in the world. NVIDIA is a core strategic partner for our build out. There's just no one who could build as fast or as big as they are going to be able to accelerate us to be able to do. We are working together with Oracle to do a lot of the infrastructure builds with SoftBank and Stargate to be able to actually do a bunch of that work as well. We're starting to expand into just trying to figure out every single way that we can actually get this compute to the world. But there's really been no partner like Jensen, like NVIDIA, and it's just been a very, very special partnership for more than a decade now. And we're moving into the next phase together. One of the things that's really important to say is that, and Sam and Greg hinted at it, this is additive. This project, 10 gigawatts of AI infrastructure is additive to everything that has been announced and contracted. Remember, they've contracted huge amounts of capacity through Azure, through OCI, through CoreWeave. And all of that is powered by NVIDIA, and we're really delighted working with all of these partners. And that's going to continue to grow. This is additive, incremental on top of that, which just kind of puts it in perspective, the scale of AI computing that's needed for the world. The demand is just exponential. Yeah, to say anything different from what's already been included in the financials that you've given or the guidance that you've given. But this is an announcement that's breaking right here on CNBC. Has this been factored in to the numbers that you've given Wall Street up to this point? This is additive to everything that we've spoken about so far. It's pretty incredible. We're also talking about AI infrastructure for the world. One of the headlines last week had to do with China and how they're dealing or not dealing with NVIDIA. When you look at where this capacity is getting built out and the global use that it's going to have, how do you think about the way different regions, the way different countries are going to tap into that and how that affects NVIDIA's competitiveness? Well, you know, President Trump was clear about this, and you've heard me say this before, that we want the world to be built on the American tech stack and the American tech stack. The AI computing stack includes chips, infrastructure, models and applications.",
        "start": 471.85,
        "duration": 554.082
    },
    {
        "text": "and how that affects NVIDIA's competitiveness? Well, you know, President Trump was clear about this, and you've heard me say this before, that we want the world to be built on the American tech stack and the American tech stack. The AI computing stack includes chips, infrastructure, models and applications. And in doing so, we want the world to be built on top of American chips. We want American infrastructure, American models and for the world to integrate with American applications. And we ought to diffuse this capability as fast as possible because the world is racing to bring AI out to the world. And so I think the need to build AI infrastructure all around the world. This is just beginning. Notice most of the infrastructure conversations we've been having are are largely located in the United States. But we're going to see AI infrastructure built in Europe and in southern parts of the world. You're going to be right, Southeast Asia, all over the world. We're going to be building AI infrastructure. Sam, there was a DeepSeek moment several months ago. The market kind of freaked out about what this means for infrastructure, for capacity, for expense. What what was the legacy of that? Given the moment that we're in right now, this hundred billion dollar investment NVIDIA is making in you, clearly you see value in these chips or or you wouldn't need that kind of money for that kind of equipment. What what does that mean? How has the way the conversation around AI models has shifted affected the way that you guys are looking at the build up to two thoughts about this? First of all, I think most of the world still thinks of AI as what ChatGPT can do. You know, it's a better version of web search or it helps me, you know, with some small tasks here and there. AI has moved an incredible distance. AI is now outperforming humans at the most difficult intellectual competitions we have. For the first time with GPT-5, you're starting to see scientists saying AI is like making novel discoveries, small ones, but real ones. And so there's this I think there's this huge mismatch in the world of what most people think AI can do and what the frontier of AI can do. And so when DeepSeek came out, I think people had this like brief freak out. I think maybe your stock went like way down in one day and people were just like, oh, this is like the end. People really want to predict the end of the compute scaling somehow. And then it turned out that people need a lot of AI and they need a lot of the frontier AI. And and so it is totally true that the cost per unit of intelligence will keep falling and falling and falling.",
        "start": 616.35,
        "duration": 678.9820000000001
    },
    {
        "text": "predict the end of the compute scaling somehow. And then it turned out that people need a lot of AI and they need a lot of the frontier AI. And and so it is totally true that the cost per unit of intelligence will keep falling and falling and falling. But on the other side, the frontier of AI, the maximum intellectual capability is going up and up. And that enables more and more use and a lot of it. So there's this huge overhang that the world, I think, does not yet grasp of where the models already are today. And when you throw a lot of inference compute at them and what they can do for you, that is totally different than ChatGPT or generate an image or whatever. And so I think that was really missed in the DeepSea moment. And why do you want to do this? It's that there's the models are at this point, like actually quite capable for things far beyond what most people use them for in Chachapiti. And the world is just catching up with that. Second thing, you know, we throw on these numbers, 10 gigawatts, 100 billion. And there's a few syllables here and there. We're all, you know, it's like a Monday morning. We're all kind of like low energy. We've got a lot of stuff we got to go do and stress about the day ahead. But we've already had a pretty long day. We had a long day. But the magnitude of this deal. Yeah, I mean, but I flew out here not knowing exactly who I was talking to. And this came together. So, yeah, thank you. But for sure. Thanks for thanks for taking a flyer on that. But the magnitude of the scale of this project. You know, 100 billion is a small dent in it. And the numbers are also like they're missing the story of what this amount of infrastructure is capable of doing. Like 10 gigawatts of compute. Again, easy to throw around numbers like that. But the the amount of work it takes to build that out, the size and scale of these like, you know, multi square mile gigantic things and the complexity at every level of the supply chain. And then what that amount of brainpower, which does not exist today, can do already today. What it will do is the models get better. Like, this is the real deal. This is the thing people have been waiting for. You know, they talk about AI or when it's going to do this and when it's going to do that. Like the the stuff that will come out of this super brain will be remarkable in a way. I think we don't really know how to think about 100 billion is a small dent. Now you sound like President Trump.",
        "start": 758.47,
        "duration": 788.8419999999999
    },
    {
        "text": "to do this and when it's going to do that. Like the the stuff that will come out of this super brain will be remarkable in a way. I think we don't really know how to think about 100 billion is a small dent. Now you sound like President Trump. You know, chat you did today. You talk to it. It gives you answers. But clearly, you want an agent that's going to go do work for you proactively while you're asleep, be able to organize your calendar or go, you know, try to work on projects for you. And so you really want every person to be able to have their own dedicated GPU. Right. You're talking order of 10 billion GPUs we're going to need this deal we're talking about. It's for millions of GPUs. Like we're still three orders of magnitude off of where we need to be. So we're doing our best to provide compute availability. But we're heading to this world where the economy is powered by compute and it's going to be a compute scarce one. So, Jon, remember this new project that we're talking about, 10 gigawatts or roughly four million, five million GPUs. That's approximately in one project. What we shipped all year this year. OK. And twice as much as last year. Twice as much as the year before that. And so. So that kind of puts it in perspective. This is a giant project. And we don't yet know how many years to amortize that over. Sort of. But it's well, we just need OpenAI and OpenAI is the fastest growing software company in history. So about the infrastructure, we're talking about data centers. But I also want to talk to you about the edge. And I guess Sam or Greg, this could be either of you. Apple had their big launch of selling iPhones last week. A lot of people thought, oh, they're behind on AI. They're trying to push this idea that these phones, because the chips they have in them are AI ready. And of course, they're talking to you guys. Are these part of the infrastructure that you need? What should the world understand about the role of the edge and devices like this? This is where this there's this important idea, which is most of the world thinks that AI is ChatGPT. And yet AI capability has moved far beyond. So for the kind of chat with ChatGPT about most stuff, you will be able to do a lot of that on your phone. And we think that's great. We released an open source model called GPT OSS recently that can run on a laptop or a phone. And it's incredible to me what you can do on a device. It's gone way further than I thought. And we'd love to see a lot of that move to the edge.",
        "start": 886.99,
        "duration": 908.941
    },
    {
        "text": "released an open source model called GPT OSS recently that can run on a laptop or a phone. And it's incredible to me what you can do on a device. It's gone way further than I thought. And we'd love to see a lot of that move to the edge. But for like, you know, the sort of standard ChatGPT queries, we hope we can push a lot of that to the edge. So what's the significance of these platforms, whether it's from Apple, whether it's from the likes of a Qualcomm in being able to provide a standard platform for you guys, for other partners, for other application providers to build AI driven applications on top of what you're going to drive some of this demand that we've been talking about here? I think you need all of it. I think that no matter how much gets built, we are still going to be in this world of compute scarcity. And every time there's a new compute platform, new availability of this kind of computational power, that it's possible to design new algorithms, new methods, new models, new products that are able to deliver benefits to everyone. And so you should expect that you'll see this whole ecosystem of models from us and from everyone else that is able to take advantage of what's out there. It's a little bit like when the App Store first came out, suddenly people started creating apps and filling that platform because there was this new availability, this new way of building. And I think we're heading towards a platform shift that's going to be much more significant than anything in history. Another potential either roadblock or opportunity is talent, right? You've got to have the right people to build this stuff, to design this stuff, etc. Immigration is a part of that. Jensen, there have been some move, some announcements over the past few days about that. How does that position the US, maybe the H1B issue and the extra charge on top of that? How does that position the US in terms of competitiveness? What are your thoughts on where we need to go? We want all the brightest minds to come to the United States. And remember, immigration is the foundation of the American dream. And we represent the American dream. And so I think immigration is really important to our company and it's really important to our nation's future. And I'm glad to see President Trump making the moves he's making. What kind of policy needs do we have from your perspective? We need to get the smartest people in the country and streamline in that process. And also sort of aligning financial incentives seems good to me. How far are we from yet another big announcement about the capital needs and how you hope to fulfill? You should expect a lot from us in the coming months.",
        "start": 1026.49,
        "duration": 1031.881
    },
    {
        "text": "people in the country and streamline in that process. And also sort of aligning financial incentives seems good to me. How far are we from yet another big announcement about the capital needs and how you hope to fulfill? You should expect a lot from us in the coming months. We've got to do great research. We have to build these products that people really want to use. And we have to figure out how to do this unprecedented infrastructure challenge and build out which there's chips, there's power, there's buildings. There's a lot of pieces that go into that. And that that latter category is going to be my big focus in the coming months. And we have very ambitious goals. So we'll have a lot to say. Jensen, the next big catalyst that you see on the horizon, I mean, AI is what's driving so much of the market and the imagination right now of economies across the world. Look, we saw in the last five, 10 years, AI moved from generative to reasoning to now thinking models went from text to multimodal. It is very clear that there will be a persistent AI connected to every device to be everything that's out there. You know, it could be a persistent AI connected to the car, connected to smart glasses, connected to the phone, as you mentioned, connected to human or robots and robots of all different sizes and shapes. And all of that is just beginning. None of it exists today. And yet everything's around the corner. That's how exciting this is. We're literally going to connect intelligence to every application, to every use case, to every device. And we're just at the beginning of that. That's the reason why we need such gigantic infrastructure. And this is the first 10 gigawatts. I assure you of that. Well, I'm looking at the CNBC app and there's NVIDIA stock right now. We can't see OpenAI stock yet, but call me when we can. Scott and the Halftime crew, back to you on this breaking news from NVIDIA headquarters here in Santa Clara. Yeah, Jon, really extraordinary. The announcement and what we just heard from those three men gathered with you there, Jon Fortt at NVIDIA headquarters today. Thank you very much for that. You saw NVIDIA shares are at the highs of the day.",
        "start": 1163.63,
        "duration": 1130.321
    }
]