[
    {
        "text": " Let's take a look at some of the AI news that happened within the past month. So, one of the interesting things that I saw was a slew of different AI releases. One of those being Deep Seek's new open source AI and gold source medal performance. Particularly, there was a Deepseek version 3.2 two special model that hit gold level on multiple 2025 Olympiad style contests including the international mathematical olympiad and the international Olympiad in informatics. Now, gold medal is pretty impressive here because not only is it really hard to do, the fact that this is an open-source model that is gold medal, that is what is, you know, taking the cake here. Now, an open API accessible model solving Olympiad tier math programming problems at elite human level was only, you know, available for closed internal systems from big United States labs. So this pushes AI like it pushes that entire frontier forward from this AI is good at textbook problems to being competitive at creative problem solving and algorithmic design which has implications for automated theorem proving scientific research and many other things. So I think this was a very you know important thing that happened. I think this is not to be underestimated and open source AI is truly truly intriguing because we do have a situation where it seems like every month there are more and more AI companies now if not from China from other places that are just racing towards well to be honest they're mostly from China they're just racing towards beating the United States at their own game so it's going to be super interesting of course there was also the 3.2 and 3.2 2 Special. I will say though that if you are using the Speci, please understand that that model, it is very very tokenheavy. So just be careful when using their model because it's a lot more token heavy than people think. So that model is basically just for you know mathematics in the sense that the average person using the model, they wouldn't get that much out of it compared to what you know an average person could if you were just using the standard Deep Seek version 3.2. So that model, the open source release once again, it is kind of surprising that Deep Seeker is still in the game. They were pretty quiet for a long time. And so them having this open source model was not a surprise, but was still a surprise because it was surprising to see them in the game after many competitors, you know, you would have thought maybe even overtook them. Now the thing is is that we also had something interesting. We had Mistral 3 which is Mistral's AI family and they have a series of open multimodal multilingual models and Mistro large 3 is the flagship big moture of experts model in the family. So you have to understand that the reason I'm talking",
        "start": 0.08,
        "duration": 331.518
    },
    {
        "text": "that we also had something interesting. We had Mistral 3 which is Mistral's AI family and they have a series of open multimodal multilingual models and Mistro large 3 is the flagship big moture of experts model in the family. So you have to understand that the reason I'm talking don't want to say they'd get angry but a lot of people would wonder why I would even include this in a video but I think it's important because Mistral is you know Europe's only I think open- source AI at the moment. It's the only open- source AI effort at the moment and it's Europe's, you know, native one if you're not counting, you know, Google Deep Mind or, you know, Google is technically like a global company. But if we're looking, you know, just at that small nation, Europe, this is, of course, the only model that they do have at the moment. Now, surprisingly, some would argue that this model is not as good as its Chinese counterparts. Of course, I think this model is not as bad as people are saying. I think, you know, you have to take into account the different ways that the models are going to be trained. Some are going to have a Chinese bias. Of course, the ones from China and the ones from the west are going to have a western bias. So, of course, if you're just looking for raw capabilities and you don't really care about that stuff, it does make sense to use the Chinese models. But we do have to understand that there is some Chinese bias. And there are certain things that, you know, if you do say to the model, it just pretends they won't exist. Of course, I'm not going to mention what those are, but it's pretty easy to figure out what those are with a quick Google search. So, you know, as for performance, I mean, it's not that great compared to, you know, Chinese models if you're talking about just Frontier performance. But I would say that it is on par with those, you know, kind of models. So I think you know the unfortunate thing is that just AI just moves so quickly and even if you do you know release a frontier model very quickly like even you know the next day you know you can really really be you know behind in a sense and it's so so surprising I mean if we look at the LM arena score it doesn't do that bad I mean it's just behind Quen 3 Kimik 2 and Deep Seek version 3.2 too, but the non-thinking models and mistrout by the way as well, this is another important part is that they don't have a thinking model. So, I'm guessing that this is why this kind of, you know, LM Marina score isn't as high as the others. And of course, the benchmarks aren't as high as they could",
        "start": 165.519,
        "duration": 570.5590000000001
    },
    {
        "text": "by the way as well, this is another important part is that they don't have a thinking model. So, I'm guessing that this is why this kind of, you know, LM Marina score isn't as high as the others. And of course, the benchmarks aren't as high as they could just yet. So, I'm not completely writing off this company. I do think that their products are a lot better than some of the Chinese ones in the sense that I've used the website, I've used the user interface, it's actually pretty decent to use. So, let me know what you guys also think about that open source area considering later on in the video I'll show you why a lot more people are going to be considering open source including myself. Now, if we're getting into more benchmarks wouldn't be, you know, possible without talking about Opus 4.5. Opus 4.5 was a really, really impressive benchmark because it showed us that Anthropic aren't slowing down. And I think what Opus 4.5 showed the industry was that there was no wall. I'm not going to talk about this for too long, but what I will say is that it surprised even me. All of these benchmarks, including humanity's last exam, an Arc AGI 2 were some of the most surprising because it was only like a few days prior to that that we did get Gemini 3. And Gemini 3 was a giant leap across the board. So, most people would have thought that, okay, Google is firmly in the race and they're probably going to hold that, you know, title for some time. But they didn't even hold it for that long. like Opus in certain areas which is what was surprising was that they managed to literally you know dominate the coding space so so narrowly like that is just you know anthropic's game and I think it's it's you know clear now that you know these companies are starting to carve out their own areas of dominance and I think if anthropic coding is just going to be that area so if you're wondering what you should use this model for of course software engineering that's where the you know complete domination is for anthropic but of course for other companies It really does just depend on what kind of use case you are. But, you know, if we're talking about the other models, of course, Gemini 3, Gemini 3 Deep Think was surprising. It's actually released today. I got a tutorial coming. Probably should be out now or after this video. But this one once again was super surprising because Humanity's last exam, it did really well above GPT 5 by quite a wide margin. Also arc AGI 2 45% is in is just incredible considering the fact that you know 70% and ARC AI 2 was a pretty difficult benchmark. So also if we look at the humanities last exam this",
        "start": 286.639,
        "duration": 841.5179999999998
    },
    {
        "text": "exam, it did really well above GPT 5 by quite a wide margin. Also arc AGI 2 45% is in is just incredible considering the fact that you know 70% and ARC AI 2 was a pretty difficult benchmark. So also if we look at the humanities last exam this surprised everyone as well because humanity's last exam you didn't really expect it to be that many gains this quickly considering the fact that it was supposed to be difficult and it was supposed to be humanity's last exam. Hopefully there's no data contamination because they designed it that way. But, you know, I guess we'll have to see moving forward just how well these models are across the board. I think for the most part, if you look at like the benchmarks, I think the most, you know, interesting thing that we saw from Gemini 3 was Nano Banana Pro. And I think the reason that was was because most people don't realize that the next phase of AI is going to be world models. And if you use Nano Banana Pro and you understand fundamentally what the AI is doing, you know that it's pretty impossible to do that without some kind of internal world model. I'm not sure what world model they do have. Genuinely, I don't know what kind of world model Google are using, but it is a very very impressive one. The use cases for Nano and Banana Pro are only possible with an AI system that is able to have a world model. So, it's quite likely, I'm not entirely sure if this is the case, that Gemini 3 has a world model, but I do know that when you use Nano Banana Pro, the vision model, it starts to think about what it does. I honestly could make a whole video on Nanab Banana Pro, but I think personally, this is just my genuine thinking, is that that was probably one of the biggest breakthroughs towards AGI that we've had this year. And I know you might be thinking, how does that make sense? You've got, you know, you know, mathematical models and proofs and stuff like that. I'm talking about purely from you know humanlike reasoning standpoint because if you can reason about images and 3D shapes and you know all of these kinds of things and the videos I've watched about Yakan and the kinds of reasoning that even the biggest skeptics were essentially saying that AI would never be able to do like they're silent like even Gary Marcus I didn't see him you know comment that much about Nab Banana Pro because oftent times his comments about you know the model not being able to understand X Y or Zed you know they were disproven with this model. So that's why I think Gemini 3 as well, you know, the Arc AGI, you can even see it was such a huge jump. That's",
        "start": 424.479,
        "duration": 1085.7579999999998
    },
    {
        "text": "times his comments about you know the model not being able to understand X Y or Zed you know they were disproven with this model. So that's why I think Gemini 3 as well, you know, the Arc AGI, you can even see it was such a huge jump. That's because, you know, we had that world model inside of the model. So I think considering the fact that these companies are now going to be moving that way in 2026, I genuinely believe there's probably going to be even more progress in 2026 than people would have expected, which kind of makes me excited but scared at the same time. Now, of course, as well, this is crazy because I didn't see anyone talking about this and I thought, might as well add this since they were talking about all of the new stories. This is Amazon's new model. So, Amazon actually released a Nova 2 model family. So, you've got Nova 2 Light, which is the fast, cheap, and smart version compared to like Claude, Haiku, and GPT 5.1 Mini. This is high price performance reasoning model. This is interestingly enough, what's really interesting about Amazon is that they've managed to handle text, images, and video. And it actually has an adjustable chain of thought, and it beats Claude 4.5 HighQ, GBT5 Mini, and Gemini Flash 2.5 on most benchmarks. That's actually Amazon's suite of models, by the way. So, that's really, really surprising because Amazon, you know, aren't fundamentally, I mean, you could argue that they're an AI company, but they aren't like fundamentally and AI is what we do. So, they've clearly got some incredible, incredible leadership, and I expect them to dominate for a few years. And this is, of course, the screenshot I'm showing you guys right now. This is Amazon Nova 2 Pro. So, this is Amazon's high-end your reasoning best. This is, you know, once again, a model that can process text, images, video, and speech. It's very strong at coding. It's very strong at complex planning and multi-document analysis, advanced math, and it outperforms many tasks against claw.5 Sonnet, GPD 5.1, and Gemini Pro Preview. So, it's interesting that Amazon can develop their own, you know, AI model that's on par with some of these, you know, Frontier Labs. And I think it's super surprising that most people aren't paying attention to this because what this means once again is that the models are becoming commoditized. I'm not saying that Amazon's models are anywhere near, you know, but they're on par in the sense that like they're not that far off, which means models are becoming commoditized, which means that, you know, in the future, these companies are going to have to differentiate themselves in a variety of different ways, which should be super interesting. Now, of course, since we're talking about, you know, Nova and and the series of models, there is actually, you know,",
        "start": 548.48,
        "duration": 1356.317
    },
    {
        "text": "commoditized, which means that, you know, in the future, these companies are going to have to differentiate themselves in a variety of different ways, which should be super interesting. Now, of course, since we're talking about, you know, Nova and and the series of models, there is actually, you know, real-time voice AI with expressive multilingual voices. There was also Nova 2 Omni which is Amazon's do everything model true multimodal text images video and it generates text and images which is pretty crazy just a whole bunch of stuff. and this got us to the question of is SW done by 2026. This wasn't just pure speculation by me, but rather an anthropic employee basically said that look, the S sw bench is going to be solved by 2026. And I think it is probably possible that that does occur and it will be interesting to see how, you know, companies are moving forward. So for me, I'm going to, you know, be really intrigued to see what the job market looks like in 2026 with regards to software development because of course it's going to change and I do believe that once these models manage to, you know, they they manage to get some more gains out of, you know, various different ways, we could potentially see, you know, 90% in Q3 of 2026 or 93% in Q4 of 2026, which is, you know, not that far away when you think about it in terms of progress versus actual time. Now, if we're talking about, you know, progress and timelines, OpenAI said, well, some of those model releases were just too good and we need to emphasize the code red. So, this is surprising. Well, it's not actually surprising, but it's surprising that OpenAI have said enough is enough. And I think the reason that this is so surprising is because OpenAI have coasted for a very long time on their ability to stay the market leader. And I guess they kind of hoped that everyone would look at what OpenAI is doing next. And Google kind of said, well, we we we don't have to look at OpenAI in order to make our move. All we really need to do is provide great products and services and the rest will do its thing. And that's been really incredible because, you know, in the in the past, you know, couple of months, we've seen that Google has moved forward and become one of the most dominant players in AI. Now they don't have the complete monopoly that open AAI has at the moment but it's gotten to the point where OpenAI has declared a code regret. So in the coming weeks and months you can expect OpenAI to improve the model such as improving the model behavior, improving image, improving voice just generally just making the product I would think not more addictive but just a better product to use. And you know it's going to be",
        "start": 685.76,
        "duration": 1631.3580000000004
    },
    {
        "text": "in the coming weeks and months you can expect OpenAI to improve the model such as improving the model behavior, improving image, improving voice just generally just making the product I would think not more addictive but just a better product to use. And you know it's going to be happens. So you can see one of the key priorities included image gen because Nano Banana Pro was incredible and so many people are using that now. I think if they manage to get a good image model out soon and it's faster than Nano Banana Pro then maybe users might not want to switch from ChatGpt because chat GBT provided it has memory and all those things it's it's kind of hard to switch. Well, it's not really hard to switch. It's just you know more of an inconvenience and humans humans are basically creatures of habit. So if OpenAI can quickly roll out an image model, you know, and then market it as much as possible with memes and stuff like that and then get, you know, the masses and retail on board, then maybe they can, you know, capture that strong area of market share. But if they can't, they're going to lose a lot of people to Google's suite of products because Google already has a large network of distribution. And you have to understand that OpenAI is going to be fighting an uphill battle. they're already losing users, you know, every single month as more individuals come into the space. So, it's it's in their best interest to kind of to kind of get this, you know, out as quickly as possible. So, you know, OpenAI, as we speak about them, they also lost this privacy fight. Now, this is this is bad. This is why I say open source, people are going to be thinking about Open Source because OpenAI lost the fight to keep the CH GBT log secret in a copyright case. So they were ordered by United States judge to hand over around 20 million anonymized chatp user logs to the New York Times and other news outlets a part of their copyright lawsuit after losing the attempt to keep those logs confidential. The court basically said that the logs are relevant evidence and that the privacy kerns can be addressed through deidentification and protective orders. So they must be produced in discovery. The long story short here is that they basically want to look at the models and they want to not the models but they want to look at the outputs and see if there was actually any copyright done and I think it's important to understand that if your you know chats and you know private conversations of course they're going to remove anything like you know names numbers and those kind of things but I think it's still important that you know this is just shows us that the conversations we have with the",
        "start": 825.12,
        "duration": 1881.3569999999995
    },
    {
        "text": "understand that if your you know chats and you know private conversations of course they're going to remove anything like you know names numbers and those kind of things but I think it's still important that you know this is just shows us that the conversations we have with the There was even one point during this case where they literally said that okay you need to retain all user data up until a certain point and that was even if you were talking in temporary chat and that was even if you were you know on the team's pan there was literally a court order that you know restricted them from deleting any user data which is pretty crazy because you know I'm sure there are individuals who share things with chatbt that they wouldn't want being looked at by a judge or someone at the New York Times. I mean privacy is everything. So you know if we go back to the earlier point open source AI deepse commarge if you can have you know like a home system where you can run open source AI you don't need to send it up to the cloud. I think once it gets streamlined to the point I think that might be you know at least for sensitive data and at least for companies for certain that's going to have to be you know industry standard. So it's going to be interesting to see how you know enterprise offerings and APIs and stuff happens because if this continues to happen and people are starting to realize that look on device AI might have to be the way forward considering the privacy concerns then I guess so be it and it's going to be interesting to see. Now, of course, I already spoke about Nab Banana Pro. , pretty crazy. Genuinely crazy. World models, all those kinds of things. And I think the realism here is crazy. Honestly, I haven't fact checked that this image is AI generated. I mean, I could easily there is a a logo in the bottom right, but it looks so, you know, realistic that I am struggling to believe it. And I've generated hyperrealistic images. cuz you know I made a tutorial today on the second channel and it still kind of irks me that I'm able to do that in a few seconds because I mean I don't think technology will increase exponentially in certain areas because once you reach this level of photo realism there isn't there you like I mean there's not really much more you can do like this is as good as it gets but it does worry me about you know other forms of media that could be photorealistic so photorealistic voices which is kind of already here photorealistic videos and just what that means for the future future of media and you know how individuals are going to navigate such a tricky space because scams are a thing",
        "start": 952.079,
        "duration": 2129.916999999999
    },
    {
        "text": "you know other forms of media that could be photorealistic so photorealistic voices which is kind of already here photorealistic videos and just what that means for the future future of media and you know how individuals are going to navigate such a tricky space because scams are a thing going to be interesting. So this is of course right now number one on image edit nano banana pro 2 2k this is of course number one. I mean it makes sense. I mean it's going to be interesting to see if opening I can't top it. I personally don't think they'll be able to. Google already had nano banana. They didn't respond. Then they had nano banana pro and they still haven't responded. Now, we also had something strange happen to Grock. Elon Musk tampered with Grock again. essentially, I don't know what this guy is doing when it comes to Grock. I mean, you have one of the smartest, most cracked teams, and you're deciding to, you know, tamper with the models so you can benefit yourself or make it seem that, you know, you are I don't even I don't even have the words to describe what would even make sense. Like, if I had a model, I would truly just leave it up to the model. Like if the model said, you know, I'm not that good of a guy, I would be like, okay, that's fair. Maybe I need to improve. But the fact that he's tampered with the model to the point where, you know, it says that he's more athletic than LeBron James and that he would beat Mike Tyson the fight, I mean, it does the inverse. If he wants people to trust what Grock is going to say, he shouldn't be tampering with the model for his own benefit or, you know, people that don't disagree with the people that disagree with him. Like there's statements from Bill Gates, like if you ask it, does Bill Gates think this is right? Long story short, if you're Elon Musk's enemy, Grock is not going to have a great opinion about you, which is of course not a good thing. And it's not really great for an AI being, let's say, fair. So, that was, you know, super super like I'm I'm I'm now fully fully skeptical of anything Grock says. And I will be taking, you know, everything that Grock says with a grain of salt. And I just want you guys to know that because, , you know, this article, it was basically saying he's fitter than LeBron James. just Anyways, if we go to the text to video leaderboard, there was runway gen 4.5, a new video, you know, model that was surprisingly better than V3. Genuinely surprising. I'm surprised. I said that three times. I don't know what I'm talking about. But yeah, the reason I'm surprised is because we didn't really expect this. There was no",
        "start": 1078.799,
        "duration": 2366.078
    },
    {
        "text": "leaderboard, there was runway gen 4.5, a new video, you know, model that was surprisingly better than V3. Genuinely surprising. I'm surprised. I said that three times. I don't know what I'm talking about. But yeah, the reason I'm surprised is because we didn't really expect this. There was no looks like in practice. So, I'm going to show you guys what that looks like right now. So, we've got a few different examples here so you can see what it looks like. I mean, have a look at the models. This is where we get like a true test of, you know, how the models are. So, the prompt of this one was an anime of a bustling marketplace in a foreign city with colorful stalls and lively characters bering with vendors. The vibrant colors and energetic scene capture the cultural richness and excitement as the m market bustles with life. So, runway gen 4.5 at the top left we can see looks pretty good. A VO3, no audio also. I mean, all of these genuinely do look pretty good. I think Sora 2.0 Pro looks probably the best in this example. There are, you know, subtleness in the styles here. , and I think one thing that I realized when I was testing the models and what I was looking at is that certain cases are different for certain models. Like Sor 2 Pro is really good when it comes to anime and anime series, but VO and other models and cling, they aren't that good. But, you know, Runaway and Gen 4.5 when it comes to maybe the realism with humans and certain delicate actions, that's where the model excels. So, I think, you know, with videos, I've got a couple examples here. I've also got this bike one. , I didn't actually make this. There's a video on Twitter. There's an entire thread. I I'll leave a link to it. But, , it kind of shows you the specific cases. Like, look, Sora 2 Pro with no audio. It just doesn't look that good. But runway Gen 4.5, you know, when you see them leaning into a turn here, the prompt is, \"On a wind blown hillside, a team of cyclists leans into a sharp turn, dirt and gravel scattering from knobbyby tires, overhead tall grass, bends in waves, and the horizon curves gently under a cloudless sky. Through pounding hearts and measured breaths, each rider tracks the next, an unspoken rhythm carrying them forward. So basically we can see here that this one VO and Runway Gen 4.5 XL because I think there's a bit of complexity here and I think that's what Gen 4.5 after. So if you aren't familiar with Runway's goals, they're basically a company that wants to do I guess you could say become like the VFX level AI when it comes to video models. So, it's going to be pretty tough to do that when",
        "start": 1198.24,
        "duration": 2643.838000000001
    },
    {
        "text": "think that's what Gen 4.5 after. So if you aren't familiar with Runway's goals, they're basically a company that wants to do I guess you could say become like the VFX level AI when it comes to video models. So, it's going to be pretty tough to do that when and Clling, but I think their approach in terms of how they've designed the software, their user interface, I think they've got a solid approach and the fact that they are , you know, now currently leading the way, especially when it comes to video creation. I think that's a good sign. So, let's take a look at another one right here. We've got Cling as well. , Sora once again. And so this one the prompt is I don't know why the video wasn't playing but this one the prompt is through a window of submerged laboratory scientists observe marine life small submarines dock nearby and delicate coral fans sway in gentle currents outside offering a glimpse into the secret underwater world. So once again, all of these I mean one thing I know about like underwater scenes, there is usually you know great responses from video models with underwater scenes just because the glitchiness of AI models actually looks good in underwater. Runaway once again looks pretty good. Sora 2 looks good. Cling looks good. V3 kind of messes up here. But I think V3 is, you know, one of those models where it's confidently wrong about certain things which produces some really, you know, interesting responses. But we can take a look at another example here. This is where we have the prompt Venus cloud city adventure animated in 1950s futurism style. A gleaming silver city floats among Venus golden clouds. Sleek flying sauces zip between Arco Towers. And there is also text that says vacation on Venus sparkles into view. So of course text is you know pretty difficult. We can see that VO3 does manage to get the text right. Sora 2 Pro manages to get the text right. Runaway Gen 4 does manage to get the text right. And I mean, it is of course going to be a case- by case basis because I think everyone has their individual different use cases, but like I said, certain styles I think you're going to have to test the model. Anime and Duno is good. V3, of course, is good for some basic stuff. And Runaway is, I guess you could say, good for complex scenes. And then, so yeah, this one is called a hyperrealistic view of a rally car taking a sharp turn on a dirt track, kicking up clouds of dust. The camera follows the wheels in close-up as they spin through gravel, showcasing the car's controller speed. So, I think, you know, depending on what you actually want here, you know, you're really going to have to judge. Runway Gen 4 doesn't really look that best here. But here's",
        "start": 1339.36,
        "duration": 2906.318
    },
    {
        "text": "dust. The camera follows the wheels in close-up as they spin through gravel, showcasing the car's controller speed. So, I think, you know, depending on what you actually want here, you know, you're really going to have to judge. Runway Gen 4 doesn't really look that best here. But here's is that all of these responses are going to be different every single time. And I think the only way to realistically actually look at which of these models is the best is to probably take a best of four or best of five because you take a best of one. Sometimes one response is going to be good and then the next three are going to be terrible or sometimes the first two are going to be good and the next two are going to be terrible. So that's one thing when it comes to video models. You do have to burn through a lot of credits to kind of gauge if the model can actually do what you're asking it to do and sometimes you will have to adjust your prompt structure based on you know the different models. So I mean it it's all generative. It's all you know anyone's best guess. But if we're talking about you know new video models there is of course cling. So, Cling released video 2.6. This is Cling's first video model with native audio. Generate an entire experience with native audio. And it's it's like Clling's, you know, Christmas themed, you know, releases. They were doing so many different releases. Honestly, I probably should have done an entire video on them, but there is a bunch of different things that Clling are now trying to do for the video space. And I think they are doing a pretty good job. native audio being added with, you know, OpenAI and with V3, I think it's really good because it adds that level of immersion. Now that Clling is here as well, this seems to be really good in terms of the performance. So, I mean, I don't really use Clling that much. I think it's okay. I think, you know, VO is just so fast and efficient that it just makes so much more sense. But, of course, for those of you who want different models for different purposes and, you know, use cases, I'm sure that there will be, you know, Cling Video 2.6 six, whereas there's going to be something that you can do with that model. Of course, there was also Cling01, the multiv. Of course, there was Cling01, which is the multimodal unified model, which lets you do quite a lot of stuff. So, definitely take a look at this. And then earlier this month, we did have the, you know, robots that were remarkably human. There are actually so many of these. I think I released a video yesterday, you know, talking about how realistic the models are and just how crazy this entire game",
        "start": 1472.96,
        "duration": 3157.0380000000005
    },
    {
        "text": "a look at this. And then earlier this month, we did have the, you know, robots that were remarkably human. There are actually so many of these. I think I released a video yesterday, you know, talking about how realistic the models are and just how crazy this entire game but it is frightening the speed at which we're getting humanoid realistic robots. And I think one day we're just going to get that update where it's like boom, this new software update now means that look, the robot that was once dancing around can now actually, you know, go ahead, jump on a computer, use a mouse and keyboard, do all of these delicate tasks because essentially what these companies are showing us is that the hardware is capable. It's just that we need, you know, the software to actually be capable and the software is the hard part. So once that gets solved when within the next 5 to 10 years maybe considering there are you know billions of dollars being poured into it it's going to be a very interesting future. Now of course this is where we have engine's T800 model. And what's crazy about this release is that quite like the last one it was so realistic that people thought it was CGI. Even when I made a video stating that it's not CGI, people were saying, \"If you don't link those videos, I'm still not believing that it's real.\" I think I saw a few comments like that and I was like, \"Wow, that means we're really passing the uncanny valley now.\" And this video was one of them where all of the quote tweets and comments were saying that look, this is CGI. Now, in all fairness, I do have to say that, you know, companies in the past have done CGI demos because the robots simply weren't capable yet. And people did think that this was one of those, but it wasn't. It genuinely wasn't one of those demos where it was CGI. This was one of those demos where the robot was just, you know, trained in reinforcement learning in some kind of digital simulation and they've applied that software update to the robot. So, it now looks incredibly human. And the result is surprising. Surprising because we've never seen a robot move like this like that at all. That just looks absolutely incredible. You need an incredible level of balance, an incredible level of, you know, dexterity, flexibility, agility to be able to do that and land it. , and I think it's crazy because if you saw how slow robots were moving, you know, just 2 years ago, to having them do these crazy fly kicks, which are really strong, by the way, I'm going to show you guys in the next video just how strong these are. I can understand the skepticism regarding the models because not the models but the robots because",
        "start": 1599.44,
        "duration": 3416.637
    },
    {
        "text": "just 2 years ago, to having them do these crazy fly kicks, which are really strong, by the way, I'm going to show you guys in the next video just how strong these are. I can understand the skepticism regarding the models because not the models but the robots because progress by any means necessary. We've kind of exploded when it comes to robotics progress. Now remember how I spoke about those kicks being rather strong. I'm not sure why the CEO wanted to do this but he decided to get kicked by the robot. He decided to you know get kicked by his engine. So, this video is going around on Twitter and basically the CEO decided he wanted to test the strength of the robot and decided to pad himself up, then of course get the robot to do a kick and literally kicks him on the ground. Now, I don't know about you guys, but this it kind of concerned me cuz I was like, if this robot is that strong, it can, you know, kick a fully grown man who's got an entire padded set of gear on and it can, you know, fully down him. I mean, does is the future going to be like these robots running around kicking us down if we're if we're not wearing the right outfit? Is there going to be some kind of crazy dictatorship where these robots are being used in the wrong way? I really do hope not. But, you know, in terms of just abilities, this is kind of cool. But like, I mean, I hope that that, you know, this robot revolution is not one that we're running towards where these robots do take control because if a robot has that much strength, I mean, that's pretty pretty concerning if you ask me. And at the least I think it's pretty funny to see this guy get kicked. I mean I don't know what the text says but yeah that's pretty funny if you ask me. Now as we spoke about mistrial before you know Europe is finally scaling back part of its tough you know AI and privacy laws. So it's actually marking a major shift after years of positioning itself as the world's tech regulator. So you know the Europe has been under pressure from big tech and the US government and internal critics because you know they just need to loosen parts of the GDPR without delay of enforcement of major AI act rules. So companies are going to find it easier to share anonymized or pseudo anonymized data and AI developers could legally use personalized data for training as long as they follow GDPR requirements. And of course there's AI act delays. There are strict stricter rules for high-risisk AI systems, those affecting health, safety, or rights. And those are actually going to be pushed back until the EU confirms that necessary standards and tools exist for",
        "start": 1732.24,
        "duration": 3682.958000000001
    },
    {
        "text": "training as long as they follow GDPR requirements. And of course there's AI act delays. There are strict stricter rules for high-risisk AI systems, those affecting health, safety, or rights. And those are actually going to be pushed back until the EU confirms that necessary standards and tools exist for to be fewer cookie pop-ups, which is of course good. , and hopefully, you know, EU doesn't have too strict regulation because I think the AI race, unfortunately, is one of those races where it is winner takes all or winner takes the majority of it. So if Europe, you know, does fall behind, they're only going to have themselves to blame. So they really need to find that balance between, you know, not making Kilobots AI completely legal and staying competitive in the AI race. And I think if these guys actually talk to the people in Silicon Valley, the people using the models, the people developing the models, they'll find out that a lot of their, you know, laws, like a lot of the times the people that are making the laws aren't the ones that are, you know, on the ground level in the tech. They're honestly often times very disconnected from the implications of the laws that they make. And you know this is this is in just many different industries. So I do hope that somehow they managed to fix this and get that changed because it it's not essentially you know too good for Europe at the moment. you you've seen with Mr. large. Of course, recently we had Titans and Mirz and I think this is the biggest thing, probably one of the biggest announcements. And you know, we'll probably go on to do a number of headlines because Google did solve one of AI's biggest weaknesses, which was memory. And you all know that, you know, AI, it doesn't continually learn. You use the model, you use the model, it's a fixed state model. It's just a snapshot in time, kind of like a history, a moment in history, I guess you could say. And this kind of solves it cuz it kind of has this continual learning. So there were two papers, Titans and Mirs. Titans is a brand new AI architecture that gives the models actual long-term memory. 2 millions tokens of context entire books remembered perfectly. And it basically copies how human brains work and it has a surprise metric that prioritizes unexpected important information and ignores the boring routine stuff exactly like your brain does. And even crazier is that it can, you know, learn and update its own memory while it's running, which is something that no other AI can do. And then MRAZ is the theoretical breakthrough underneath it all. And it reveals that every major AI architecture like transformers, RNNs, members, everything is just secretly doing the same thing just differently. And this framework opens up the door to designing way better memory systems. And",
        "start": 1867.279,
        "duration": 3949.5980000000013
    },
    {
        "text": "other AI can do. And then MRAZ is the theoretical breakthrough underneath it all. And it reveals that every major AI architecture like transformers, RNNs, members, everything is just secretly doing the same thing just differently. And this framework opens up the door to designing way better memory systems. And So overall, that was, you know, very, very incredible by Google because not only did they just release a front end model, they also just released a new research paper that shows they're going to be moving in a completely new paradigm potentially for the future. So, if you guys enjoyed this video, hope you guys have a wonderful day. I'll see you guys in the next one.",
        "start": 2002.399,
        "duration": 3988.476000000001
    }
]