[
    {
        "text": " So, Google Gemini 3 is finally here, and this model certainly lives up to the hype. From breaking benchmarks on multiple different things, this is the most intelligent model you have access to right now. So, let's dive into everything that we really need to know. For 2 seconds, I'm going to show you guys this short Google intro and then we're going to get into the really juicy stuff. Gemini has been multimodal since the beginning. Our Gemini 1 models could see, hear, understand vast amounts of information, and natively generate across modalities. Gemini 2 took the next step with advanced reasoning, enabling AI agents to think, code, and take action. We're taking all of this to the next level with Gemini 3. Gemini 3 is the strongest model in the world for multimodality and reasoning. It's our most intelligent model that helps you bring any idea to life. Starting today, we're rolling out Gemini 3 in our products so you can experience it for yourself. So, one of the things I want to show you guys first is the multimodal understanding. Most people don't realize you could literally put in a PDF, say that you want to learn about something and have it visualized exactly what's in that PDF. You see, Google isn't just showing 3D visuals. This is true multimodal understanding at a level that we haven't seen before. It's understanding a PDF and the hidden text behind them. It's looking at it. It's making the code. It's not just reading the word. It's understanding absolutely everything. And it's building an entire educational app just from scratch. Not a description, not a summary. It has a full UI, a clean layout, navigational buttons, typed style, typography. This is really, really cool. It's even got 3D visualizations that really just change the game for those of you who are visual learners. And you have to understand that this being able to nail the design in zero shot is just completely insane. This is the kind of thing that really starts to help people. This is the first time that an AI model can read a paper, understand the core idea, then build a visual explainer, turn it into an app, all while breaking it into steps and guiding you through the concepts. We've never had an AI that's been able to do all of this as fast and as efficient as this before. Sure, Claude has existed, but the level that Gemini 3 is able to understand due to its multimodal capabilities is completely insane. And remember guys, this is a multimodal model. So, this just completely takes the cake. Most models are not this good when it comes to cross multimodal benchmarks because they really can't see and hear. But Google is completely different in that area. Remember how we just spoke about the crazy multimodal nature of Gemini 3? Well, Gemini 3 gets even crazier because it can analyze full videos. That's right. Gemini has an",
        "start": 1.12,
        "duration": 317.75999999999993
    },
    {
        "text": "comes to cross multimodal benchmarks because they really can't see and hear. But Google is completely different in that area. Remember how we just spoke about the crazy multimodal nature of Gemini 3? Well, Gemini 3 gets even crazier because it can analyze full videos. That's right. Gemini has an means it's able to use its multimodal capabilities to actually look at your video and analyze it in a way that gives you really good advice. So Gemini isn't just, you know, looking at the colors and shapes. It's breaking down your stance, the paddle angle, the footwork, the timing, the reaction speed, the positioning. That's biomechanical analysis, which is something that only coaches could do. [music] The AI seeing the pattern in your motion, not just the motion itself. And remember, Gemini is an AI. It's going to be able to pick up on things that you don't even notice. Humans miss things when they don't watch recordings of themselves. But Gemini is able to catch the tiny wrist tilts, leaning too far forward, inconsistent follow through, late split steps, inefficient swing arcs. This example of someone playing paddle and then just asking Gemini, \"Hey, how can I improve?\" This is a much bigger example of what video analysis can really do for you. You have to understand that this is technology that is available to everyone. Not everyone has a pickle ball coach, a friend who plays, a club membership, or a time for one-on-one training. And Gemini can basically turn a one 10-second clip into a personalized coaching session, which is democratizing skill improvement. So, think about this. If it can do pickle ball, it can do tennis, it can do basketball, it can do golf swings, it can do football, this is the step to an AI coach that can act like a seasoned trainer. And remember, multimodal understanding doesn't just have to stop at analysis for pickle ball. It can analyze your speaking, your walking. There's a million different things that it could do. Maybe you're in the gym and it analyzes your form. Multimodal capability truly just unlocks a vast range of capabilities that I guess your mind is truly the only limitation. Now, once again, if we dive even more into the multimodal capabilities, if you've got Google AI Studio, which I'll dive into a little bit later, accessing Gemini 3 and using the multimodal capabilities to literally turn any image into an interactive piece of work is absolutely incredible. Gemini 3 can look at this messy drawing, figure out what's a header, what's a button, then turn the whole thing into a clean HTML CSS. It turned a simple picture of a chessboard into a full visual game. This is crazy because it collapses the design to code workflow in a single step. Which means that sometimes you might not need a Figma, you might not need a web developer. You might not even need to be that technical. You can",
        "start": 160.16,
        "duration": 583.6799999999998
    },
    {
        "text": "a chessboard into a full visual game. This is crazy because it collapses the design to code workflow in a single step. Which means that sometimes you might not need a Figma, you might not need a web developer. You might not even need to be that technical. You can photo, and the model is going to turn this into a working website. Most multimodal models can describe the website, view can rewrite the text in it, and almost none can interpret the layout, generate a usable running code that also respects that layout. Gemini 3 here is doing deep visual understanding [music] aentic coding and cross multimodal consistency which is incredible end-to-end product creation idea sketch photo website fully interactive not 10 tools this is one shot you have to understand that this is going to open up a new area of capabilities like I said truly truly grasp the fact that this is a multimodal agentic model with a vast range of capabilities enabling it to do things across every level from reasoning coding and creating so so that you can truly get what you need to get done. Now, one feature that most people don't know exists is Google's new AI mode. This is an experimental feature built directly into Google Search that transforms the traditional search experience into a conversational AI powered assistant. Instead of just returning a list of links, AI mode uses Google's Gemini model to generate comprehensive synthesized answers to your questions, often pulling together information from multiple sources, including real-time data, images, and even product listings. And this makes searching more intuitive, dynamic, and capable of handling complex multi-step queries, basically like in a chat interface. Now, what's good is that you now have thinking enabled, which means that AI mode is leveraging Gemini's advanced reasoning and multimodal capabilities to deeply understand your questions. Whether you type, speak, or upload an image, when you ask something, the system breaks your query into subtopics, searches across multiple stories, multiple sources simultaneously, then synthesizes your results into a clear conversational answer. But now, the fundamental new approach you can see is that Gemini is able to generate things on the fly. EI mode is designed to be more interactive, letting users explore the topics in depth, clarify their needs, and get those truly tailored responses that evolve as the conversation progresses. Which basically means it's like having a knowledgeable assistant who can not only answer your questions, but also help you dig deeper, compare the options, and even take real world actions. This means that you now no longer need to jump between apps or platforms to get creative or analytical help. Everything is going to be happening right inside Google search, making the experience smoother and more intuitive. So, you have to understand that Google search is going to change. Gemini 3 is embedded in there. And this is a shift from passive information retrieval to active generative assistant designed to help",
        "start": 294.96,
        "duration": 879.3600000000006
    },
    {
        "text": "is going to be happening right inside Google search, making the experience smoother and more intuitive. So, you have to understand that Google search is going to change. Gemini 3 is embedded in there. And this is a shift from passive information retrieval to active generative assistant designed to help create, explore, and act. Making search much more powerful, flexible, and personalized. For me, I'm probably going to be using this almost every day because I do find that whilst in the initial days AI mode was truly terrible, right now with Gemini 3, this kind of search cannot be beaten. Now, by combining Gemini 3 with improved tool use, Gemini 3 can take complex actions on your behalf. This is an example of Gemini 3 organizing your inbox. Google's AI Ultra subscribers can try these agentic capabilities in the Gemini app with Gemini agent today. Basically, Gemini is no longer just a chatbot. It's evolved. It's a true digital worker. And honestly, it's quite impressive. You have to understand that almost everyone has a Gmail and now it's able to reason and take actions, not just reply. The most things that it can do is, you know, understand your emails. It can understand the context, decide what matters, and then take actions inside your inbox on its own. This is the first time that Google is actually showing real autonomy inside your personal apps. I don't know about you guys, but this is probably going to save me a huge amount of time. Doing multi-step planning like a human assistant. This is incredible. Scanning your inbox, classifying different types of emails, identifying the tasks, sorting by priority, summarizing, generating responses, choosing which tools to use, that's basically how a human personal assistant handles a cluttered inbox. And for it being able to understand your entire workflow, not just single tasks, that is incredible. Gemini 3 here shows us that it can manage tasks, make decisions, follow instructions without handholding, and use external tools correctly. This is basically Google's agent reveal moment. And they've shown this in a few demos before, but we've never really been able to see it utilized effectively. So, this is really cool and I can't wait to use this because my inbox is basically a mess at this point. And I would expect this thing to evolve as time goes on. I mean, a lot of the times we were promised AI agents doing work for us. And I do think that with Google just embedded with Gemini 3 across multiple different areas, we can finally start to see how this is impacting the efficiency of our day-to-day lives. Now, Gemini 3 coding is absolutely incredible. Everyone knows what Sonet 4.5 is, but trust me when I tell you, Google is not to be slept on. The agentic coding and autonomous workflows where it can autonomously plan, scaffold, and execute multi-step coding projects from a single prompt, handling not just single files, but",
        "start": 444.639,
        "duration": 1155.7590000000002
    },
    {
        "text": "is absolutely incredible. Everyone knows what Sonet 4.5 is, but trust me when I tell you, Google is not to be slept on. The agentic coding and autonomous workflows where it can autonomously plan, scaffold, and execute multi-step coding projects from a single prompt, handling not just single files, but you know, help you in vibe coding. It's pretty insane what Gemini can do. I've been on Twitter for quite some time. And the amount of different examples that [music] there are, it's truly, truly incredible. The model also supports a massive 1 million token context window, enabling [music] it to read, digest, and reason over entire code bases or very large engineering documents, which is a crucial factor for legacy code migration, software testing, and code review. And through Gemini CLI, complex Unix shell commands or automation scripts are generated from plain English requests and formatted output is passed back into readable explanations. It's truly truly incredible what this is able to do. It basically delivers top tier results on nearly every coding related benchmark. Let me show you guys exactly what I mean. I do apologize for this kind of strange crop, but it's the only way that I could truly show you guys just how powerful the models are at coding. Right here are the three, I guess you could say, most important coding benchmarks. We've got LiveBench Pro, Terminal Bench 2.0, and FWE Bench verified. Google is basically now the number one leader across all of these benchmarks. And let me just explain to you why that's such a big deal. You see, number one is, you know, live codebench pro. This is competitive coding difficulty. How smart is the model at pure algorithmic problem solving? This is like giving AI code forces and ICPC level programming problems the same brutal puzzles top competitive programmers solve. A higher ELO basically means that you have stronger raw intelligence. Gemini 3.0 coming in at 2,439, nearly double what other models have is pretty incredible. Anything above 2,000 is elite human competitor. By the way, this means it's not just good at coding, it's strategic and mathematically sharp. Terminal Bench 2.0 O is where you basically say can the model act like a real developer using a real computer. This measures the agentic terminal usage and using a real shell environment. This means it's not writing code. It's navigating folders, running commands, installing packages, and editing files. And it's also fixing errors through the terminal. Basically, think of it like a real world engineer. And Gemini 3 again scores 54.2%. That's a large jump from the previous models. And this is basically the benchmark that predicts AI developer agents. Meaning that the replace the junior engineer capability is really really starting to show here. And of course the last one SW bench verified. Can the model fix real bugs in GitHub projects in one attempt? SWB bench uses actual GitHub repos and real historic",
        "start": 585.2,
        "duration": 1453.1190000000006
    },
    {
        "text": "that predicts AI developer agents. Meaning that the replace the junior engineer capability is really really starting to show here. And of course the last one SW bench verified. Can the model fix real bugs in GitHub projects in one attempt? SWB bench uses actual GitHub repos and real historic and it must produce a real patch that passes the test. Here we can see that Gemini 3.0 Pro actually comes in just behind Claude Sonic 4.5. However, I would say that it's a remarkable jump from the previous iteration of the model. And to be real with you guys, it's only literally 1% behind the other models. So, if we're being honest, it doesn't seem like that big of a difference. But what we will have to see is the actual vibes of the model. What I mean by that is how do users report the model is coding. So far, I've tested both of these and I don't really have any crazy sort of projects that I'm going to be coding on, but I can't really see a difference. But as time goes on, you're going to start to see those differences become more and more pronounced. But considering that Gemini 3.0 is better at the terminal bench and the live codebench pro, I can't see how overtime people wouldn't prefer Google Gemini to these other models, considering that overall it has a more robust level of intelligence. Now, that wasn't the only thing that Google have done when it comes to the code. Google have also launched anti-gravity, their new agentic development platform, which [music] is basically Google's version of an AI coding assistant that can actually do tasks for you, not just suggest code. Think Cursor and Windsurf, but from Google. So, it's basically working with multiple AI agents at once, several coding assistants working together, and it's built on top of their IDE, which is their code editor. And this uses Gemini 3.0 Pro. So, take a look. Google is basically going after cursor here. Every breakthrough in model intelligence for coding encourages us to rethink what development should look like. Gemini 3 is our latest such model advancement. So we went out to build the next step change of an IDE. Introducing Google Antigraph, a new way of working for this next era of agentic intelligence. [music] It is the ideal agentic development home base. Does it have an IDE? Yes, but also has a whole lot more. We started with the core ID and added pieces that evolved the IDE towards an agent first feature such as browser use, asynchronous interaction patterns and an additional novel agent first product form factor, helping you experience liftoff. Your new focus is architecting the solution, not implementing every single step. So congratulations, you have been elevated to a manager of agents. Instead of an agent living in your tools, your tools are now instruments for many agents. The agent begins to work autonomously by creating an",
        "start": 736.16,
        "duration": 1726.9600000000005
    },
    {
        "text": "experience liftoff. Your new focus is architecting the solution, not implementing every single step. So congratulations, you have been elevated to a manager of agents. Instead of an agent living in your tools, your tools are now instruments for many agents. The agent begins to work autonomously by creating an multiple surfaces to complete your request. Our agent is more capable tackling complex tasks and even doing so in parallel, letting you ship more than ever before. You can verify your code quality at a glance, then ship with absolute confidence. The agent can automatically take browser screenshots of bug fixes or even screen recordings of feature implementations, proving the work was done and tested. By providing these instant, verifiable artifacts, you can confidently trust and merge the code without hours of manual review. Anti-gravity eliminates the pain of trying to polish nearperfect results by easily guiding the agents 90% solution all the way to 100%. Now, let's say the agent produces a landing page mockup with Nano Banana and you now want to make some UI adjustments. You can give visual comments just like a designer, letting you leave feedback exactly where the problem is. And you can also leave comments on code diffs or feedback on screen recordings of browser use. It's a brand new way of collaborating with agents, and you're going to love it. Say goodbye to what held you down before. Welcome to Google Anti Rabbit. Now, of course, what I wanted to do with this video is I wanted to focus on the raw capabilities of what Gemini 3.0 can actually do for you before diving into some of the crazier benchmarks that some of us would like to pay attention to. I mean, honestly, on this list, there are very few examples where Gemini 3.0 is beaten. And the examples where Gemini 3 takes the cake, it's honestly quite by a large margin. Now, I'm actually going to dive into some of the most important benchmarks because most people will miss these, but it truly just shows you how far ahead Google is. This is their third iteration of the model and by all means, this is incredible to go from Bard to this where honestly they were quite behind and now they are leading the pack, innovating, showing people exactly what they should be doing. First off is LM Marina. This is an open community-driven benchmarking platform used to evaluate and rank AI models through real user feedback on head-to-head battles between model outputs. Essentially, you just pit two AI models against each other anonymously using the same prompt. Users will then vote on which answer they believe is better based on the criteria like helpfulness, clarity, or creativity. And each upvote updates a dynamic leaderboard using an EL rating system. So you can essentially see here that literally until a few hours ago, Grock 4.1 thinking was arguably the best model, but now Gemini 3.0 Pro takes the",
        "start": 874.88,
        "duration": 1996.6380000000006
    },
    {
        "text": "better based on the criteria like helpfulness, clarity, or creativity. And each upvote updates a dynamic leaderboard using an EL rating system. So you can essentially see here that literally until a few hours ago, Grock 4.1 thinking was arguably the best model, but now Gemini 3.0 Pro takes the subjective because whilst yes, it does matter that people are actually using the model on a day-to-day basis and they actually like the model, I think that this benchmark is probably the least impressive. Very impressive, but the least impressive because what I'm about to show you next is just insane. So this is AR AGI 2. And if it looks a little bit confusing, that's fine. Just look at the fact that Gemini 3.0 Pro is quite up there. So, Arc AI2 is basically considered one of the hardest benchmarks for AI because it's designed to test its core reasoning ability of the models and [music] it's abstract problem solving and adaptability that goes far beyond, you know, standard memorization or pattern matching. It intentionally features tasks that are straightforward for humans but exceptionally hard for current AI systems which basically tries to highlight the weaknesses the previous benchmarks either missed or underestimated. The problem is that current benchmarks are getting really saturated so they had to make something that current AIs would struggle with. Now remember guys, each task on the ARC AGI2 benchmark requires novel reasoning. They're basically visual tasks that require them to infer underlying rules, generalize from very few examples. And essentially, they kind of want the model to sort of think on the fly and kind of like act like a human. So, so you can see that Gemini 3.0 Pro on the Rhi2 benchmark, or at least Deep Think, is literally miles ahead of any other model. A literal jump ahead in terms of capability. And I would argue that this is one that's super important because the ARC AGI2 benchmark is one that is important because it shows us that Gemini 3 deep think is actually approaching the human baseline. Humans averaged around 60% on this benchmark and reaching, you know, the 45% brings the AI into the range of advanced human problem solvers on some of the hardest reasoning challenges available. This means that they're compressing years of anticipated progress into literally one model generation. That's crazy. I don't think you understand. And I think it's because this model is a true multimodal expert. If you do ever wonder what some of these arc AI questions do look like because people tend to talk about it. This is kind of what it does look like. I'm not going to spend time to explain to you exactly how to solve this. I mean, you could pause the video and try to solve it yourself, but these are the kind of questions where the AI struggles with. So, vending bench is designed to test how well can LM manage a vending",
        "start": 1011.519,
        "duration": 2279.9979999999996
    },
    {
        "text": "spend time to explain to you exactly how to solve this. I mean, you could pause the video and try to solve it yourself, but these are the kind of questions where the AI struggles with. So, vending bench is designed to test how well can LM manage a vending well can it manage a vending machine but you know the idea is basically to move away from short isolated tasks and see whether AI can sustain coherent logical and profitable decisions across thousands of steps simulating months of business operation in a single run. Now the AI agent must perform routine but independent tasks ordering projects managing inventory setting prices responding to daily events yada yada yada boring boring business stuff. But the benchmark stresses long-term coherence, the agent's ability to stay organized, remember outstanding orders, adapt the strategy, and avoid meltdowns. And we can see here that compared to the other models, the only model that truly starts to, you know, increase the balance over time is Gemini 3.0 Pro. That's why I think this model is truly a jump in terms of reasoning capabilities. These benchmarks, Arc, AGI 2, Vending Bench, this is where you start to see the real raw capabilities of the model start to shine. This is where you start to see the models come into that human level territory where they can actually start to do things that other humans would be able to do. We can see that whilst other models are ridiculously smart like Gemini, you know, 2.5 Pro, GB2 5.1, Claude Sonic 4.5, and they do marginally increase their balance over time, Gemini 3, we can literally see that the chart is just going up a lot steeper than any other model. Now, here's another benchmark that most people have missed is browser use. So once again we can see a huge jump from the other frontier models. Gemini 3.0 thinking is at a 72.9%. And if you don't know what browser uses, this is an open source AI framework that basically enables LLM or agents to interact with and control web browsers using natural language instructions. Basically simulating humanlike web activity and automation. So essentially all they're trying to do is see how well can an AI control a computer/ a browser like a human. And once again, we're seeing a massive jump from the Gemini series of models compared to the previous gen of models. I genuinely didn't think it would be this much of a huge jump, but considering the model is that good in terms of the multimodal capabilities, I think considering they've designed it with all of these aentic capabilities, it only makes sense that it just really goes that next step. Of course, you've got Gemini 3 and Gemini Deep Think is basically the model that is truly truly smart. Deep Think basically excels at some of the most challenging AI benchmarks and there's a lot to discuss, but honestly, if you",
        "start": 1155.12,
        "duration": 2566.239
    },
    {
        "text": "sense that it just really goes that next step. Of course, you've got Gemini 3 and Gemini Deep Think is basically the model that is truly truly smart. Deep Think basically excels at some of the most challenging AI benchmarks and there's a lot to discuss, but honestly, if you a minute, we can see just how far ahead the model is. I've already spoken a lot about how Gemini 3 is and I've already spoken about this on the other AI benchmarks, but we can see that Deep Think is truly truly a step ahead. So, the fact that the RAW model is that far ahead, I mean, it's honestly quite surprising that Google was able to do this. And when you add thinking on top of that, the capabilities just jump even further. Now, we just spoke for quite some time about what this model can do. So, what can this model not do? Well, one of the things I wanted to see was what can the model not do? And eventually, I came across this picture on Twitter, which is just a bunch of fingers in perhaps an AI generated images. And I asked this model several times, guys. I tried different prompts. I tried different ways to reason. I said, \"How many fingers? Count each finger. Think step by step.\" , [music] and it didn't. It just couldn't understand that there were more than five fingers on this model. Now, I think this is not to downplay what Gemini 3.0 is doing here, but to show you guys just how different LLMs are from humans in terms of their raw understanding of what's going on. Even though we can clearly see eight digits that are weird overlapping, llms, you know, fail to see the illusion. And it's because LLMs don't often have a clean 3D representation of hands or fingers. They don't see that like this part belongs to finger A, this part belongs to finger B. They just see local edges, textures, skin curves, joints, shadows. So when the fingers are overlapping like this, the model just collapses them into fewer features and thinks that the hand essentially has fewer fingers, which is why the models basically say, I'm so sure that there are five fingers even when there are 7 to eight visible. So these vision models sometimes the vision, you know, does struggle, but I think this is a real edge case that is probably going to be solved in the future versions. But I wanted to show this cuz I thought it was pretty funny that it really couldn't solve this. And it didn't seem like that difficult of a task, but it was there. So, let me know what you guys get up to with Gemini 3.0 Pro. There'll be much more videos coming on the subject.",
        "start": 1300.32,
        "duration": 2799.197000000003
    }
]