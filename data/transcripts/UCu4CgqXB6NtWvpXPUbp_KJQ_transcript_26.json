[
    {
        "text": " Please welcome to the stage president semiameas Joe Stkunis. It's Semicon and we are focused on AI today. So it makes sense to start with a presentation from a leader of the most valuable company in the world and the leading semiconductor supplier to the AI community, Nvidia. Dr. Timothy Costa, general manager industrial and computational engineering at NVIDIA, spearheads the strategic development and implementation of solutions for critical industries including semiconductors. He leads the industrial engineering and quantum verticals as well as responsibility for the CUDA X product line. Timothy will explore how AI superconducting powers a new era of smart manufacturing. Our smart manufacturing pavilion yesterday was just overwhelmingly crowded. It is the hottest topic here today. From AIdriven chip design and physics accurate simulation to autonomous robotics and photorealistic fab digital twins. His presentation is titled AI supercomputing for the next generation semiconductor design and manufacturing. Please join me in welcoming Dr. Timothy Costa to the keynote stage. Thank you Tim. [music] All right. How's my mic? Everyone hear me? Great. so thank you to the organizers to all of you for being here to all the other great speakers who've made this a wonderful event. we are going to need all of 25 minutes to get through this presentation. So I won't I won't belabor that too much longer. I'm here to talk about AI supercomputing for next generation semiconductor design and manufacturing. That is a mouthful. so I think if we were to kind of summarize what we want to talk about here, it's really the the virtuous cycle between AI and the superconducting industry. We're going to talk about a couple of large opportunities that are in front of the semiconductor ecosystem today. We're going to talk about how AI can help capture those opportunities, help accelerate the work of the semiconductor industry across its value chain, ultimately allowing it to capture that opportunity which is in AI and create that virtuous cycle. Let's get going. All right, so we are at the start of a new industrial revolution. This has a TAM in the trillions and is driven by two kind of two key areas. Just as dynamos turned energy into industrial productivity in the first industrial revolution, AI factories transform energy into intelligence. Physical AI will transform all industries, automating the world's millions of factories and hundreds of thousands of warehouses. Every building will become autonomous, orchestrating autonomous robots within it. To capture that opportunity, continuous innovation is required across a very simplified cartoon of the semiconductor design and manufacturing workflow. From chip design through to system integration, innovations are needed in energy efficiency, materials, design tools, and manufacturing processes. And if I were to do this talk in four slides, we would spend the rest of time on this one because this is really the meat of the conversation. It's about how we can leverage AI and accelerated computing to to help with that innovation on the previous slide and",
        "start": 0.16,
        "duration": 392.67999999999995
    },
    {
        "text": "if I were to do this talk in four slides, we would spend the rest of time on this one because this is really the meat of the conversation. It's about how we can leverage AI and accelerated computing to to help with that innovation on the previous slide and opportunity that is presented to us with AI factories and physical AI from accelerated semiconductor design to factory digital twins for fabs agents which will increase the productivity of semiconductor engineers AI and ML for mask and wafer detection and yield optimization. These are just a few of the areas where we see an opportunity and already some implementation of AI and accelerated computing to move our industry forwards. Now, I suspect most people in the audience have no idea who I am. You've probably heard of Nvidia, though. Despite that, I think I will go through just a brief history of Nvidia because it helps position what we're here to talk about today. , Nvidia starts many years ago as a GPU chip company. over time transitions into a systems company and a data center company but today is going through another transformation into an AI infrastructure company. AI infrastructure is incredibly complex. It starts with land, power and shell. It encompasses CPUs and GPUs and DPUs and nicks and switches and memory and storage. The list goes on and on. It's orchestrated by complicated software stacks at the user and system level. And these are what these are what's allow us to deliver AI factories to all the industries which will benefit from them in the future. And ultimately AI infrastructure and AI factories are a collaboration with the semiconductor ecosystem. So I actually want to start off here by playing a video that highlights the complexity of putting together one element of this AI infrastructure. It's something that we put together for you know for a few events in the past and I've updated for here today and it's just it's a really great exploration of some of this technology. Blackwell is an engineering marvel. It begins as a blank silicon wafer. Hundreds of chip processing and ultraviolet lithography steps build up each of the 200 billion transistors layer by layer on [music] a 12in wafer. The wafer is scribed into individual blackwell die tested and sorted separating the good dyes [music] to move forward. The chip on wafer on substrate process attaches 32 Blackwell dyes and 128 HPM stacks on a custom silicon interposer wafer. Metal interconnect traces are etched directly into it, connecting Blackwell GPUs and HBM stacks into each system and package unit, locking everything into place. Then the assembly is baked, molded, and cured, creating the Blackwell B200 Super Chip. Each Blackwell is stress tested in ovens at 125\u00b0 C and pushed to its limits for several hours. Robots work around the clock to pick and place over 10,000 components onto the Grace Blackwell PCB. Meanwhile, custom liquid cooling copper",
        "start": 203.12,
        "duration": 750.6039999999997
    },
    {
        "text": "baked, molded, and cured, creating the Blackwell B200 Super Chip. Each Blackwell is stress tested in ovens at 125\u00b0 C and pushed to its limits for several hours. Robots work around the clock to pick and place over 10,000 components onto the Grace Blackwell PCB. Meanwhile, custom liquid cooling copper optimal temperatures. At another facility, Connect X7 Super Nix are built to enable scale out communications and Bluefield 3 DPUs to offload and accelerate networking, storage, and security [music] tasks. All these parts converge to be carefully integrated into GB200 compute trays. MVLink is the breakthrough high-speed link that Nvidia invented to connect multiple GPUs and scale up into a massive virtual GPU. The MVLink switch tray is constructed with MVLink switch chips providing 14.4 terabytes per second of all to all bandwidth. MVLink spines form a custom blindmated back plane with 5,000 copper cables [music] connecting all 72 black wells or 144 GPU dies into one giant GPU delivering 130 tab per second of all to-all bandwidth more than the global [music] internet's peak traffic from around the world parts arrive to be assembled by skilled technicians into a rack scale [music] AI supercomput In total, 1.2 million components, 2 m of copper cable, 130 trillion transistors, weighing nearly 2 tons. Blackwell is more than a technological wonder. It's a testament to the power of global collaboration and innovation, fueling the discoveries and solutions that will shape our future everywhere. We are driven to enable the geniuses of our time to do their life's work. And we can't wait to see the breakthroughs you deliver. It's such cool video. I [applause] think for anyone working in this industry, I've seen it a few times now and still get goosebumps. To those walking in, the best seats in the house are still available. If you'd like to to jump up here. so yeah, it's really amazing to see the complexity both of the hardware. and I think just as impressive to me is the partnerships involved in creating that AI infrastructure. but of course AI infrastructure is is not just hardware. it's more than the CPUs, GPUs, GPUs, nicks, switches, racks, all of that work together. up at the top here is is a is just as important and that's the software layer. It encompasses libraries which bring accelerated computing and AI into all different industries. It encompasses AI and AI APIs and microservices to bring both agentic and physical AI and AI physics to bear on these challenging problems. and it encompasses microservices and and APIs to to allow people to create digital twins. and that'll be very important as we start to talk about physical AI towards the end of this presentation. But let's start here. If you have been to a GTC, you have seen this slide. we Jensen shows it every time he speaks and that's because it is absolutely foundational and fundamental to Nvidia. This is accelerated computing",
        "start": 398.88,
        "duration": 1106.0069999999994
    },
    {
        "text": "to talk about physical AI towards the end of this presentation. But let's start here. If you have been to a GTC, you have seen this slide. we Jensen shows it every time he speaks and that's because it is absolutely foundational and fundamental to Nvidia. This is accelerated computing strategy. When we look at how should we go about bringing the value of accelerated computing to partner with a new industry ecosystem, we start by saying well what are the common APIs and workloads that we could build a library for so that we can provide the building blocks as a platform and so solution providers in those industries can add value from accelerated computing. So whether we're talking about signal processing and 5G and 6G radio with aerial and Siona weather analytics and climate with Earth 2 quantum computing with Kqu Quantum and CUDAQ we enter these industries by building libraries to support the ecosystem and accelerate solution providers that's no different in semiconductor and we're going to dig in on this a little bit but briefly Warp Coups and CU litho are all used heavily within the semiconductor industry today and have much more opportunity to to transform it moving forwards. While this slide tends to stay pretty fixed GTC to GTC, the number [clears throat] of libraries and the APIs that we have certainly does not. I've been working on CUDAX libraries since I started at NVIDIA. Many of these did not exist when I started and certainly the API suites within them have grown tremendously. , so it's a core part of our strategy to this day in terms of how we enter new markets to partner with those ecosystems. Those partnerships are really important part of the strategy. And so we're thrilled to see applied materials, cadence, ka, lamb research, seammens, and synopsis engaging with us on accelerating the workloads of semiconductor manufacturing and design through CUDA X adoption. It looks like the slides got a little bit wonky here, but that's fine. so here we have structural analysis, we have lithography, we have discrete element methods, we have circuit simulation, we have inspection, we have CFD and we have TCAD. so all kinds of workloads which are highly relevant to semiconductor design and manufacturing and seeing between 20 and up to 100x of TCAD performance boost from integration of of CUDAX libraries and the TCAD example we will take a look at in a little more depth in a few slides I do want to call out two libraries in particular this one I suspect most folks have not heard of KUDSS is relatively new it's roughly a year old that may not be exactly accurate but it's close but gaining a lot of traction in this ecosystem adopted by applied materials, cadence, Samsung synopsis and TSMC. This is a it's a really great example of the CUDA X strategy in the sense that it what it does is it optimizes a core",
        "start": 606.24,
        "duration": 1401.1239999999991
    },
    {
        "text": "be exactly accurate but it's close but gaining a lot of traction in this ecosystem adopted by applied materials, cadence, Samsung synopsis and TSMC. This is a it's a really great example of the CUDA X strategy in the sense that it what it does is it optimizes a core different engineering workloads that is direct direct solvers. [clears throat] And you can see really phenomenal performance impact here on the on the kernels that it provides from reordering linear solve and matrix factorization going from 1160 and 80x faster by adopting the library kitho you probably have heard of this has been talked about in many many GTCs over the last few years but this is our library to accelerate the building blocks of computational lithography for our partners for our EDA partners who who provide those tools and I've been thrilled to work with Samsung Synopsis and TSMC on this over the last few years. Just as important as the CUDA X libraries are for accelerating the core workloads of of semiconductor design and engineering and all other industries, we also want to take a look at what can the impact of AI physics be. Over the last year, year and a half, we've seen a tremendous growth in the adoption the enthusiasm for the development of new model architectures and AI physics for really all manufacturing industrial industries. And so if we ask ourselves well where can that intersect with with semiconductor the answer is in many many places. In some cases these are places where model architectures and models have been developed by other industries already. things like thermal analysis, stress analysis, CFD. In others, they're very particular to semiconductor and we see opportunity for for AI physics to to have a a massive impact on those workloads. Those are places like computational lithography, timing and power analysis, physical signoffs. But really across this cartoon simplified semiconductor design and engineering flow, we see the opportunity for AI physics to intercept in many places. This is all possible with Nvidia Physics Nemo. This is our framework for AI physics. It starts as a highly GPU optimized PyTorch. It's fully open source. And what we add on top of PyTorch is some APIs and features to enforce physics constraints into the models that are being trained. The result of that is you get a high more accurate model. and because it's highly GPU optimized, you get great performance on Nvidia Nvidia hardware. But at its core, this is PyTorch. so if you're comfortable using PyTorch, you'll be comfortable using physics and email. Okay, let's take a look at one use case in particular. so this is one of my favorite slides really in in any talk. and then the reason for that is that before we dig in at all to any of the details. just abstractly you can immediately see the complexity of the workload and all the interception points",
        "start": 757.279,
        "duration": 1714.326
    },
    {
        "text": "in particular. so this is one of my favorite slides really in in any talk. and then the reason for that is that before we dig in at all to any of the details. just abstractly you can immediately see the complexity of the workload and all the interception points if you hunt, you'll see up there 100x, you can see the impact of that. , so tremendously complicated workload. , lots of technology used from accelerated computing and AI to to have an impact and a very large impact at 100x. I mean, TCAD is a very complicated use case. You can see all of the elements here from computational topography to to molecular dynamics, electrostatics, electromagnetics, flow dynamics, gosh, lots of different workloads. and then you can see how that maps to various libraries that NVIDIA has available for those use cases including physics Nemo to integrate AI physics. So this has been a great result that's that's come out of TSMC for their TCAD use cases. I've mentioned a few times already and I will many more times the importance of collaboration in our in our engagement with the semiconductor industry and and frankly all other industries. so I'm excited here to talk about our new collaboration with Lamb Research. so yesterday in this keynote Sessa celebrated how Lamb and Nvidia are accelerating the device roadmap for AI applications and using the latest AI technologies from NVIDIA to drive breakthroughs in semiconductor design and manufacturing. This is a really great example of that of that virtuous cycle that I was talking about earlier. What we have here is Lamb adopting Nvidia technologies to accelerate their tools. Lamb tools resulting in Nvidia building better technologies. So a truly symbiotic virtuous cycle between Lamb and Nvidia. Lamb's atomic level breakthroughs are enabling HPM, DRAM, logic devices and advanced packaging and GPUs powering the next wave of AI technologies. Semi semiare solutions and virtual twins are then running on the latest Nvidia AI platform accelerating research development and talent across the industry. So really excited about this collaboration. Okay. So just as important as AI [clears throat] physics is Agentic AI. Agentic AI has the opportunity to transform and and really enhance the capabilities and productivity of our engineers in the in the semiconductor ecosystem. And just like with CUDA X, this is a this is something that we engage and build for our partners and go to market through our partners. So I'm going to go through a few of their platforms you know one by one here and talk about the NVIDIA partnership. so Cadence's Jet AI powers cadence AI design tools across domains like digital and analog chip design verification debug PCB layout multifysics optimization and data center design and op and operation. It's a scalable customizable foundation to bring AI into every part of the engineering workflow. Seaman's Hughes AI is defining the future of aent and semiconductor and PCB design, enabling automated workflows,",
        "start": 918.48,
        "duration": 2032.7839999999997
    },
    {
        "text": "and analog chip design verification debug PCB layout multifysics optimization and data center design and op and operation. It's a scalable customizable foundation to bring AI into every part of the engineering workflow. Seaman's Hughes AI is defining the future of aent and semiconductor and PCB design, enabling automated workflows, providing unprecedented speed, accuracy, and collaboration in EDA. Synopsis is building a nextgen agentic AI platform for for its semiconductor design and manufacturing powered by NVIDIA, harnessing GPU acceleration, generative AI, and autonomous agents to radically speed up and democratize engineering workflows. All right, so let's talk about AI factories. so today, as I mentioned before, AI factories transform electricity into intelligence. Like all factories, they are incredibly complex to build. In order to do that in all the industries where they are required, we need to simulate them first. And for that, we've created what we call the OV blueprint for AIAC digital twins. I know it rolls right off the right off the tongue. I have a video now to show you how that's in how that's how that works. So, let's play it. TSMC with MEDAI generate 3D layouts of an entire fab from 2D CAD and develop AI tools on co-op that can simulate [music] and optimize intricate piping systems across multiple floors, saving months of time. Quanta, Wistron, and Pegatron plan new facilities and production lines virtually prior to physical construction, saving millions in costs by reducing downtime. Pegatron simulates solder paste dispensing, reducing production defects. Quanta uses Seaman's Team Center X with Omniverse to analyze and plan multi-step processes. Foxcon, Wistron, and Quanta simulate power [music] and cooling efficiency of test data centers with Cadence reality digital twin. and to develop physical AI enabled robots. Each company uses its digital twin as a robot gym to develop, train, test, and simulate robots, whether manipulators, AMRs, humanoids, or vision AI agents as they perform their tasks or work together [music] as a diverse fleet. Pegatron uses [music] NVIDIA Metropolis to build AI agents who help employees learn complex techniques. The age of industrial AI is here. Okay. So, I'm going to ask you all to just pay so close attention because that was actually the wrong video. And and so that's going to come up in a little bit and the context for it's going to be given. But this was our video about about physical AI and how we use blueprints to how companies in the ecosystem are using blueprints to to enhance their fabs. And so we're going to talk context about that here in a few minutes. And then we're going to get to a video which will be from the previous context which is about the OV blueprint for digital twins of AI factories. So lock in guys. All right. So let's talk about physical AI. So [clears throat] physical AI are models that understand and modify physical environments. they will transform manufacturing industries, automating the world's millions of",
        "start": 1081.84,
        "duration": 2444.0630000000006
    },
    {
        "text": "the previous context which is about the OV blueprint for digital twins of AI factories. So lock in guys. All right. So let's talk about physical AI. So [clears throat] physical AI are models that understand and modify physical environments. they will transform manufacturing industries, automating the world's millions of warehouses. Robotics is going to play a huge role in physical AI and in manufacturing. And every robotics company will need three computers to build their physical AI. The first of those computers is to train the AI. that should be uninteresting to to right any any AI you need a computer to train the AI now when we talk about deploying a physical AI we're talking about a physical instantiation in the world and so where an AI is going to be controlling that physical instantiation and so for that you need another computer you need a computer in the robot that's the second computer and then the third is because we're going to be putting this into the physical world we want to be able we need to be able to simulate its environment in true with true physics in a third computer so that we can be sure the robot is ready, be sure that it's safe, be sure that it's going to operate correctly, so that it can take feedback from sensors and experience in the physical world and further enhance the model and so it can refine the AI model by being run in collaboration with the physical instantiation. So all these three computers need to be working interactively to deliver on the opportunity of physical AI. That third one is really is where we start to talk about the importance of digital twins. And digital twins are already being used across manufacturing in semiconductor for use cases like layout and planning, process simulation, remote monitoring, robot fleet simulation, industrial inspection. And now I would play the video that just played but you already saw that. So you saw a lot of great examples of people deploying digital twins in semiconductor manufacturing. and so now we're going to take a look at the OV blueprint for for AI factory digital twins. Let's play the video. The world is racing to build state-of-the-art largecale AI factories. Bringing up an AI gigafactory is an extraordinary feat of engineering requiring tens of thousands of workers from suppliers, architects, contractors, and engineers to build, ship, and assemble [music] nearly 5 billion components and over 200,000 m of fiber, nearly the distance [music] from the Earth to the moon. The NVIDIA Omniverse blueprint for AI factory digital twins enables us to design and optimize these AI factories long before physical construction starts. Here, Nvidia engineers use the blueprint to plan a 1 gawatt [music] AI factory, integrating 3D and layout data of the latest NVIDIA DGX Super Pods and advanced power and cooling systems from Vertive and Schneider Electric and optimized topology from NVIDIA Air, a framework for simulating network",
        "start": 1297.2,
        "duration": 2775.4630000000006
    },
    {
        "text": "construction starts. Here, Nvidia engineers use the blueprint to plan a 1 gawatt [music] AI factory, integrating 3D and layout data of the latest NVIDIA DGX Super Pods and advanced power and cooling systems from Vertive and Schneider Electric and optimized topology from NVIDIA Air, a framework for simulating network is traditionally done in silos. The Omniverse blueprint lets our engineering teams work in parallel and collaboratively, letting us explore various configurations to maximizing TCO and power usage effectiveness. NVIDIA uses Cadence Reality Digital Twin accelerated by CUDA and Omniverse libraries to simulate air and liquid cooling systems and Schneider Electric with EAP, an application to simulate power block efficiency and reliability. Realtime simulation lets us iterate and run largecale what-if scenarios in seconds versus hours. We use the digital twin to communicate instructions to the large body of teams and suppliers, reducing execution errors and accelerating time to bring up. And when planning for retrofits or upgrades, we can easily test and simulate cost and downtime, ensuring a futureproof AI factory. All right, so we have one minute left, which is perfect because it's wrap-up time. , so to wrap up, the semiconductor ecosystem is at the start of a new industrial revolution. There are two trillion dollar opportunities in front of us. , one presented to us by the by AI factories and the other in physical AI. To capture those opportunities, innovation is required across many parts of the semiconductor design and manufacturing workloads. And AI supercomputing and accelerated computing is here to help. from accelerated computing for semiconductor design and manufacturing applications, agentic AI to increase the capability of semiconductor engineers, factory digital twins to automate fabs, robotics in those fabs, and vision AI for problems like mask and wafer inspection. AI and accelerated computing is ready to help across all these areas. And of course, this is done as a partnership with the ecosystem. We're a part of this ecosystem and so we're thrilled to capture this opportunity together. Thank you. [applause]",
        "start": 1467.44,
        "duration": 3020.038000000001
    }
]