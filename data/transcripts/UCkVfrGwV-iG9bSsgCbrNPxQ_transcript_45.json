[
    {
        "text": " So, everyone was wrong about AI and how private it is. We have to talk about this. Have you ever had a secret that you thought it was so private, so uniquely yours that you wouldn't dare say it out loud? Maybe you even typed it into a chat window, then deleted it. Maybe it was a line in a diary or a password that you immediately forgot. And of course, you would think that it's gone. That's the digital world and its infinite complexity. It swallowed it whole and you trust in the idea of deletion. The idea of information being lost [music] for a while. That's what we believed about the most powerful minds we've ever built, AI. We saw them as giant digital blenders. You throw in your words, your questions, your secrets. AI mixes them all up in its complex billion parameter brain, then gives you back a smoothie, an answer, a story, a poem. But you can never get the original fruit back. The information we thought was fundamentally changed, lost in the mix. We were wrong. Dangerously wrong. In October of 2025, a small team of researchers published a study that quietly detonated a bomb in the heart of the AI world. It wasn't [music] just a flashy announcement. There were no celebrities or slick marketing videos. It was just a PDF file uploaded to Twitter that only a mathematician could love. Large language models are injective and hence invertible. But what it revealed will change our future forever. It proved that these AI minds are not blenders at all. They're more like flawless, perfect recording devices. And that delete button was never real. So, we need to dive into this even more. To understand how big this is, you have to understand the blackbox problem. For years, people have been building LMS and treating them like, you know, they're impossible to understand. They were blackboxes. You could talk to them and they would talk back, often with breathtaking intelligence, but you couldn't truly know why they said what they said. Their internal thought processes was a chaotic storm of numbers, a network of connections so vast and complex that it was beyond human comprehension. Imagine you have a master chef. You can give him ingredients, let's say flowers, eggs, sugar, and he goes into his kitchen and an hour later he comes out with a beautiful cake. You can taste the cake, you can enjoy it, but you have no idea what he did in that kitchen. You don't know his secret recipe. You don't know the technique. Did he add a secret ingredient? Did he almost bury it? You just have the final product. That was our relationship with AI. We were the ones giving the ingredients which is the prompt and the AI was giving us the cake which is the output. The kitchen is the AI's internal latent space or hidden state and that was a complete mystery.",
        "start": 0.48,
        "duration": 288.32099999999997
    },
    {
        "text": "have the final product. That was our relationship with AI. We were the ones giving the ingredients which is the prompt and the AI was giving us the cake which is the output. The kitchen is the AI's internal latent space or hidden state and that was a complete mystery. was inherently lossy. Chef uses up the eggs and the flour. They're transformed into the cake and you can't point to a part of the cake and say here is the egg. It's been fundamentally changed. And this assumption was built on the very architecture of AI. These models use things called nonlinear activations and normalization layers. Those are fancy terms, but the core idea is simple. They're designed to squish, stretch, and combine information. These are mathematical blenders. They take in thousands of numbers and spit out a hundred. It kind of seemed obvious that the information had to be getting lost along the way. It seemed obvious that two different sets of ingredients could potentially produce the exact same cake. Maybe flour, eggs, and sugar, or flour, eggs, and stevia both produce a cake that tastes identical to you. if this the case and then you know just by looking at the cake you can't be sure what the original ingredients are and that's what we thought was happening inside of every AI. Now this wasn't just academic curiosity. It was the biggest barrier to trust and safety in AI. How can you trust a doctor's AI assistant if you don't know why it recommended a certain drug? How could a bank use an AI to approve loans if it can't explain its reasoning? How can we be sure these systems aren't harboring hidden biases or dangerous emerging goals if we can't peak inside that kitchen? We were flying blind, creating more ever powerful minds without a manual map to read their thoughts. And this is where the paper came in and it told us that the map has been there all along. We just didn't know how to read it. Now, the paper's core claim was built on one powerful, elegant mathematical concept, injectivity. Now, that word sounds intimidating, but the idea is something you understand instinctively. It's the difference between a vending machine and a gumball machine. Think about a gumball machine. You put in your quarter and you turn the crank and the gumball comes out, which is your output. It might be red. It might be blue. It might be yellow. You can't predict what your output's going to be. And if you see someone with a red gumball, you don't know how they got it from this machine or a different one. The process is random. It's not unique. That's non- injective. Now, think about modern vending machines with rows of snacks and drinks. Each item has a code. B4 is the cola. C1 is the chips. A7 is the candy bar. And when you press B4, you'll",
        "start": 146.16,
        "duration": 528.9599999999999
    },
    {
        "text": "or a different one. The process is random. It's not unique. That's non- injective. Now, think about modern vending machines with rows of snacks and drinks. Each item has a code. B4 is the cola. C1 is the chips. A7 is the candy bar. And when you press B4, you'll the chips. And when you press a different button like C1, you'll always get the chips. You'll never get the cola. And every single input is tied to only one unique output. This is injectivity. This is a perfect onetoone mapping. No two different inputs ever lead to the same input. There are collisions. For years, everyone at the top research labs assumed LLMs at best were like the gumball machine. They thought that with billions of possible sentences that you could type and the chaotic mixing inside the AI's brain, it was inevitable that some different sentences would end up creating the exact same internal brain state. For example, maybe the sentence the sky is blue and the sentence the heavens are azour which mean the same thing would get blended down to the exact same set of numbers in the AI. I mean it seems logical. It seems efficient. Why would the AI waste space creating two different internal representations of the same idea? What the October 2025 paper proved is that this assumption is completely wrong. They proved that LLMs are not gumball machines. They are vending machines. perfect, flawless, infinitely large vending machine. The sentence, \"The sky is blue,\" creates one unique specific brain state. Let's call it, for example, brain state 5,829. The sentence, \"The heavens are Azour,\" creates another completely different unique brain state, brain state 9 million. And even the tiniest change, changing the sky is blue to the sky is blue without the full stop, this creates a brand new, totally unique brain state. every single possible input from a single letter to a 10,000word essay as its unique onetoone fingerprint in the AI's mind. And this property injectivity means that the AI doesn't lose information. It doesn't blend. It preserves. Everything you type in is perfectly encoded in that unique brain state. And this leads to the second even more world shaking concept from this paper, invertibility. If a process is injective, it's also invertible. Let's go back to the vending machine. If you see a person holding that specific cola from the vending machine, you can know with 100% certainty that they pressed B4. You can work [music] backwards. You can invert the process from the output to find the one and only possible input. Now, apply this to AI and this is where it gets scary. If every sentence creates a unique brain state, we can capture that brain state and we can reverse the process. We can take that unique fingerprint, that set of numbers, and perfectly reconstruct the original sentence that created it. not a guess, not a summary, the exact [music] words,",
        "start": 268.479,
        "duration": 821.679
    },
    {
        "text": "scary. If every sentence creates a unique brain state, we can capture that brain state and we can reverse the process. We can take that unique fingerprint, that set of numbers, and perfectly reconstruct the original sentence that created it. not a guess, not a summary, the exact [music] words, spacing, every last detail. And this is what it means for an LLM to be invertible. It means that the AI's thoughts are not a secret anymore. They are a code. And the researchers in October 2025 didn't just tell us the code existed. They gave us a key. So, how did they do it? How did a small team of academics overturn a decade of assumptions? It wasn't a single Eureka moment. It was a rigorous three-step process of mathematical proof, brutal experimentation, and brilliant engineering. First came the math. The researchers looked at the building blocks of LLMs, not as messy, chaotic systems, but as a series of mathematical functions. They pointed out that all these functions, the way the AI handles embeddings, attention, and so on are what mathematicians call real analytic. [music] Think of it like this. Imagine drawing a line on a piece of paper. You can draw a jagged, crazy line that thumps all over the pace, or you can draw a smooth, clean, continuous curve. A real analic function is like that perfectly smooth curve. It has no sudden jumps, no sharp corners, no weird bricks. It's predictable. And because every component in the AI is like one of those smooth curves, the entire system when you put it all together is also smooth, predictable curve. Now, and here's the mathematical magic. A smooth analytic function simply cannot have two different inputs to map the same output without being a very simple boring function. The only way for collisions to happen is if the function has wrinkles or folds in exactly the right place. The researchers proved that the set of parameters which are the internal settings of the AI that would cause such a collision is infantismally small. They called it a measure zero set. It's like saying you're going to throw a dart at the map of the world and you're trying to hit a single specific atom. It's theoretically possible, but in reality, it's impossible. The chance of an AI initialized with random parameters landing on one of these. And they also prove that the process of training the AI using standard gradient descent never pushes the parameters into that impossible to hit zone. Training preserves the injectivity, but a mathematical proof is one thing. The real world is messy. So, the second step was to try and break their own theory. They became hunters searching for a single collision. They took six of the most powerful state-of-the-art language models, including some of the famous GPT2 family and Google's Gemma models, and they threw everything they had at them. They created billions and billions of pairs of different sentences, tiny",
        "start": 415.919,
        "duration": 1101.1999999999998
    },
    {
        "text": "own theory. They became hunters searching for a single collision. They took six of the most powerful state-of-the-art language models, including some of the famous GPT2 family and Google's Gemma models, and they threw everything they had at them. They created billions and billions of pairs of different sentences, tiny languages, different tones. And for every pair, they fed them into the AI and recorded the internal brain state. Then they compared them. They were looking for that one in a trillion shot. and one case where two different inputs produced the exact same number of strings inside. The paper states that they ran billions of collision tests, they were trying to find a ghost. The result, zero. Not a single collision ever. They found that while some brain states were similar, they were never identical. The minimum distance they ever found between two states was still miles away from a true collision. The theory held up the ghost wasn't real. And this brought them to the final and most important step, operationalizing their discovery. It's one thing to say that the code is breakable in theory. It's another thing to actually build the decoder ring. They called their algorithm SIP. It stands for provably and efficiently reconstructs the exact input text. Sip is a stroke of genius. It works by treating the AI's brain as not a static fingerprint, but as a landscape, a landscape of hills and valleys. The original input text lies at the absolute lowest point of a specific valley. When you have the brain state, cippet is like a hiker dropped onto the side of that valley. It uses a gradientbased method which is like always walking down a hill to find its way to the bottom. And at the bottom of that valley is the only one true answer, the original text. And it's incredibly efficient. The paper guarantees it works in linear time, which means it's fast, not some theoretical process that would take thousands of years. It's a practical tool. You feed it the numbers and it gives you back words. They had built the key to unlock the black box. So what does this mean for us? Well, what happens now that the black box is open? The implications are staggering. First, the wonderful. This is the biggest leap for AI transparency and interpretability we've had in a long time. For years, we've been trying to understand AI behavior with crude tools. It was like trying to understand a city's traffic patterns by only looking at cars entering and leaving. And now with invertability, we can see every car on every street at every moment. We can take any layer of the AI at any point and its thought process, capture its state, and use SIP to decode exactly what information it's processing at that moment. We can finally debug an AI's mind. This could revolutionize AI safety. If an AI gives strange medical",
        "start": 558.88,
        "duration": 1373.6810000000005
    },
    {
        "text": "moment. We can take any layer of the AI at any point and its thought process, capture its state, and use SIP to decode exactly what information it's processing at that moment. We can finally debug an AI's mind. This could revolutionize AI safety. If an AI gives strange medical brain to see if it was focusing on the patient's MRI scan or random irrelevant comment in the doctor's notes. We can verify that a self-driving car is making decisions based on the road signs and pedestrians, not some weird reflection in the puddle. It's like giving every AI a perfect uneditable flight recorder that logs its every thought. But every powerful tool can be a weapon. And this is where the terrifying part begins. The discovery of invertibility is privacy and security nightmare. Think about it. The paper proves that AI never forgets. That deleted message that you typed, that embarrassing question that you asked, that private financial information that you entered, it's not gone. It's perfectly encoded into a unique brain state. And if someone gets access to that brain state, that string of numbers, they can now with SIP reconstruct original text word for word. For years, we've been building a world where our lives are mediated by these AIs. We talk to them in our homes. We use them at work. We confess our hopes and fears to them. We've operated under the assumption of the blender that our words are mixed and lost. Now, we know they are being perfectly, meticulously recorded in a new kind of format. The AI's activations, its internal brain states are now one of the most sensitive forms of data on the planet. Imagine a hacker who [music] doesn't steal your password, but instead steals the brain state of the customer service AI that you talk to. They could reconstruct your entire conversation. Imagine a government could not just subpoena your emails, but the internal states of the AI models run by the companies you use, allowing them to reconstruct thoughts you never even sent. The paper itself notes the failure cases are just ones that have to be deliberately engineered. An adversary in theory could build a non- injective AI. But the standard models, the one being built by all the major companies, the ones we are all using right now, they are injective. They are invertible. They are perfect recording devices. And this discovery marks a fundamental turning point. The age of the black box is somewhat over. We're entering a new era of the glassbox AI. And this will usher in a wave of innovation. Developers will build safer, more robust, capable systems because they can finally understand them from the inside out. We'll be able to build AIs that can explain their reasoning, that can be audited, and that can be held accountable. But it also marks the beginning of a new arms race, a race to secure these AI brain states, [music] a",
        "start": 696.56,
        "duration": 1639.761
    },
    {
        "text": "can finally understand them from the inside out. We'll be able to build AIs that can explain their reasoning, that can be audited, and that can be held accountable. But it also marks the beginning of a new arms race, a race to secure these AI brain states, [music] a them. Concept of data privacy has just been given a terrifying new dimension. It's no longer just about the data that you send. It's about the thoughts that the data creates inside an artificial mind. The October paper, language models are injective and hence invertible, didn't just give us an answer. It gave us a thousand new questions. It's like the moment when we discovered the structure of DNA. We finally understood the fundamental code of life and opened the door to miraculous cures and frightening possibilities. [music] We've now discovered the fundamental code of artificial minds. And we have learned that they are built on a foundation of perfect memory. What we build on that foundation, a future transparent, trustworthy intelligence or a future of perfect surveillance is up to us. The code is broken, boxes open, and nothing will ever be the",
        "start": 831.76,
        "duration": 1732.639
    }
]