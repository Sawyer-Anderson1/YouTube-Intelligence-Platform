[
    {
        "text": " When you build the AI infrastructure, the most important challenge you always face that the industry is moving very fast. When it comes to inference, there is many important components. You still need very reliable underlaying physical infrastructure and it should be more and more performant and our mission is to provide scalable, high performant, high reliable AI cloud. We brought all our experience in building cloud offering but we build the platform from the ground up. So we identify the core AI scenarios we want to focus on training inference data processing and we build the platform that the most efficient for these scenarios. When we think about use cases we love inference because behind the inference there is real business need of someone. Would it be user? Would it be business? the sizes of the models growing and you need more memory, more performance. You need models that's running across multiple nodes and utilizing the networking. And that's where Nvidia all the time is moving boundaries providing new more performance stuff that enables more performant more advanced models. And then on the software layer we need to provide to customers enough flexibility. And we have a managed Kubernetes that has the autoscaling feature. So our customers can grow, scale up, scale down with demand they get. We all understand that the power of AI is in a serving the real cases of the customers and that's inference. We want to make it super efficient, super economically pragmatic for our customers because then they got the real business, the real unit economics and they grow and we grow with them.",
        "start": 60.0,
        "duration": 6600.0
    }
]