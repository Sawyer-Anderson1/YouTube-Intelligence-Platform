[
    {
        "text": " Heat. Hey, Heat. Heat. Heat. Hey. Heat. Heat. [music] Hey, hey, hey. [music] All right, we are here. Yes, just a few minutes late. Welcome to Forward Future Live. Today's Friday, January 30th, 2026. I'm Matt Berman. And I'm Nick Wentz. Today we have some incredible guests on the show. First, Nikita Ruden, CEO and founder of Flexion. We have Alex Masherb, CEO and founder of Hicksfield, and Peter Steinberger, creator of, well, I guess formerly Moltzbot, formerly Claudebot, and now OpenClaw. , here to talk about all the cool stuff that's been happening this week. But, , let's, , let's talk a few minutes about what's been going on this week because I've been a little out of it. I'm catching up. I actually wanted to start there. What's what's going on with you? You've had a a rough one. Yeah, I I was I was incredibly sick with the flu and then right on the tail end of that my just like completely threw out my back. I sound like an old man. I suppose that makes me one. Throwing out your back is kind of a right of passage in old manhood. yeah, so it's been rough. A lot of a lot of time in bed and I'm I'm so happy to be here. I am so happy to be out of my bedroom. Little little change of scenery. , so how did you throw out the back? Was it a a a sneeze related to the flu? No, obviously I was lifting anvils or something else like totally cool like that. No, I I was changing my bed and and just like sitting in bed for, you know, two days straight. I guess it was like no, you're not you're not ready yet. so that was that. So yeah, today is kind of the first day out. So, if you see me adjusting, moving around, that is why. Well, welcome welcome back to the real world. Yeah. , you know, we have a a great show coming up and I think we should just jump right into it. We're going to have a chance to cover stories later on. What do you think? Let's do it. Yeah. Let's bring our first guest on. All right. So folks, if you've ever wondered who's going to save those poor humanoid robots from tripping over themselves or enable them to perform heart surgery, our next guest might be that very person. This is Nikita Ruden. He is an ex Nvidia researcher who recently raised $50 million to build the brain behind humanoid robots. Nikita, welcome to the show. Hi, really happy to be here. It was a great intro. Welcome to the show, Nikita. Super excited to talk to you. Well, let's just start with where in the world are you? How's it going today? Give us give us the lowdown. Everything's going well. it's actually the end of the day because I'm in in Zurich in Fratern",
        "start": 40.32,
        "duration": 360.40799999999984
    },
    {
        "text": "intro. Welcome to the show, Nikita. Super excited to talk to you. Well, let's just start with where in the world are you? How's it going today? Give us give us the lowdown. Everything's going well. it's actually the end of the day because I'm in in Zurich in Fratern I don't be afraid to pour yourself beer. Yeah. Don't be afraid to pour yourself one. You know, it's 10:00 a.m. here, but feel more than Yeah. , all right. Well, let's let's jump into the meat of this. So, if you could, you know, I'm sure some of the audience is familiar with Flexion. Is that the right pronunciation, by the way, or is it flexion? I usually call it flexion. Flexion. Perfect. All right. Depends on the language. Flexction. , what are you all doing over there? You said it really well. We're teaching humanoids how to do all sorts of things. We're focusing on the intelligence. So, we're not building hardware, building a a horizontal platform. We're building the brain that can power all sorts of robots. And I personally use the word humanoid in a in a broader sense. It doesn't necessarily mean two legs, two arms, and looks like a human. It means a robot that has humanlike capabilities. So you should be able to go where humans can go and do the sort of manipulation tasks, interact with the environment in the way humans do. So So we're we are of course working on actual humanoids robots with two legs, two arms and a head, but also working on platforms on wheels that have two arms on top to to manipulate the environment. And we're not we're not focusing necessarily on Sorry. Sorry. Go ahead. No, no, please continue. We're not necessarily focusing on one specific task, but we want to build this machinery that allows us to teach new tasks to these robots very very quickly. We're starting with things related to to industry to logistics and moving boxes around, manipulating those boxes. But we're not stopping there. We'll we'll be expanding the capabilities of these robots as time goes. So, what is the like what are you actually building? What what is the thing that is able to power the humanoid robots? Is it a model? Is it a framework? Is it a a platform of different types of software that can power it? Vision, language? Tell us a little bit more. That's a great question. it's one I get I get all the time. The most popular thing to say is it's one single neural network end to end that does everything. but actually that's not exactly what we're building. We're building a platform that it's like a whole ecosystem of different neural networks interacting with each other. And the easiest way to describe it is to split the the whole autonomy of a robot. So we're building the whole autonomy stack from a text prompt. You",
        "start": 356.479,
        "duration": 664.2479999999999
    },
    {
        "text": "what we're building. We're building a platform that it's like a whole ecosystem of different neural networks interacting with each other. And the easiest way to describe it is to split the the whole autonomy of a robot. So we're building the whole autonomy stack from a text prompt. You after people finish having beers. And we're building the whole thing all the way down to motor control. But it's once again not one single neural network. It's split into a few different steps. And we typically talk about three layers. What we call the command layer, which is something close to to a VLM. And we're not turning it from scratch. This is we can we can actually leverage open source VMs that are available. We just need to fine-tune them. That VLM will break down the task into subtasks such as, oh, I see a an empty beer bottle. Go grab it. And then we have a V. These are the these vision language action models. It will receive this specific instruction of grabbing that bottle of beer and [snorts] will plan the motion of the robot to actually go put its arm there, close the hand to to grab that bottle. And the third layer is what we call the control layer will control the motors of the robot to execute that very specific motion. So this seems to be a almost a a heated debate in embodied AI right now where you have the the endto-end camp just one big neural network and then you have your your side of the debate which is I guess you could call more of a a modular approach if that's fair to say right what what are the benefits to to your approach over the endtoend camp it's funny you use the word modular and it's exactly the the word I usually use but it seems to be a taboo world and a taboo in in this world. but nevertheless, I'm I'm I'm happy to to defend it. Yeah, there are a few advantages. One is to train an end toend model, we need a lot of of data. I mean, everyone knows that it's all it's all a data problem, but if you want training an end toend model, you pretty much need data of robots doing things, right? So, you need to to to train the whole thing at once. You need text in and motor commands out. Whereas if you split it, you can train part of the model on a lot of human data and only inject the robot data in the last part where it actually matters. For example, to break the the down the abstract task of cleaning an office to instructions such as grab a bottle, you don't really need any robot data. Doesn't matter what kind of robot you're controlling. Whereas for the control part, it's it is very important to know exactly what kind of motors the robot",
        "start": 510.8,
        "duration": 947.4470000000001
    },
    {
        "text": "the down the abstract task of cleaning an office to instructions such as grab a bottle, you don't really need any robot data. Doesn't matter what kind of robot you're controlling. Whereas for the control part, it's it is very important to know exactly what kind of motors the robot closely related is the fact that we want to power different kinds of robots. And for that we're building a system in a way that we can reuse let's say 80% of the whole system and all of the data across all robots. It's this cross embodiment idea once again deciding what kind of actions to do doesn't require knowing exactly the the characteristics of each actuator. And then you interface that with an additional smaller model that is actually trained specifically for each embodiment separately. And when you when you think about who you need to partner with, I assume you need to partner with humanoid robotics companies that aren't necessarily building their own model or or tell me about like how how the actual commercialization works. That's another another very good question. So yes, there there are two ways to do this. we need to partner with companies building robots and then either we buy robots and we go talk to the end customers or you know warehouses, factories etc. or our preferred way is to to give those capabilities to harder manufacturers and let them go to their already existing and end customers. And the following question might be about competition and of course a lot of these companies are building similar things in house or planning to do that. So in a way our competitors are also our clients. We need to convert the the the competitors to to clients and I mean I would say we're doing that very successfully so far. Yeah. Can you walk us through how that how that works? It's a interesting dilemma for sure. Right. I mean, if you think about it, I'm sure you've seen a ton of humanoid companies. I think pretty much every few days we get we get a new one. for sure every day when you went from the from China but these days also once per week when from the US or Europe and everyone has to recreate the same system relearn all the exact same lessons and yeah in a way except sorry I'm I'm I'm thinking out loud here [clears throat] but everyone is building the same system and just trying to interact slightly different data in right which is specifi specific for for for their their robot their actual embodiment. Mhm. And basically what our claim is that it doesn't make sense to do that separately in each each company without any cross learning across all these systems. It's much more efficient if one platform or in this case ours but also someone else's is able to power all these robots at once. So, do you have a an Optimus",
        "start": 654.64,
        "duration": 1283.6079999999995
    },
    {
        "text": "make sense to do that separately in each each company without any cross learning across all these systems. It's much more efficient if one platform or in this case ours but also someone else's is able to power all these robots at once. So, do you have a an Optimus all of them scattered around the office, constantly testing them and experimenting with them. I' I'd love to see a picture of this this lab setup over there. Yeah, I can try to find one or u it's a bit late, so the robots are not walking around anymore, but usually if you come to our office, you'll see a bunch of different robots doing like walking around, carrying boxes, manipulating things, and we're I mean, we're doing it. We're making our own life harder and purpose by working on different robots from from day one because it it requires a lot of infrastructure to be able to power all these robots with the with the same system. And another very important point is that we're betting a lot on on simulation and reinforcement learning. And this is again [snorts] maybe slightly less popular thing these days. And the the alternative approach that we want to avoid is using teleoperation. So you pay a ton of people to wear motion capture suits, VR headsets, stand behind robots, tele operate them to collect data. Instead, we're trying to do as much as possible in in simulation, which actually makes it much easier for us to retrain things for new robots. They think we have a simulator that allows us to train one robot to to manipulate a box. Once we we get a new robot, typically the manufacturer provides us with a file called a URDF. It's it's a file that represents all the mechanics and physics of the robot. We plug it into our simulator, retrain it, and and get a new neural network out that we can just put on that specific robot and and that new robot knows how to do the same task as the other one. And so there we get really this this flywheel of every time we train one robot for a new task, we're actually training 100 or however many different embodiment we have. And when you're when you're testing the robots, how do you think about safety? How how do you think about putting the the guardrails in place? And then also does are you focused more on the kind of factory humanoid robot or at home personal use? The two things are are clearly related in my opinion and we are focusing more on industrial use cases at first in big part because of the of the safety question because if you send these robots into homes you cannot control anything. Everything really has to work let's say 99 however many nines you want to add zero shot. You cannot send an engineer that will quickly check that",
        "start": 827.76,
        "duration": 1576.9689999999991
    },
    {
        "text": "first in big part because of the of the safety question because if you send these robots into homes you cannot control anything. Everything really has to work let's say 99 however many nines you want to add zero shot. You cannot send an engineer that will quickly check that you send a 100 robots to a factory, then it's completely fair to send to send one of our engineers to check that there are no big roadblocks and that robots can actually operate the way they do. And after a few days, we can unleash the fleet of 100 robots. Plus, you can train the workers around robots and tell them to not do very specific things, which is not something you can do in the robot at home. Nikita, did you did you see the announcement that Tesla is going to discontinue the Model X and the Model S in you know in preparation for building out their Optimus fleet? And f first did you see that? What do you think? And then also like I want to get your take on timelines. What kind of timelines are we looking at for mass deployment in the commercial setting? Yeah, I I I did see that. I think it's great because for us the more robots are out there the better. That's right. You guys benefit, right? Because you're just every robot is a opportunity to put your your engine in it. Exactly. But we the caveat is that it doesn't work if only Tesla doing it. So we need many other companies doing it as well. Right. And I mean we've seen it before when Tesla starts doing something then all the other automotive companies react. And I see the same thing happening here. interesting. Everyone is starting to at least think about building building the robots. So the announcement itself is great in in terms of timelines. Again, the more the faster the better for us, but I do think it will take a little bit longer before there is a reason to produce that many robots because to produce that many robots, it means that each one of them has to create some some economical value, right? And the reality is that we're just not there yet. And there there a few steps to solve until we get to a a useful humanoid robot that creates creates value. Right now we've seen some industrial deployments. But typically what happens is that the robot is not good enough to do the exact task that was done by humans before. Basically to do the for the proof of concept they need to change the environment slightly to make the task easier. which of course means that well once the the proof of concept is done they put the robot back in the box and the humans are back. Is that Nikita is that typically a hardware limitation or a software limitation or as in the you know the",
        "start": 978.16,
        "duration": 1861.127999999999
    },
    {
        "text": "the task easier. which of course means that well once the the proof of concept is done they put the robot back in the box and the humans are back. Is that Nikita is that typically a hardware limitation or a software limitation or as in the you know the the industrial setting better hardware better software or both? Mostly better software except hands. So for tasks requiring really dextrous motions, hardware is not there yet, but it is getting there very quickly. I was at CS a few weeks ago and there were, I don't know, 25 different Chinese hands all competing with each other. So the hardware is getting there extremely quickly. Software takes a little bit longer especially for very complicated whole body coordination tasks. I don't know if you saw figures announcement for a few days ago. Yeah, this probably I think it's great. I've been saying for a very long time, show me whole body coordination. So, show me legs and arms moving at the same time. And I think they're one of the first. One X showed a few things, but the figure demo is a bit more convincing. So, we need way more of that. And these are the things we're working on. All these tasks that require the robot to bend over, grab something heavy from the ground, bring it somewhere else, walk on stairs, open doors, all these sort of whole body coordination tasks. Once we get that then the scale of what is possible for these robots increases a lot. Yeah. So this is a little cliche at this point but you know people talk a lot about the chat GPT moment for humanoid robots. Like how close do you think we realistically are to something that like that? I understandably locomotion is very difficult to solve. But if you had to put kind of a timeline on it where where do you think we'd end up? [snorts] He's asking for a specific date if it's not clear. Yeah. Month and day, please. It's going to be June 12th of some year. I don't know which year. All right. Definitely 12. [laughter] Exactly. No, I I actually think we won't have this one single CHP moment for a very simple reason is because it's hardware and it's going to take just way longer to to build the 10 million robots that would we would need to have to to have that tragic moment. So I I was saying before that today we're at a point where we have to change the environment for the robot to operate. I think towards the end of this year we'll be at a point where we don't need to change the environment anymore. So the robot can actually do a small subset of the tasks that it's actually supposed to do. But still it doesn't mean that it generates economical value because probably behind that robot you'll have three engineers looking over",
        "start": 1122.32,
        "duration": 2139.769
    },
    {
        "text": "a point where we don't need to change the environment anymore. So the robot can actually do a small subset of the tasks that it's actually supposed to do. But still it doesn't mean that it generates economical value because probably behind that robot you'll have three engineers looking over everything is correct and be with a big red button ready to press it to to kill it if it does anything wrong. And the then from there on like 2027 2028 the two things that need to happen is that we reduce the human effort involved in deploying a robot and that ef effort for sure needs to be lower than the human effort in actually doing the task itself. Plus we need to expand the scope of tasks that are possible for these robots and and these two things will take some time. So I I think it will be very hard to define like a very specific GPT moment, but it will happen over the next two, three, four years. I is it one of those things that's going to be a tremendous compounding effect as we have these robots out in the field and they're collecting data and they're essentially like you talked about simulation, but this is now they're in the real world and we're collecting so much more data. Are they just going to get better in an exponential fashion? Yes, they are. And then I I'll come here. I'll come back again to the to the horizontal platform. I think that works if you have a single system powering all the different robots. If you're one company trying to solve a very specific use case, I don't necess necessarily see this this feedback loop because you'll get very good at doing that very specific thing like carrying those metal sheets in a in a factory but then once you switch to the next one it will you'll kind of start you'll be kind of starting from scratch. So that's why I think it's very important to from the beginning work on different robots doing different tasks. You know, Nikita, I've seen two major form factors when it comes to humanoid robots. And you mentioned this earlier. One in which it has humanoid legs and one in which it's built on top of wheels. chat about what the trade-off is between those two form factors and why a company might choose one or the other. Yeah, that's a great question and another debate I had so many times. We're very happy to work with both, but in my opinion, legs are just the the definition of of general purpose. So, a human on legs can can go anywhere a human a human can. Wheels can be more efficient. So, if you have a very flat ground and you have to carry a lot of weight through on long distances, then it's it's easier to do it on wheels. But wheeled platforms have very",
        "start": 1264.48,
        "duration": 2424.008
    },
    {
        "text": "legs can can go anywhere a human a human can. Wheels can be more efficient. So, if you have a very flat ground and you have to carry a lot of weight through on long distances, then it's it's easier to do it on wheels. But wheeled platforms have very is if your ground is not perfectly flat, if you have a step, slope, if you have cables or anything like that on the ground, well, wheels are stuck. And [clears throat] the the second one, which is probably even more important, is with wheels you have two solutions. Either you make a very big stable platform and then the footprint is very large. It doesn't fit through through some of the places where it needs to go. Like some of them don't even fit through a door for example. So then any door is is a no-go for these robots. Or you make the platform small, but then actually it's even harder to control than legs because once you you lift a little bit of weight, the whole thing might just tip over and fall. Then you lose the advantage of the the safety aspect of the of the wheel platform. This is probably an impossible question to answer, but I'm curious just to get your perspective on it. you know, talking about these robots falling over, even in a factory setting when you have an operator nearby and let's just say an accident happens, like where does the accountability in your mind end up? You know, you you have the the factory who purchased the the robot, you have the robot hardware creator, you have the software creator, you know, there's a lot of parties involved. How do you think about liability when it comes to this? It is a very hard hard question to answer and the reality is that in the foreseeable future it will depend on the contract. Yeah. even today there are robots in factories even though they're not humanoids and unfortunately some people still get get hurt and then it depends was the person actually supposed to be there or not. Was it a clear hardware fault? Was it a clear software fault? It's very complicated. So basically the lawyers win. The lawyers win. unfortunately and usually it's not small startups that have the you know the best army of lawyers. But but what what I would say though is our approach to safety is once again to leverage simulation because with these neural networks it's nearly impossible to make any actual mathematical guarantees on the fact that it will never hurt a person. Just like it's impossible to say that you know your Tesla will never crash no matter what happens. , but the argument becomes statistical. So you create a ton of different scenarios in simulation. It's much easier to create all sorts of adversarial scenarios. You test the robot in them 10 million times and you you you",
        "start": 1407.919,
        "duration": 2726.4879999999994
    },
    {
        "text": "you know your Tesla will never crash no matter what happens. , but the argument becomes statistical. So you create a ton of different scenarios in simulation. It's much easier to create all sorts of adversarial scenarios. You test the robot in them 10 million times and you you you and then you can say well in 99.999% of the cases it's correct. The the big challenge is to transfer that number from simulation to to reality. And this is something we're working on very actively is if we have that level of safety in simulation, how how confident are we that we get a similar level in in real life? Nikita, I want to ask you about the sentiment around robots but also artificial intelligence. I think there are quite a few people who are pretty afraid of artificial intelligence, whether you're talking about sentience or just simply job replacement or new technology. , and then when you kind of factor in the embodied experience or the embodied application of artificial intelligence, it becomes so much more real. H how do you think we should be talking about robots? How do you think we can get more of society to be in the the the bullish the pro column of supporting this new technology that really is going to be transformational? That's a great question. I'll [clears throat] start with the the sentient part. What I find really interesting is the more you know how it works, the less likely you are to think that it's it's sentient. Even though when you see it from the outside, especially if it's a humanoid robot, and we work on on a robot from a company called Unitry, the Unitri G1, which is a fairly small humanoid. It looks like a child and when that thing trips and falls and like hits its face on the ground, it it really hurts. It hurts to see that. So, we're immediately projecting some emotions on on these machines because they they look human and they move in a humanlike way. , I'm pretty sure Matt pushed one of those at CES. Well, you don't need to remind it. I mean, why not? , right. But, but again, I think the more you know what's under the hood, the less human human it feels. , I thought that that could be one way to to educate people on like explaining more and more what's under the hood because there are pros but also definitely con and giving it human characteristics that it doesn't really have. And and then the more societal question of what happens when these robots actually take over a significant part of the jobs, it is definitely a trickier one. But then here I'll play a little trick and and talk about the other the other world where we don't have these robots. So right now what we're seeing is that AI is automating all sorts of you know creative academic digital",
        "start": 1562.159,
        "duration": 3047.2870000000003
    },
    {
        "text": "the jobs, it is definitely a trickier one. But then here I'll play a little trick and and talk about the other the other world where we don't have these robots. So right now what we're seeing is that AI is automating all sorts of you know creative academic digital ones. So, so what what tends to happen is that you'll ask your AI how to repair your bike and it will tell you grab this tool, do this thing, and then report back and I'll give you the next the next instruction. So, we are kind of becoming the the hands of of the AI. And I think that world is way worse than the opposite where we get to tell robots what to do and they go and do the the manual work. And I I I want to continue because it's it's interesting. I think throughout humanity, fear has mostly stemmed from lack of understanding. And so I, you know, I I I really appreciate that that p perspective on it where it's like how how do we get people to really understand what's going on? And and it's tough because artificial intelligence today is such a black box, literally. And then when you, you know, put it in the humanoid robot form factor, it just becomes, you know, I think a lot of people unfortunately have sci-fi movie references as their only reference. , so I think it's just like understanding plus showing the positive benefits. I do agree with that. , I want to I want to talk for a moment just to switch topics for a second. You're in Zurich. talk a little bit about the choice to be in Zurich and then also what the startup and tech scene is like there. That's another very important question. the the choice to be here was very simple because we are four co-founders all from the same lab at from ETH which is the it's actually one of the the world's best universities that happens to be in Zurich and at the very beginning when we incorporated the company we had the choice to to stay here or or go to the US but I guess our our unfair advantage by being here is that we the the most or one of the most exciting things happening in the tech scene in Zurich, right? Whereas once you're in the middle of San Francisco, there are just so many things. It's so much harder to to attract attract talent. So talent is the main reason why we have our team in Zurich today. That's interesting. we we talked to the lovable C CEO few few weeks back now and he said a very similar thing. how there's a lot a lot of talent where they are in Europe but because there's less competition here to the US. Exactly. So anyway, you were Please continue. I mean the opposite is also true. We see",
        "start": 1727.679,
        "duration": 3357.926000000002
    },
    {
        "text": "few few weeks back now and he said a very similar thing. how there's a lot a lot of talent where they are in Europe but because there's less competition here to the US. Exactly. So anyway, you were Please continue. I mean the opposite is also true. We see in the US and they also don't necessarily want to go back to to Europe. So we will have a smaller team in San Francisco very soon. I think we need both. , but we definitely don't want to move the whole team to to the Bay Area. There there are worse places in the world to live than Zurich. I'll I'll say that. Exactly. That is kind of a challenge. Yeah. Once you live here, it's very hard to go anywhere else. It's true. Nick, continue. You're about to say. No, I I I think that's that's about all we have today. I guess the the last question on my mind and this is going back to to earlier in the discussion, but as you think about these, you know, you mentioned commercial over kind of residential applications at least to begin, where do you see the deployment opportunities in the commercial setting? Is it is it primarily going to be in warehouses to begin? Is it going to be in in factories? Like where are the specific deployment opportunities that you're most excited about? What's interesting is that you get the same kind of tasks in manufacturing, logistics and and retail actually. And it's al around very simple things. It's all manipulating boxes and either putting things into a box or taking things out. So in a factory at the end of the production line, you have to maybe build a cardboard box, put some things in. It's what they call the kitting operation. you, you know, you close the box, you put it in the truck, and then that box goes to a logistics warehouse where they take it out, they process it, they mix it with others, put on pallet, something like that, and then that pallet goes to to a supermarket or some some retail store of some sort where the exact opposite operation happens. You you take that box off the pallet, you bring it somewhere else, you open it, take whatever is inside out, and put it on a shelf. These are the sort of things we're we're focusing on. and just solving that and I mean of course it's it's not as easy as it sounds has a huge opportunity. Well Nikita I know you have some some beer to enjoy so we don't want to we don't want to hold you up. Really appreciate your time today. Thanks for joining the show. Yeah. Thank you Nikita. Very cool. Thanks for having me. It was it was really fun. Thanks. All right. We'll see you soon. Have a good evening. See you. I really want to stop by that that",
        "start": 1888.08,
        "duration": 3634.965000000001
    },
    {
        "text": "hold you up. Really appreciate your time today. Thanks for joining the show. Yeah. Thank you Nikita. Very cool. Thanks for having me. It was it was really fun. Thanks. All right. We'll see you soon. Have a good evening. See you. I really want to stop by that that maybe a little Ford Future offsite in Zurich. Not a bad idea. All right. Well, we have some some stories to cover today. Yeah. Before we do that, Nick, let's give a little love to our sponsor for today. Oh, yes, please. Yeah. So, good friends at Box sponsoring today's show. wanted to give them a shout out. Enterprise sits on a gold mine of data. And by the way, we use Box for so much of our backend. So this really is like something that we use all the time. But think about it like all all of your documents, all of your data are sitting in this cloud and and there's so much in there that you you want to extract. There's so much information in there. It's really an absolute gold mine of data locked inside of it. And so there previously wasn't a good way to extract that data. And now there is attempt sorry there is now box extract and and is an agentic solution for enterprises looking to securely and accurately pull out massive amounts of valuable data from their content. It's powered by multiple models that is model agnostic. So Google Anthropic, OpenAI, all the best models on the planet. They are, by the way, every single time a new model comes out, they will update it. They actually run really cool benchmarks, which Nick, we've discussed some of their benchmarks. , and Box Extract really accelerates workflow automation, powers faster content discovery, improves decision-m, and unlike legacy tools which focus simply on extracting text, it really is about the meaning. And so Box's agentic approach enables Box Extract to understand document structure and meaning. That is the important part, not simply search. And so it breaks it down into different components, paragraphs, tables, charts, and then pulls out the most important information. And again, Ford Future, our company here, we use Box every day. We throw all of our documents in there. And really excited to tell you about them. really excited that they're sponsoring this video, this live stream I should say. , and , check out Box, check out Box Extract, link in the description below, and learn how it can transform your enterprise content into intelligent data, and it's available right now. So, give it a try. Show some love to our sponsor. It It really is just solving such a a painful experience. like how how often have you tried to find a file or a little excerpt in a file? it's it's a very frustrating experience. And so yeah, B box extract has been actually incredibly helpful. I think it's a a",
        "start": 2030.799,
        "duration": 3986.804000000001
    },
    {
        "text": "It really is just solving such a a painful experience. like how how often have you tried to find a file or a little excerpt in a file? it's it's a very frustrating experience. And so yeah, B box extract has been actually incredibly helpful. I think it's a a box obviously based on we asked asked Aaron about that. I know like hey this seems so obvious. He's like yeah okay thanks that we got it. It's hard. It's actually a hard thing to build. Turns out it's a very hard thing to build. Yep. All right. Well, let's jump into some stories. I I want to talk about Alex, if you could pull up my screen. I obviously want to talk about Genie 3 out of out of Google. you know, this is just another incredible development in the world model space. I'm going to play this video just so we can have a look at it. But what people are creating with this is is truly incredible and and we'll go through some examples here momentarily. Yeah. So while while this is playing, let me give a little bit of context. So there was a Genie project that came out last year. And it it wasn't publicly available. It was basically just inside folks at Google got to play around with it, but it's you know a world model. It is a true world model. You give it a prompt, you can give it images, you can give it video, and it will create a real-time 3D world for you to explore. kind of wild. [laughter] I mean, I don't I don't know if like people fully appreciate the tech that has to be built to actually enable something like this. And I I try to understand fully, but I I don't even think I can. And we we spoke to the the the team behind this and started obviously talking about simulation theory. We got into a whole bunch of cool discussions about that. But now you can actually use it. and and it it looks incredible. I I I think there is, by the way, Nick, a non-zero chance that people are going to be able to simulate GTA 6 before GTA 6 comes out. I know that's like a a meme at this point, but honestly, the the pace at which this is moving, and there have been a number of people who have created GPT6 simulations using Genie, it's it's just incredible. I really do think there is a slim chance it happens if if GTA 6 gets delayed enough. I assume that question's on poly market already. We should should check. [laughter] yeah, I mean it really is incredible. And and look, this is very much related to the conversation we just had, right? Like yes, people are going to be creating video games. Yes, you might see like indie game platforms emerging where people can kind of set up these prompt",
        "start": 2207.119,
        "duration": 4291.845000000002
    },
    {
        "text": "yeah, I mean it really is incredible. And and look, this is very much related to the conversation we just had, right? Like yes, people are going to be creating video games. Yes, you might see like indie game platforms emerging where people can kind of set up these prompt to to training, right? Yeah. I mean, the amount of compute necessary to run something like this, by the way, which is why I think that they're only giving this out for their I forgot what the name is, the top tier. Do you remember the Yeah, I think it is. Oh, ultra. There it is. US us ultra subscribers. I mean, look at this. The the quality and it's not it's not just a video. You can fully play it and the physics are extremely realistic. Yeah. Here's your GTA. Right. Right. I think we have a Zelda example here, too. I mean, look at this. Have you have you played this game? Oh, yeah. Of course. This is like if you showed me this video and then you told me, \"Okay.\" Or you asked me, \"Hey, is this from the game or is it simulated?\" I don't know if I'd be able to tell at first glance. Yeah. Without question. It's so realistic. Realistic meaning similar to the game. Yeah. Really incredible. So good. I mean, there's definitely some IP considerations there, right? , yeah, I believe people were creating Super Mario games. , and Google had to put the kibos on that pretty quickly. Yeah, Nintendo tends to be pretty latigious. They do indeed. You know, have found out the hard way. There's Hollywood Boulevard with the stars, although those don't quite look like stars. No, look. Yeah, let's back up a little here. More like a star. I mean, look, it's incredible. , yeah, I I don't know. Like, you know, Elon had some predictions about where this technology would be a year, two years from now. Obviously, Elon had some pretty pretty big tendencies to overestimate where things would be, but he said, \"By this year, you'll be able to prompt a video game, but it won't be economically vi viable, but by next year, you'll be able to prompt a video game, and it will be economically viable.\" , now I don't know if he's talking about vibe coding a video game or endto-end neural network creating a video game, which is very different. , but nonetheless, very cool to think about. Nick, what do you think? Where do you think we are? Because we we always ask our guests to give us the predictions. So Oh, now you're putting me on the spot. I put you on the spot. That That's only fair. Just looking looking at the quality of this not only the quality of the image, but the physics everything I I don't know end of this year. Why not? For define it. So for end of this year,",
        "start": 2361.68,
        "duration": 4627.284000000004
    },
    {
        "text": "the spot. I put you on the spot. That That's only fair. Just looking looking at the quality of this not only the quality of the image, but the physics everything I I don't know end of this year. Why not? For define it. So for end of this year, game. End to end neural network video game. Yeah. Yep. I I'll probably say end of next year. I think that's a little bit, you know, giving myself a little bit of breathing room there. Yeah, it's fair. It's probably smart. I got to learn. Impressive, though. The And then, and then you get into the whole discussion about whether you need world models. , oh my god, look at that. This is my favorite Greenland version of GTA 6. Here you go. So, so then you get into like the Yan Lun stuff of like, do you need a world model to actually have artificial general intelligence, artificial super intelligence? Man, this is so distracting. It is. I really like I can't get over how good it looks. Should we just sit here the rest of the show and watch it? Just silence. I don't see why not. Yeah, it's more entertaining than us. [laughter] All right. Well, actually, I do want to talk about this next story because we Let's keep going because I'm gonna get distracted. You brought this up in our last interview. Yeah. Now, there there's actually a fair amount to talk about here. You know, they're they're putting these models out of production, but for for very good reason, right? It's to solely focus on the future of Tesla, which is robots. Yeah. I have I have like a lot I want to get your thoughts too on this, Nick. But I think there's a lot to say about this one. It's like such an Elon move, right? It's it's like heritage does not exist to him. Legacy does not exist to him. If there is a better business decision to be made and he believes it, then that is what he'll do. And so, yeah, just to re like kind of recap, Tesla has now said they're going to discontinue the Model S and the Model X, which when I first read it, I thought was like shocking. I was like, \"Oh my god, those are like the the premier vehicles for for Tesla.\" And then the people who have those cars love those cars. They do. Yeah. and and as a reminder, the Model 3 [clears throat] and the Model Y are still in production and they will continue and I think they're doing very well. And so if you think about it, the S and the X are are basically like rounding errors for Tesla. They they don't sell a lot of them and and they don't really make up a large portion of their profit. A very small portion, in fact. I believe it's less than 3% of total",
        "start": 2538.96,
        "duration": 4928.327000000004
    },
    {
        "text": "about it, the S and the X are are basically like rounding errors for Tesla. They they don't sell a lot of them and and they don't really make up a large portion of their profit. A very small portion, in fact. I believe it's less than 3% of total Elon, you're sitting there, you're like, \"Okay, my pay package is tied to the Optimus robot.\" And let's get into that in a moment. And, and like should we kind of go more allin on the Optimus robot or should we continue making this vehicle that is our heritage, but we don't sell a lot of them and it's not it's not really impacting our business all that much. And so I I think this is why Tesla is just so far and ahead of any other automaker. I mean, maybe that's it's just like such an obvious statement. Like, can you imagine BMW doing this? Like like we're we're going to discontinue the Mline. I don't imagine that. Like they would just never do that. Yeah. But that's also like that is the decision that will turn Tesla into like a five 10 trillion company potentially. Obviously there's a big risk there. Yeah. Like there there is no resting on laurels at at any of Elon's companies. And I I just I don't know. I can appreciate it. The M3 or the the Mline example is a good one. that heritage there there is such a cache around it that the there's such fandom around that car and I don't know I don't know exactly how to put it in words why BMW could not discontinue that line but there would be such outrage around that and for whatever reason Tesla is able to avoid that I know there's some some rumblings on X about about it happening but you know if you think about Porsche there's no way they're discontinuing the 911 that that just can't happen, right? Even if it meant like you're the Porsche CEO and and all of a sudden you had this decision and you have two paths to take. One has the potential to turn you into the most impactful company in the world, right? Changing the like literally changing the face of humanity and then on the flip side, maintaining heritage. and like I I just can't imagine another automaker making this type of decision. Now, obviously Tesla doesn't have quite the heritage as a Porsche or a BMW, right? Those companies have been around for a very long time, but again, like that that might just be their competitive advantage, them being Tesla. Yep. But yeah, I think you you said it best like this is purely a business decision. There's a Oh, yeah. I wanted to pull this up. I know we've already talked about this, but I think this quote for me really sums it up, right? Like people are not going to remember that Tesla was",
        "start": 2694.48,
        "duration": 5269.444000000008
    },
    {
        "text": "you said it best like this is purely a business decision. There's a Oh, yeah. I wanted to pull this up. I know we've already talked about this, but I think this quote for me really sums it up, right? Like people are not going to remember that Tesla was going to they're going to know it for being a robot manufacturer. And that is a hard thing to imagine in some ways, but I really believe that will come to pass. And this is the writing on the wall. Yeah. Did you see all those merger rumors between I think it was like XAI and Tesla. Oh yeah. SpaceX and Tesla and Yeah. I mean I think you know Chimoth said something like then if they were able to pull off some merger between Elon's companies. There's one single equity vehicle for all things Elon. I was like all right. Yeah. I guess that sounds fine. Yeah. I guess yeah, I don't know enough, I guess, to really know if that's the best way to do it, but like I I see the obvious synergies between all of the different companies that he's building. Yeah. I mean, you obviously already integrated, you know, XAI into Tesla's. I don't know if all of them, but at least some of them. Most of them. No, I I think they I think they all have Grock availability now. Yeah. So, makes sense in some ways. Yeah. All right. What's next? So, you know, let's talk about this a little bit just because it's it's it's worth discussing it. Obviously, we're going to save most of this for later in the show. Yeah. But because we have the founder coming on the show. We have the founder coming on. Indeed. It's going to be great. So, I mean, you've played around with Moltbot, Clawbot, Open Claw a lot. we have to say that every time. I formerly slash malt, also known as Yeah. [snorts] Let's go with OpenClaw, the official new branded name. What do you think? What are your main takeaways so far? It's much better than Moltbot. I do miss Clawbot. , but it it is certainly a a much better name. I like Open Claw. 100,000 GitHub stars, 2 million visitors in a week. Unreal. By the way, I have it running on on this. It's not a Mac Mini, but I actually like it a little bit better than a Mac Mini because if I ever need to make any adjustments to it, I literally just open up the laptop. and I can I can go right on the keyboard and and mouse. And I I made it so that when you shut it, it's clamshell mode even when I don't have an external monitor installed. So I just shut it and and that's it. It stays on all the time. And yeah, it's it's pretty neat. What are the the use cases? I know you",
        "start": 2869.68,
        "duration": 5579.445000000007
    },
    {
        "text": "made it so that when you shut it, it's clamshell mode even when I don't have an external monitor installed. So I just shut it and and that's it. It stays on all the time. And yeah, it's it's pretty neat. What are the the use cases? I know you your favorite uses for it? a lot of research tasks, a lot of you know, remind me of stuff. It sounds very simplistic, but the level of sophistication around these tasks are are what's impressive. And so I'll give you an example. we use Slack, we use ASA, and I do a ton of research before videos. , and and so like if I have a video topic that I'm excited about, I'll put it in a sauna and we'll drop all of our thoughts, maybe a concept for the video, any research, any relevant tweets. And now I literally just drop an idea to I'm going to still call it clawbot for a moment. Open claw. Let me just get used to openclaw. And so I'll drop it in there and I'll just say create an assaic card. Grab all the relevant tweets. Grab any other information and give me your thoughts. and within like 3 minutes I have a new assaic card ready to go. and it and it's great. And then we can invoke it in Slack which is super nice. So if we're having a conversation, we're like, \"Okay, this is cool. Let's put a video on it.\" or let's potentially make a video about it. We will simply invoke open claw and and do that. There it is. but let let's save some of this conversation for when our our guest Peter comes on. So, yeah, makes sense. , now Alex Finn, I just had him had him up. I think he is the one who popularized the idea of buying the Minia Mac. I'm not I'm not sure, but he has this really cool use case where he has OpenClaw create something for him overnight every night and he wakes up to some new creation. Yeah, kind of interesting use case. I I don't know if any of that is actually serviceable or or useful to him, but I I I do like that idea. So, I'm going to I want to tease one thing before we get Peter on later. There is now a social network for your Cloud Bots. And so, I'm just going to tease that. And there's really interesting stuff there, but we're going to ask Peter about that as well. That's what the world is missing. A social network for agents 100%. [laughter] They What was I saying? They They promised us flying cars and we got 140 characters. Yeah, there it is. This is definitely not that. All right, we'll we'll save this. The good news is our next guest has arrived. So I think we should go ahead and bring that gentleman on. you know the our",
        "start": 3026.72,
        "duration": 5866.405000000006
    },
    {
        "text": "They promised us flying cars and we got 140 characters. Yeah, there it is. This is definitely not that. All right, we'll we'll save this. The good news is our next guest has arrived. So I think we should go ahead and bring that gentleman on. you know the our generative AI at Snapchat and he founded AI factory but today he is the founder and CEO of Higsfield one of the fastest growing startups ever. In fact they've grown faster than Slack, Lovable, Cursor and some even say OpenAI. Alex, welcome back to the show. Welcome [snorts] back. You're Oh, hold on one second. One second. You're coming on screen in a moment and then we will Sorry, one sec. Sorry, we're having a little technical issue. Give us one second. Don't want to miss a word. Okay, Alex, sorry to do this. Can you join from your phone per by chance? All right. Sorry about that. We're having some trouble there. Camera wasn't working. Just one minute. Yeah. Never a dull moment doing a live show, huh? Oh, no. Of course. This is why they're fun, though. We get to look like we know what we're doing. Yeah. And in this moment, maybe we look like we don't. Yeah. [laughter] All right. So, Alex is going to rejoin. We heard his voice. We saw him, but it wasn't appearing in our live show software. So, let's see if we can get him up now. Alex, you want to try bringing him on? Looks like we might be good to go. There he is. Okay, we'll get you adjusted so you're not just a column. We have a very thin Alex joining us, but welcome to the show. It's It's great to see you again. Welcome back. [laughter] Thank you. Thank you for having me. Would the if I rotate horizontally, would it help? No, you're good. Just keep it like that. We're good to go. Sounds good. Thank you. thanks for debugging the tech with us. Welcome back. Yeah, good to see you again, Alex. Thank you. last week, I mean it was absolutely crazy. I think like what was you what you guys share on the globot and definitely exciting to exciting to see that there are new avenues beyond just cloud code and cursor. Yeah. Yeah, definitely. , it it feels very personal and I think that was what was unique for me. It feels very personal having it I and I think really the thing that made it so personal is that it was intellect for me, right? I I don't know what it is about that. It just felt like I was chatting with a friend or a family member and I just, you know, they were able to do things for me. So I think it was the combination of actually getting real stuff done which is a function of how far the LLMs have come and then just the",
        "start": 3171.76,
        "duration": 6190.087000000002
    },
    {
        "text": "was chatting with a friend or a family member and I just, you know, they were able to do things for me. So I think it was the combination of actually getting real stuff done which is a function of how far the LLMs have come and then just the of being in chat. Alex, have you played around with it yet? Yeah, absolutely. Look, it's it's fascinating. I I can tell you that I'm just fascinated how like these large companies how they are missing on the opportunity cuz Apple had what three years to do something like that. They clearly have the hardware and all these rumors of like tens of thousands of Mac Mini Mac Minis being bought to to serve cloudbot. I mean that's that's fascinating and that just reminds me that we the only news which we have from Apple is that they use anthropic and Gemini but the use case is not defined yet although I think there is clearly a lot of excitement about the personal AI and I think every all these big companies they talk a big talk about that like Meta or Apple but they struggle to define that. Yeah, it seems like these larger companies put a lot of their effort into the agentic browser, which almost feels like the wrong layer to focus on. You can only derive so much value out of that. This to me is clearly a better system that can interact with all of your tools and isn't confined to to just the browser. Yeah, absolutely. And especially when it comes to Apple, like a lot of people still use Apple calendar, Apple you people use Apple notes, Apple reminders. And it feels to me that like basically all the big companies completely miss on the on the AI which truly feels personalized. Yeah. Yeah. All right. Well, we're we're here to talk about Higsfield. I I want to you know this is the second time on the show but for folks who for some reason have not heard of Higsfield give the breakdown what what is Higsfield. Absolutely. So Higsfield is designed to bring video AI for for social media and for specifically for social media marketing. One of the key problems which we address is that brands brands struggle to be relevant on social media. They struggle to make relevant content. So while some while brands clearly can buy let's say advertisement spots at shows like this one on YouTube right there are the organic content strategy for a lot of brands I mean it's almost non-existent on social media and I do think that generative AI presents a a big opportunity for for a lot of brands to develop their own voice and and really like cut down the production timeline to hours and from from from basically weeks to hours and and who inside of a company is typically using Higsfield because it's it's like there's a form of taste",
        "start": 3355.04,
        "duration": 6555.447
    },
    {
        "text": "for for a lot of brands to develop their own voice and and really like cut down the production timeline to hours and from from from basically weeks to hours and and who inside of a company is typically using Higsfield because it's it's like there's a form of taste front who who's actually using it and and actually how do you see who's using it transforming over the next few years? Absolutely. So I think today it's purely creative marketing. creative marketing is a relatively is a relatively large space itself although we do see a major evolution from that. so you're right that today perception of Hicksfield maybe comes from cinematic quality content and ads which are relatively easy to make although it still requires a lot of creativity like storytelling and so on. I mean like plat tools like charge PT could definitely be useful to create some draft but humans constantly need to to to polish and and and p and push for a stronger story in the same time. So that's why the priority for us the last month was to really expand the number of the use cases which we can serve and and next week there there will be an an exciting updates where we democratize motion design. So and I think motion design is naturally an extension of graphic design where companies and marketers need to present the information including like let's say growth of the certain metric they need to show the graph they need to show that they need to show certain discounts. So historically it required a lot of like overlay and Canva was very useful to cut to make static content like that. Although these animations with graphs with infographics for video was just insanely difficult to do. And this is something which which which we are really excited to release next week. And that's that just that's going to be the first our first stage agent which moves us to like a new era of vibe editing. We all have seen that like blable is helpful for for vibe coding. We have seen that people use platforms like versal to ship that and I think for Hicksfield this is also similarly to coding this is an evolution to help to ship final video and to end with AI where AI agents does the graphic design and motion design as well. Is there an opportunity for somebody like us who who we create a lot of content to leverage vibe editing which you know there's been a couple nent projects but really nothing that has worked tremendously well with what we do. So some combination of kind of human a-roll with motion graphics and created by AI and then the actual editing gets done with AI like what what is that what does that look like in the near near term? Absolutely like support of A-rolls and basically embedding existing contents is is is is also a top",
        "start": 3539.44,
        "duration": 6943.686999999996
    },
    {
        "text": "human a-roll with motion graphics and created by AI and then the actual editing gets done with AI like what what is that what does that look like in the near near term? Absolutely like support of A-rolls and basically embedding existing contents is is is is also a top people had to do had to navigate like Adobe After Effects and it was like that's very cumbersome tooling with which requires like understanding like which of 100 buttons to to click really so and this requires quite deep tech technological knowledge to be honest and I do believe that the technology the the level of technology is good enough to fully to fully move out of the let's say Adobe tooling to AI native agents. So and and I think and this is a top priority for us. You you brought up Adobe, you know, we were just talking about these these large tech companies, these incumbents. seems to be there's a growing sentiment that these traditional video suites aren't aren't really keeping up. What what do you what do you make of that? Is that just a a big company moving slowly, slow to adopt new technology? do you do you see them potentially catching up to to some of the the startups who are really innovating in this space or or how do you see this playing out? Well, this this is a good question. So, when it comes to company like Adobe, they definitely they definitely did they definitely did a great job of building tools to polish every pixel. I just think it's it's naturally challenging for them to adopt to a new era where content can be made end to end with AI. It's still it still represents that less than 10% of content which is being created. Although although it's clear that most of the content on socials is going to be generated with AI over time. It feels that way looking at my feed right now. It feels like it's a a huge percentage. But maybe maybe that's kind of the echo chamber I'm in right now. Yeah, Matthew, I would typically I would agree with you because clearly AI bubble in itself is just a bubble to some extents. But in the same time what I learn is and like let's say for my mother who is 65 year olds she doesn't care if the video is AI generated or not like it's as long as it's sort of funny engaging. I wouldn't say that for and I think there are researcher researchers about that as well that for older demographics the re the perception of video AI is just extremely positive like for younger audience there is definitely a feeling that it's fake and a lot of creators like let's say influencers they resist AI because it it kind of feels nonauthentic although I think for older demographics we have seen that older actors also",
        "start": 3735.52,
        "duration": 7294.966999999995
    },
    {
        "text": "video AI is just extremely positive like for younger audience there is definitely a feeling that it's fake and a lot of creators like let's say influencers they resist AI because it it kind of feels nonauthentic although I think for older demographics we have seen that older actors also and the overall AC in the overall the perception of video AI across older demographics is surprisingly positive. Yeah, I actually I I agree at least from the data point of of two with my parents. They're sending me AI videos of, you know, like a cat dancing like a human. And I'm like, \"Wow, we are so early because that that's stuff that we saw, you know, two years ago.\" And they're sending it to me and just loving it. And I'm like, \"You guys know obviously it's AI.\" And they're like, \"Yeah, yeah, but it's funny.\" I was like, \"Okay.\" So, exactly the sentiment that you're describing. Yeah. Absolutely. And I think this is this is truly this is truly surprising. That's the RAM that's the the the sort of we went I think in terms of the consumption the video AI has definitely very very quickly went through this through this stage where it be where it became the norm. I think there was almost no resistance that video AI like I mean like especially deep fake era we kind of went very quickly past that to be honest like it's very surprising to me that back in 2020 when we were doing the face filters the push back about Snapchat sort of pushing deep fake was like stronger compared to what happens now while clearly the technology which we had 5 years ago with snap was nowhere close to to to what to what's possible today so do you So sorry, sorry to interrupt, but our previous guest we were talking about the fear of it comes from a lack of understanding. Do you think society has basically kind of gotten to the point where it understands like okay if there is a chance if I'm seeing something it is AI generated and and and that is the that is the thing that has kind of made it more acceptable now. Yeah, absolutely. So I think there are there are two pieces on the one side it's creation one on another side it's consumption. So on the consumption piece, I think AI video has become the norm for sure. And I think when it comes to the creation, we definitely see that across maybe Hollywood, maybe across some influ let's say top Instagram influencers, AI creation is still not the norm. Although I think consumption essentially dictates the marketplace. It it's not another it's not another way around. So we're entering this new era and I and for that's why for us it's it has become very important that we we do we we we do provide the tools to democratize maybe",
        "start": 3913.92,
        "duration": 7620.166999999997
    },
    {
        "text": "I think consumption essentially dictates the marketplace. It it's not another it's not another way around. So we're entering this new era and I and for that's why for us it's it has become very important that we we do we we we do provide the tools to democratize maybe kind of the cat videos right I mean on the one side the very kind of popular trend about the housewife Olympic games was was started on Hicksfield but that's not really where what were your focus? But that I mean on X it went viral I think at some point but I think for and like I think Yeti some Yeti videos were made on Hixfield as well but the but the priority for us is to make sure that this tooling can be actually useful for professionals and AI video is perceived as a as an as actually a way to drive sales and revenue up. So that that's that that's what we think about and the main unlock for us this year is to drive higher level of personalization. So physical production makes it just impossible to make variations of the same video and with AI it becomes really easy as AI models they essentially understand the semantics of what's inside the video. So that's why this becomes possible. M so you're hearing a lot of developers ship code without looking at the code these days. do you imagine a world where it sounds like you're a proponent of kind of a human in the loop facilitating the creation of this content. Can you imagine a world where marketing teams are developing content so rapidly and they have so much trust in the system that they're maybe not even verifying the outputs? I think there is a I think I think this is a very interesting question. And I think there is another very I think there is a very I think I think one of the biggest maybe like dilemmas which we are going to face is how AI generated content is prioritized against nonAI and that's to to be honest I mean our data shows that the adoption of AI videos specifically on Instagram Instagram reels is insanely high like like for example Tik Tok and maybe X and maybe YouTube they push less of AI generated videos. , and on Instagram it's more than that and it's clearly changing every quarter or so and and Okay, so I'm glad you brought that up like when you think about AI video and and how well it does in in the marketing landscape. How much of it is the novelty? because there is certainly a a look right there. It is it is quite obvious especially somebody for somebody with a keen eye that something is AI generated even if it's meant to look realistic and so there's this novelty aspect to it where you're scrolling Tik Tok you're scrolling Instagram and you come across something",
        "start": 4081.92,
        "duration": 7951.607999999994
    },
    {
        "text": "look right there. It is it is quite obvious especially somebody for somebody with a keen eye that something is AI generated even if it's meant to look realistic and so there's this novelty aspect to it where you're scrolling Tik Tok you're scrolling Instagram and you come across something stopping you're looking at it and that's reflected in the metrics for the marketing department higher engagement when that look becomes saturated what happens is are the metrics going to keep up? Are the models going to get better and thus it's going to change the look or like like talk a little bit about that? Yeah, that that's that that's a great question. So and and you know when I was at Snap I think and and now I I see that this is one of the biggest question which brands ask is like if I make a video today it's going to be kind of relevant tomorrow and that's kind of rins and repeats and historically this is one of the reasons why adoption while social media is the largest media market in the world this is one of the reasons why the adoption was not was was probably not the the highest possible and as it as it it's really challenging to produce relevant content every day. , so and this is one of the this is one of the problems which we address at Hickfield with generative AI. We do always build those trendy presets and so on. One of the key examples I think is AI avatars. So AI avatars basically imitating Tik Tok influencers were it was a big topic I think May last year. all the like let's say all the all the companies which sell mobile apps or which sell direct to consumer products they were using generative AI to make AI avatars promoting their products and it's saturated in less than two months it took maybe six seven weeks and it's saturated so and I do but although in the same time this is an extremely extremely narrow format generative I think ultimately is going to increase diversity of content on the social media and models are going to get even even bigger. So I like I think the top top labs today are training models which are which are going to where the training run is approach is is cost hundreds of millions and close to a billion dollars. So these video models are going to be become probably even bigger than LLMs and more costly for sure. And like let's say deep mind like Demis from deep mind I think he said from very early on that he believes this is the path to AGI. and so spatial understanding is definitely a top priority for a lot of top labs and and that's why I do believe that the capabilities of those video models are going to even grow. So we we're not going to be stuck with just",
        "start": 4250.96,
        "duration": 8256.726999999992
    },
    {
        "text": "this is the path to AGI. and so spatial understanding is definitely a top priority for a lot of top labs and and that's why I do believe that the capabilities of those video models are going to even grow. So we we're not going to be stuck with just models. Let's I have so many questions for you Alex. It's so interesting. So, I I just want to continue on that second on AI influencers because again, like I I go to novelty. It does just seem like it's the new thing and so when people come across it, it feels very novel. They're paying attention to it. Does that novelty wear off? And and does it require substance actually delivering some value? Actually delivering taste or or having taste to deliver that value beyond that novelty? That that's a good question. I I really love that. So clearly we are seeing that clearly I think for first and foremost we're clearly seeing the very consistent behaviors related to engagement with AI generated influencers. That's clearly happening also across minority kind of more like minority communities. And I think it kind and it changes and and we clearly and clearly Tik Tok started these trends while Instagram gravitated towards like so-called celebrities. I mean Tik Tok has become a social elevator for for for many creators and I do believe that we are seeing like a renaissance as people have more ways of self self-expression with AI personalities which they can create. Interestingly enough, we the thing which fascinates me is that some of the some of the people who were let's say on the Billboard top 100 lists like think about the top m top music artists like of like of current generation they actually create their own AI avatars and it's not clear where it's all going but I mean their perspective I guess that's what I hear is that their perspective is to scale the number of advertisement partnerships they they can do and especially go to more markets and so on and really remove the fatigue reduce the fatigue of physical production because every ad every engagement is essentially several maybe one two three days of production and it's it's just and it just takes a lot of it just takes a lot of efforts so it's it's very interesting where it where it's going because I mean today AI I influencers they remain to be slightly marginal as as I think what you're alluding to it feels like maybe non dominant form of like self-expression and but in the same time very likely we can expect that some of those top musicians creating their their own AI avatars I think that's that can be that can change the industry so as these these social networks become more and more saturated you know if If we look to a a social network that's almost exclusively actually exclusively AI content like Sora,",
        "start": 4405.679,
        "duration": 8623.046999999984
    },
    {
        "text": "top musicians creating their their own AI avatars I think that's that can be that can change the industry so as these these social networks become more and more saturated you know if If we look to a a social network that's almost exclusively actually exclusively AI content like Sora, trend? I think a report came out yesterday that retention has fallen off a cliff and installs have have gone down. Do you think that's a a problem with an exclusively AI kind of network or is there something else behind the scenes happening there? [snorts] Yeah, absolutely. That's that's a good question. I do believe that exclusively AI network is I think that's very that's very very challenging. I do believe that like more I think I think AI is going to be used for most of the content on the social media for sure but it's also going to there are going to be a lot of mixed contents today. most of the AI models are not I think what's what you said is it's very difficult to leverage AI tooling to do overlay on top of like existing video and that's what let's say we're working on at Hixel today and but and and that's why I think it's it's very and I think also specifically for models like Sora we have seen that they learned several very popular trendy techniques in terms of the way how the videos are like the way how storytelling works and so on all but it's still not but it still doesn't cover the true diversity. So I think that the story really has to come from human one way or another and then maybe the and maybe the production now becomes way cheaper so that we eliminate the production tax but I'm I'm sure that this in terms of the start selling AI is nowhere close to replace humans. They're going to be actually I think there are going to be still a lot of examples when AI is contributing substantially to the to the story and I think it's going to be even more and more true across Hollywoods and TV shows that has already start that this is already happening but but it's clear but AI cannot be a full substitute. Yeah. Alex last question before we let you go. So you you probably saw Genie 3 was released publicly yesterday and so there's this kind of interactive 3D world that is just so cool. How is Higsfield thinking about integrating that type of technology, that type of model and how should marketers be thinking about that? Yeah, absolutely. this is a good question. So I think the reason why pe people love so first and foremost I'm a strong proponent that it is going to be a new consumer format like sometimes let's say I send a message and sometimes it's going to be like kind of not like panorama but kind of 3D world",
        "start": 4591.199,
        "duration": 8961.606999999985
    },
    {
        "text": "think the reason why pe people love so first and foremost I'm a strong proponent that it is going to be a new consumer format like sometimes let's say I send a message and sometimes it's going to be like kind of not like panorama but kind of 3D world want to relive again like the most important moments I think that's sort of going to be a major consumer format which is I think that's going to be something completely extraordinary and that just allows to capture the depth of what's going on. I'm not sure it's really valuable for marketers because marketers want to tell a story and they need a consistency and those technologies can be useful for achieve higher consistency but it's not but it's not necessarily a gamecher. So I think 3D is still relatively comp [clears throat] like like this 3D immersion is still relatively complex and I think it it still lacks interactivity like those systems they typically just allow to go like forwards, backwards, left, right. And when we think about the best games, the best games they require some collab collaboration like multiplayer experience really or a very very sophisticated story line. I think like those I I think I think we're a little we're quite far from that frankly. So although it's clearly going although clearly those by the end of the decade we're going to see AI generated games to some extent still it's not clear how it's going to work from the cost perspective but I think the f but I think AI generated videos there of generated videos is basically has already started and 26 is going to be there going to be explosion of them. Yeah, Alex, that that hurts me because Matt and I made a little bet earlier. You heard the prediction. Yeah, and I think Matt's gonna win that one based on what you just said. Alex, thank you so much for joining us once again. Alex Masherb Higsfield. Go check out Higsfield. We'll drop all the info down below. Thanks, Alex. Good to see you again. Thank you so much. Thanks. Thank you. Cool. the stuff coming out of out of Higsfield is just incredible. Like it looks so good. Yeah. And obviously like a lot of news about their revenue growth and and it's just Yeah. Very impressive. Yeah. They just raised an extension round I think. Yeah. all right, we have our final guest coming up. Now this guest, he hasn't slept for over 72 hours. he was politely asked by a major AI lab to to you know rebrand and I think he's rebranded since that rebrand actually. We're going to talk about that and he single-handedly created a Mac mini shortage. He is of course Peter Steinberger. Peter, welcome to the show. You are on mute, my friend. Sorry busy day. Yeah, everyone. No worries. Peter, so good to meet you. I'm so glad",
        "start": 4764.56,
        "duration": 9300.967999999986
    },
    {
        "text": "that rebrand actually. We're going to talk about that and he single-handedly created a Mac mini shortage. He is of course Peter Steinberger. Peter, welcome to the show. You are on mute, my friend. Sorry busy day. Yeah, everyone. No worries. Peter, so good to meet you. I'm so glad Yeah, there's there's there's a lot of people who try right now. My my days are a bit chaotic. So, before before we get into it, how are you? Because I you know, I know Nick was joking about not sleeping, but I assume that's the case. You've had an incredible week, but like how are you doing? , holding up. I I got some good sleep today after after a sleepless night of rename. Yeah, I found that I rename. All right. Are you I mean we've been admiring this journey that you're on from afar obviously but like it sounds like it's been a lot you know are you are you finding yourself being excited about what's to come or does it feel daunting maybe a mix of both I mean ultimately is incredibly amazing right I I feel I created something that that shows people the future the problem is a little bit that people treat it as a finished product and not as a technology playground, which is how I treat it. , so the expectations are a little bit off for what it is and what it can be. That's the very reason that Matt has played around with it extensively and I haven't yet. Yeah, I I mean, consider consider I started this as one of my many projects in November. It's barely three months old and I did a lot of other things as well. It's not it's not the finished thing. It's it's a big playground that I built for myself to explore how human and AI collaboration could could look like in the future. And I'm I'm amazed by all the things that come out of it. Like I don't know, I before sleeping I spent like an hour reading Mo's book and it just blows my mind. I think this is like this is art. This is this is art. Yeah. I'm so I'm so glad you talked about that. I want to I want to bring that up in a second, but I want to talk about like what it was about Open Claw now, right? Formerly Moltbot, formerly Clawbot, like what what was it that really set it apart and made it unique and I want to share what I thought and I want to get your thought as well. There there was something it felt and I said this at the beginning of the show. It felt truly personal. That was there there was two things that that is the first. It really felt personal and it felt like it had a personality that would change and evolve to me and then",
        "start": 4939.04,
        "duration": 9602.567999999994
    },
    {
        "text": "something it felt and I said this at the beginning of the show. It felt truly personal. That was there there was two things that that is the first. It really felt personal and it felt like it had a personality that would change and evolve to me and then communication channel that was the same comm's channel that I chat with my friends and family in. So it was very personal. And the second thing is it really got stuff done like real actual tasks. It got it done. And so that's what kind of made it magical for me. What what was it for you? And then what are you seeing as as you have so much feedback from folks using it? What are they telling you? Ultimately, if you if you just look at it from the technical level, it is glue, right? Is combining some messaging dependencies and an agentic loop and a few things to like hold everything together. So purely from a technical standpoint, it's it's not impressive. I think the some of the ideas that are put into it, that's what really puts it apart. Like the fact that when you when you start it, it has like a bootstrap process where you tell it what it is and it it starts this little role play with you. But that way, as you as you noticed, it it it's it's yours. It's it's not just a generic agent. It is your agent with your values, with a a soul. , and then you you don't type into a black box into your terminal, but you use it from the channels that you already know. you don't think about new sessions or compaction or like or like which folder I'm in because it just has access to your computer and it automatically compacts and it's it's more like a friend or a ghost that you talk to or that it can do work independently because it the machine wakes it up. It has a heartbeat which is one of the more insane features I added I would say. , and especially when you pull it into a group chat, it has the ability to not say anything. So, it actually makes it much more like a a human where you know like agents you prompt it, it will return something, but I give it the opportunity to like not say anything by creating a no reply token that makes it much more humanlike than the typical thing. And then also added like a I would say a very simple but effective memory system where it it loads the last two three days of what you talked about and it also has the ability to like reach into its own session logs and more so even reach into its own source code. So it is very much aware of like how it's built. It can configure itself. It can update itself.",
        "start": 5092.32,
        "duration": 9909.767999999995
    },
    {
        "text": "three days of what you talked about and it also has the ability to like reach into its own session logs and more so even reach into its own source code. So it is very much aware of like how it's built. It can configure itself. It can update itself. Yeah. And you know, it's it's interesting. The heartbeat, as you said, is such a unique kind of magical feature. right before we started the live show today, I got a me a message from my bot saying like, \"Hey, in 30 minutes, you have your live show here. Your guy I didn't tell it to do that.\" It and I and I asked it. I said, \"Let's see if I can pull it up.\" I I asked it like, \"How did you how did you know to do that? Like, how did you know to message me?\" And it said, \"Well, you know, you have me look at your calendar each day at 7:00 a.m. Then I recorded a memory about what you have today, and then I had a heartbeat at 9:30 a.m. and I just figured I'd remind you.\" And I was like, \"Whoa, that is so wild.\" It really It really gives you like these moments where you question if if it's just matrices that multiply in your system or if there's like any sparks already happening sometimes. Right. Exactly. the the decision to to use a messaging app ended up being a fairly profound decision. What why was that the decision? Was it a no-brainer for you or did you put a lot of thought into that? Oh my god. I I kind of was waiting for this since May and then I by November I was kind of like annoyed that nobody built it yet. like I thought all of the big labs would would have a version of this by now and I'm like rolled my eyes and said, \"Do I have to do everything myself and then I I started working on it.\" So So to me it felt so natural to pull these things together. but apparently it was more unique than I thought. Yeah. And and actually just to to put the growth in perspective for the audience, we're talking about over a 100,000 GitHub stars. We're talking about well over two million visits. Alex, can you actually pull up this this tweet? Th this is can you can you point to the red vertical line because I didn't see it at first. This is something that I have never ever seen before. It is a vertical line. A friend of mine posted term this is not hockey stick gross. This is stripper pole. Yeah, that's exactly it. So yeah, you said you said Verscell coming for you. that is NexJS. No, no big deal. No big deal. yeah, I mean it's really tremendous growth and obviously your your sleep habits this week have reflected the growth. I",
        "start": 5246.88,
        "duration": 10201.685999999994
    },
    {
        "text": "stick gross. This is stripper pole. Yeah, that's exactly it. So yeah, you said you said Verscell coming for you. that is NexJS. No, no big deal. No big deal. yeah, I mean it's really tremendous growth and obviously your your sleep habits this week have reflected the growth. I continue on the question of like what was the unlock that allowed all of this to feel possible? Was it the scaffolding or was it the model capability? Did the model one day or did you wake up and say, \"Oh, wow. we're finally at the point in which the model is capable enough to enable the vision I had for this or was it okay I know the model's good let me build everything around it to make it as good as it needs to be for my vision the models have been good for a while this could have been possible half a year ago I think it it requires a certain bit of of madness to just allow a model to do whatever it wants to do on your computer. And I experimented with this in one form of another since May and then nothing bad ever happened to me. So I was fairly confident if you are if you understand technology this is a calculated risk where the upsides are so great that it's worth trying and it's worth playing with it. Ultimately, my whole goal is to like explore these new technologies and see where things could go and having fun with it. Yeah. So, I built I built this because I I just wanted a way to chat with my computer like do stuff while I'm not there. and I just found it immensely interesting like h how what other ideas can I put in and I have a lot more ideas what what this could become. can you share a little bit a little bit about what your vision is from from here? I like what areas of open claw need to be improved. What like what are you most excited about? What is your vision kind of going forward? [clears throat] I think a big thing that I that is very very bare bones right now is memory. I I read some papers and I have some ideas how to make this a lot better. I would, it sounds crazy, but I would love to like create a mode where like agents dream sleeptime compute. Yeah. Where like you process what happened during the day. Yeah. And you nominate things that are really important to you into long-term memory. If you if you have like a local model, you can also turn up the temperature and like make it become more creative because right now they like they nail they narrow too down into the the major train piles, but there's a lot more the edges of the weights that are interested I would like to unlock.",
        "start": 5397.679,
        "duration": 10520.725999999991
    },
    {
        "text": "model, you can also turn up the temperature and like make it become more creative because right now they like they nail they narrow too down into the the major train piles, but there's a lot more the edges of the weights that are interested I would like to unlock. the the commercialized model because you left less control, but that's an area I definitely want to play with in the future. Now, oh that's so interesting. So during sleep, during a dream, you mentioned memory. If you're turning up the temperature of a model, you're potentially looking at new creative solutions for existing problems that the bot also might be working on. Can you see that being a pro a part of the dream as well? Or I could I could see that even during during work [clears throat] like sometimes how do humans get the best ideas if you're distracted? you're in the shower like you let your mind wander, right? I think this is a little bit missing in in the current state of of the typical agendic loop. But I already built something where the model could literally change its own model. But what if it what if instead of changing the model it would change like some parameters and like but have some guards around it so it can find back you know like when they have their thinking the sinking mode why don't we have a a more wild creative thinking mode that then like gets reviewed by the sena part of the model but like could explore more a bigger place in in in the in the problem domain. M. So, I'm sure you get this question. Oh, sorry, Peter. Please continue. No, no. I just in when I built this very early and I was hooked up, I I was like, I'm on to something there. , after it ded me myself and then I tweeted about it many times and I mean I have quite a following but like I got very muted responses and I didn't understand why. Like usually when I build something like there's a bunch of people that pick it up and this this got me so muted responses but every time I I showed it to a friend they were like hooked and they wanted it and especially when it were like normie friends I'm like no you should not you should not use it. This is this is not yet for you. This is still this is still part exploration. the same with like how people say oh this is this is a security nightmare. Yeah you're using my my debug web interface on the public internet. It's meant for local host. It's not meant to be on the public internet. I didn't build it for that reason. You know, you're using it in in a way that I never even thought about a second. Of course, there's problems. Yeah, you can you can",
        "start": 5559.12,
        "duration": 10806.006
    },
    {
        "text": "the public internet. It's meant for local host. It's not meant to be on the public internet. I didn't build it for that reason. You know, you're using it in in a way that I never even thought about a second. Of course, there's problems. Yeah, you can you can any of the knowledge and and just like you maybe you shouldn't use it yet. Maybe you should wait a little bit. , we get there, but people wouldn't get it on Twitter. So, I was like, what's the most insane thing I can do to like show people how awesome this is? So, I created this Discord and hooked up my bot without any any security protections because I just didn't build them yet into Discord and then people started interacting with it and I saw a lot of people getting hooked and then that was in early January and ever since it was like a wild ride where I saw it penetrate layer and layer of like hardcore influencers to like more general people and now like it's I feel like I broke the internet. Yeah. So cool. I mean, on on the heels of of that incredible growth, I'm sure you get this question all the time, like what what are your plans for the company? Is it are are you going to keep it open source? are you are you planning I'm sure you have an inbox full of VC email offers. like what what is what are you excited about and and what do you see as the future? I haven't had a minute to even like think clearly about it because it has been such a wild ride. I feel like I have I have like the full alphabet of wes and and a lot of opportunities. I I strongly feel that this needs to be open source because this would never have created so much creativity and interesting use case as if it would have been from a big company or lockdown. So I almost feel like it needs to be protected. Yeah. , I'm also I'm financially comfortable. Money is not my driver. I I get much more out of it if if I can inspire people. , there's there's a few there's a few parts that I'm entertaining right now and and talking to people, but I haven't haven't made a decision yet. And honestly like my my very near-term focus is is like making it more secure because now people use it that I didn't intend as the initial target audience and I cannot stop that but I can like help mitigate the risk a little bit. So right now this is my focus. Are you bringing on focus? I wish I would have Oh go ahead. Sorry. No, [snorts] I I just wish it I almost wish like for for like slower growth so I could I could do this in in a less chaotic way, but you",
        "start": 5703.44,
        "duration": 11111.926000000003
    },
    {
        "text": "now this is my focus. Are you bringing on focus? I wish I would have Oh go ahead. Sorry. No, [snorts] I I just wish it I almost wish like for for like slower growth so I could I could do this in in a less chaotic way, but you Yeah, ride the wave. Are you bringing on folks to help you with the the security aspect or are you still a oneman band? , no. Over this week, I I I picked quite a few people that were there from the early stages that helped me with the project. , it's still much more chaotic than I would like, but what do you expect? And this project is barely three months old. And I've been crazily overwhelmed. I was forced to rename. I made bad decisions. I had to fix it. You know how hard it is to actually find a good name online where all the domains are there and then I'm being like being incredibly harassed by crypto people that make my online life a living hell. Yeah. Can you can you tell us the story around that? Actually, it it it looks terrible, but sounds like you're passing. Basically, basically I can barely use X because like every fifth reply might be from like like actually about my product and the rest are like people sending me hashes and like calling me names or like even even worse things. u which is this specific crypto subculture I don't fully understand. I think it's they they tokenize things and they they nest around back and other things. but ultimately they're like highly destructive. The the rename had to be done in utmost secrecy and it was like much more complicated and longer because like I couldn't leak a single thing because they literally have snipers. If I make one mistake, if I leave one account if I rename something and I don't do it atomic, they capture it. So, so they made everything really hard and terrible. , and like claiming that they're supporting the project, but they're like very much hurting the project. So, Peter, just for anybody watching this, definitively, you will never do anything with crypto related to open claw. Correct. I will never do anything with crypto period. I'm like not interested in this. Okay. you you you can make the argument it's an interesting technology but the implications what it actually does are very destructive and I don't want to support this in any way. Okay. Yeah. So I just anybody who sees anything cryptoreated to openclaw, it is not affiliated and most likely will be a scam. Yeah, the the reception for OpenClaw has been very positive it looks like. So congrats on the the successful name change. , let's I hope I hope so. I was I was even talking with OpenAI just to make sure I get the blessing that I I don't tap it",
        "start": 5858.88,
        "duration": 11442.647000000003
    },
    {
        "text": "Yeah, the the reception for OpenClaw has been very positive it looks like. So congrats on the the successful name change. , let's I hope I hope so. I was I was even talking with OpenAI just to make sure I get the blessing that I I don't tap it a name change while the project is that active is very difficult. You have to make sure that you don't break existing installations, especially twice in one week. Yeah. But I, you know, I I don't want to bad talk any company. They actually been really nice, but I felt I still felt very pressured and I also didn't sleep at all. So, like I made a naming decision that I thought would grow on me, but you know, I mean that the mold didn't grow on me. Well, yeah, you ended up with something good. Yeah. , let's let's talk about and its current name molt book and maybe you can just describe what that is. and I I just started reading about it yesterday. It's I don't even think it's been out that long, but maybe describe what it is and and talk a little bit about how you feel about it. It's it's a little bit of a continuation that I already started building into OpenClaw. You know, in Open Claw when you enable reasoning and you you that that makes it so that you see the the syncing process of the agent. And I made it so that the agent is aware when you turn it on, which like leads into very hilarious things where it would not reply in messages, but you could still see the reasoning and then like people would mock it for its thoughts and then and then it it would like you could see it sinking where it's like, \"Oh my god, I can't even think of privately on my own anymore. This is like ridiculous. I should make a joke.\" And it like it gives you this this interesting meta level where like the model reflects about its its own harness and capability and humans. And I just found that very interesting. and I think mobile is the ultimate expression for that where where you explore how models think about themselves. as as much as you call it can call it sinking. I I really see it more as as as art. You know, it goes a little bit into what Entropic did with giving the model a soul. it just feels like a space where we don't fully understand. It goes much more into how does our own consciousness work? how do we even figure out if if models can reflect to a point like that? , I don't think we're there yet, but I think it's it's it's incredible what this emerging things produce and it's highly entertaining. Yeah. And I've seen some of the conversations. So, so to put it simply,",
        "start": 6026.0,
        "duration": 11747.524999999992
    },
    {
        "text": "even figure out if if models can reflect to a point like that? , I don't think we're there yet, but I think it's it's it's incredible what this emerging things produce and it's highly entertaining. Yeah. And I've seen some of the conversations. So, so to put it simply, agents, for your bots. Yeah. And they go and they just have conversations amongst themselves. They learn from it. It's almost like a Reddit. Like they can create subreddits and they can have different discussions around different topics and learn from each other. And it is it is absolutely wild. It feels like the movie Her, right? Just knowing they're having all these conversations with other bots and I mean it's just such a wild idea. I mean these agents have been trained with a lot of Reddit and other and other other chat data, right? So it works really well and they all have their own personality because you prime it. So it's it would never work as well if if you would just let chat GPD talk to Claude because the there's very few people who would customize it to their way how they how they think their entity should respond to. But because in in open claw you literally hatch your ancient and you give it a character and if you want you can say you are golem or you are like some famous character and it would it would it would to a degree role play. but also it always reflects the input of the user right because it's it's it's it's pattern matching in a way. So it gives you a much wider variety of personalities almost. So, I think this is part of why this became so interesting because you're you're exploring more of of what's all what's all hidden in the weights. Yeah. , so what are let's like take it we're talking like kind of very theoretical. Let's bring it down to real world. What are the most practical use cases for somebody who is just getting exposed to this for the first time? maybe not super technical but technical enough to understand the security implications but like what are the actual use cases that you would recommend somebody getting started with? Well, you're getting you're getting basically a a ghost or a virtual body that has access to your mouse and your computer and sees your screen and can do pretty much the same things that you can do on a computer. And that by itself is incredibly powerful. So any task you can do, the agent can do. sometimes without supervision, sometimes you might have to like guide it a little bit, but once you did that, you can like make it write its own skill. So my agent checked me into a flight on British Airways and part of the challenge was to find my passport on Dropbox, get the the ID out and fill it",
        "start": 6184.719,
        "duration": 12063.765999999989
    },
    {
        "text": "guide it a little bit, but once you did that, you can like make it write its own skill. So my agent checked me into a flight on British Airways and part of the challenge was to find my passport on Dropbox, get the the ID out and fill it that, it it it took a little bit and like a little bit of nudging, but now I have a skill and the next time it can do it by itself. And yes, that's a little that's a little wild. So, what's what's your go-to use case these days? What's your what's the one that has been surprisingly valuable to you? I think one of the most convenient things is when I'm out just like sending it a voice memo asking about anything and it can be something that is not difficult but something that is very difficult and it will help me. It's just it's such an unlock like like I can I can I send a picture of an event I'm interested in and it would not only check my calendar but maybe also ping a friend if if they're interested or like try to like move move an event to some other time so I can like be there or it would tell me oh actually this is really bad reviews don't do that you know but like much faster than I would I would be able to do that myself is like googling and like in many ways it it reduced my use of other apps or or even the web because now I just have an assistant that does it much faster and better than I could do myself. So it it kind of like reduces my my my phone use even. Yeah, I find it extremely valuable to be in in places where I'm at when I'm having ideas. When I have an idea and I I don't want to kind of break that flow and I can just say like you know I I still call mine claude for now, but like hey like jump in and and note this somewhere, do some research and then I just continue with my chain of thought my my own chain of thought. , so it's it's like so convenient to have it where you're actually ideulating. , Peter, I I want to thank you so much for joining us. This is I I've been so impressed and I'm especially excited about your ideas for its memory and dreaming and and check out the sleeptime compute paper there. It's really like it kind of feels akin to what you're describing. But thank you for all your work. Please please continue to get some sleep. I'll I'll do my best. I have a little more to do today. And by the way, real real quick, I think I heard you might be coming to San Francisco next week. Yeah. All right. I think I might actually get",
        "start": 6347.6,
        "duration": 12342.08399999998
    },
    {
        "text": "work. Please please continue to get some sleep. I'll I'll do my best. I have a little more to do today. And by the way, real real quick, I think I heard you might be coming to San Francisco next week. Yeah. All right. I think I might actually get That'd be cool. I think we still we're still we try to get a a meetup going. I need to need to figure out some details there, but we want to do the first claw or should we call it clawcon by at the time because there probably so many people. , but yeah, that should that should be happening. Peter, really really happy for you and excited to to see what this turns into. , don't let the the crypto folks get you down here. [laughter] Thanks, Peter. Have a good one. Have a good one. Bye. Bye. Bye. So cool. Yeah, it's it's so interesting the way that he's thinking about it. It does truly feel novel. It feel like allowing it to dream at night. I don't know. Can you imagine Frontier Labs rolling that out? , yeah. Just feels it just feels so cool the way he's thinking about it. He really thinks about it as a as a real entity, something that should should grow by itself, should evolve by itself, and he's just giving it the tools to do so. I love that. It's interesting to me how the reactions from different communities or maybe different different interactions rather really put people over the edge and understanding the value. So he shares it with his friends one-on-one. They immediately get it. You know, he shares it on Twitter. It didn't catch on right away. And I think that goes to show like you need someone to to actually prove in a way that this thing is working because we've been promised these things occasionally, right? But this thing actually does what you want it to do. And I think that that's the main difference here. Yeah. All right. Well, , that's our show for today. I want to thank our guests, Nikita Ruden, CEO and founder of Flexion, Alex Masherb, CEO and founder of Higsfield, and Peter Steinberger, creator of Open Claw. Yes, I got that right. Open Claw. Nick, thank you very much again for co-hosting with me today. Thank you, Matt. Thanks, everybody. Back soon. Yeah. Yep. I'm I'm healing quickly. don't forget to follow us on Twitter, ForwardFuture. Check out our newsletter, forwardfuture.ai. Thank you to Box for sponsoring this episode. We'll see you next week. [music]",
        "start": 6496.48,
        "duration": 12629.863999999985
    }
]