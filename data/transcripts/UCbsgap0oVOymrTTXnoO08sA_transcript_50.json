[
    {
        "text": " Open AAI is scrambling with an internal code red and secretly building a new model called garlic. Apple just dropped a wild new way to shrink entire documents into tiny memory tokens. Microsoft finally solved the annoying delay in AI voices with a real-time TTS model. Alibaba showed off an avatar system that can stream video forever without breaking. [music] And Tencent released a video generator that regular people can actually run at home. All of it happened at once, so let's talk about it. All right, so the whole thing kicked off with a leak inside OpenAI. And honestly, this one [music] caught everyone offguard. According to people familiar with what's happening inside the company, Sam Alman basically walked into the office after Google pushed Gemini 3 to the top of the LM Marina charts [music] and told his team he was declaring a code red. And that phrase isn't used lightly. It means Open AI feels real pressure. It means the competition closed the gap enough for them to treat it like an emergency. And once that internal message came out, the next part of the story almost immediately surfaced. Open AAI has been secretly building a new model called Garlic. Now, from what Mark Chen, the company's chief research officer, said internally, Garlic is performing extremely well in OpenAI's own tests. It's beating Gemini 3 and Anthropics Opus 4.5 in areas like reasoning and coding, which is impressive because those two models basically became the gold standard in those categories over the past few months. So, the fact that OpenAI has something behind the curtain that's outperforming both says a lot about how seriously they're treating the competition right now. But what makes Garlic interesting is why it exists. Apparently, OpenAI went back into their pre-training system, the phase where the model learns from massive amounts of data, and they fixed a bunch of issues that were holding the newer generations back. Instead of forcing the model to squeeze in all the fine grain details from the beginning, they rebuilt the process so it focuses on bigger, broader connections first and then narrows down into the specifics later. It sounds like a small change, but inside OpenAI, it's been described as a major shift. And fixing those early problems suddenly allowed them to pack more knowledge into smaller models. Smaller models matter because they're cheaper to train and [music] faster to run. And when companies like Mistl, Deepseek, and a few labs in China started showing off small models that punch way above their weight class, it clearly pushed OpenAI to respond. The thing is, Garlic is totally separate from another internal project called Shallot Pete, which Sam Alman talked about earlier, that was also supposed to fix pre-training bugs. So, OpenAI is basically running multiple model lines at once, trying to leapfrog itself and avoid falling behind anyone else. The timing for garlic isn't official, but when Mark Chen was asked",
        "start": 2.56,
        "duration": 316.08000000000004
    },
    {
        "text": "project called Shallot Pete, which Sam Alman talked about earlier, that was also supposed to fix pre-training bugs. So, OpenAI is basically running multiple model lines at once, trying to leapfrog itself and avoid falling behind anyone else. The timing for garlic isn't official, but when Mark Chen was asked And considering how aggressively OpenAI is moving right now, the safe bet is early next year. And what's even wilder is that the work they did for Garlic apparently already unlocked progress on the next big model after it. So, whatever this code red triggered inside the company, it clearly set off a chain reaction. The contrast between labs right now is fascinating, too. Anthropic, for example, doesn't feel any of this pressure. Daario Ammoday said at the New York Times Dealbook Summit that they're not competing for the same audience as OpenAI and Google. Anthropic focuses on enterprise customers and their Claude code system already hit a $1 billion revenue run rate only 6 months after launch. When one of your tools alone is bringing in a billion dollar pace, you don't really need a code red. All right, quick pause. You see how we stay on top of AI news every single day and break everything down in a clear, structured way. A big part of that consistency comes from having the right workflow. So, I put together a free guide with 10 of the best prompts to help you with becoming more productive at your job, your business, and you everyday life. You can grab it using the link in the description or by scanning the QR code on the screen. These are the same prompts I use to plan my day, cut through noise, and turn ideas into something usable fast. The AI Power Prompt starter pack is free, it's practical, and it's waiting for you in the description. All right, now back to the video. And while the big labs were busy flexing, Apple quietly dropped one of the most impressive research releases of the entire year, a system called Clara. Now, if you've ever used a chatbot that tries to search through long documents or answer questions that need multiple pieces of information, you already know how messy it gets. Most systems today grab huge chunks of text, jam them into the context window, and hope the model can sort everything out. That method works, but it's slow, expensive, and gets [music] worse as documents get longer. Apple decided to completely rethink that whole process. Clara compresses documents into tiny sets of memory tokens. Basically, super dense summaries that still keep all the important meaning. Then, it uses those tokens for both retrieval and generation. So instead of an AI grabbing thousands of words every time you ask a question, it just pulls a tiny bundle of compressed tokens and works directly inside that shared space. The wild part is that Apple didn't just compress the documents. They trained the retriever",
        "start": 160.56,
        "duration": 587.1999999999999
    },
    {
        "text": "both retrieval and generation. So instead of an AI grabbing thousands of words every time you ask a question, it just pulls a tiny bundle of compressed tokens and works directly inside that shared space. The wild part is that Apple didn't just compress the documents. They trained the retriever from each other. Most rag systems [music] today treat those two parts separately. Apple made them operate as one brain. To build this, they trained on about 2 million Wikipedia passages from 2021. A local Quen 32B model generated simple Q&A, multihop Q&A, and paraphrases for each document. Then a verification loop of up to 10 rounds cleaned everything up until the data was consistent and complete. They used two losses for training. Cross entropy to teach the model how to answer questions [music] using the compressed memory and an MSE loss to make sure the memory tokens and full document tokens stay aligned. Those details may sound nerdy, but they're why Clara performs so well at high compression levels. The numbers are honestly insane. At four times compression, Clara's Compressor hits an average F1 score of 39.86 on benchmark data sets like Natural Questions and Hot Pot QA. That's 5.37 [music] points better than LLM Lingua 2 and more than a point ahead of Pisco. Two of the strongest baselines in the field. Under an Oracle setup where the correct document is guaranteed to be in the list, the model gets 66.76 F1, which beats the other methods by a wide margin. Even more surprising, compressed document representations sometimes outperform full text retrieval pipelines like BGE plus Mistral 7B. You're literally getting better results using drastically shorter inputs. And when Clara runs as a reranker, it hits 96.21 recall at 5 on Hotpot QA, beating supervised retrievers that were trained specifically for relevance. Apple released three versions of Clara: Base, Instruct, and E2E, plus the entire training pipeline. That move alone signals that Apple might be preparing a larger push into the LLM space. And if that happens, things will get very interesting very fast. Then Microsoft came in with something completely different. A real-time voice model called Vibe Voice Realtime 0.5B. Now, if you've ever used an AI assistant that takes a second or two before speaking, you know how awkward that pause feels. Microsoft finally solved that problem. This model can start speaking in about 300 milliseconds. That's basically instant. And the whole system was designed for agents LLMs that talk while they're thinking. The moment the language model starts generating text, Vibe voice jumps in and begins turning those tokens into speech while the LLM continues producing the rest. Hello, it is AI Revolution channel. It uses only an acoustic tokenizer running at 7.5 hertz instead of mixing semantic and acoustic tokens like some larger versions. That tokenizer is based on a sigma VAE system with seven transformer layers and a huge downsampling factor 3,200 times from 24",
        "start": 298.4,
        "duration": 900.0420000000001
    },
    {
        "text": "rest. Hello, it is AI Revolution channel. It uses only an acoustic tokenizer running at 7.5 hertz instead of mixing semantic and acoustic tokens like some larger versions. That tokenizer is based on a sigma VAE system with seven transformer layers and a huge downsampling factor 3,200 times from 24 four-layer diffusion head conditioned on hidden states from a Quen 2.5 to 0.5B model. The whole stack adds up to around 1 billion parameters, which is relatively light compared to some of the giant TTS models out there. Performance on Libre Speech Test Clean shows a 2% word error rate and a speaker similarity score of 0.695. That puts it in the same league as strong models like Valley 2 and Voice Box. And unlike those older systems, this one is optimized for long form speech, meaning it stays stable across entire conversations. The model can generate about 10 minutes of audio inside its 8K context window. And because it doesn't attempt to create music or background noise, it stays super clean for assistant style use cases. Microsoft officially recommends running it as a small micros service next to your LLM. The LLM streams [music] text Vibe voice streams audio and the two stay perfectly synced. And then we get to the drop that caught the entire visual AI community by surprise. Live avatar from Alibaba and several major Chinese universities. This thing feels like the moment animated avatars finally became a real product instead of some clunky research demo. Live Avatar uses a 14 billion parameter diffusion model that can generate video at over 20 frames per second in real time. And when I say real time, I mean you can speak into your mic and the avatar responds instantly with smooth facial motion, gestures, expressions, the whole package. It even supports streaming for over 10,000 seconds without drifting or losing identity. They pulled this off using a mix of techniques. First, they used something called distribution matching distillation to shrink a heavy multi-step video diffusion system into a model that works with only four sampling steps. Then they created timestep forcing pipeline parallelism, a method that spreads the dnoising process across several GPUs, so you get nearlinear speedups. That alone gave them an 84 times improvement over the original baseline. But the real genius of Live Avatar is how it solves the long video decay problem. Most auto reggressive video generators break down over time. Colors shift, faces warp, identities drift, motion becomes weird. Live avatar fixes that with three clever ideas. Rolling row PE, adaptive attention sync, and history corrupt. Rolling row PE keeps positional information stable even as video length increases. Adaptive attention sync replaces the original reference frame with a generated one so the model doesn't drift away from the real distribution. and history corrupt adds controlled noise to the cache during training so the model learns how to recover from small mistakes instead of letting them compound. These three elements together let live avatar",
        "start": 456.16,
        "duration": 1209.962000000001
    },
    {
        "text": "original reference frame with a generated one so the model doesn't drift away from the real distribution. and history corrupt adds controlled noise to the cache during training so the model learns how to recover from small mistakes instead of letting them compound. These three elements together let live avatar streaming video without losing quality which we've never really seen before. And the demo with two AI agents talking to each other using real-time avatars honestly looks like something straight out of a sci-fi movie. And while all of that was happening, Tencent dropped Huan Video 1.5, probably the most accessible, highquality video generator released this year. This model has only 8.3 billion parameters, [music] which is tiny compared to other video systems, but it delivers top tier video quality with smooth motion, strong prompt following, clean text rendering, and stable camera movement. The big thing is how fast it runs. The new step distilled 480p model can generate videos in 8 or 12 steps. And on an RTX 490, that means full generation in around 75 seconds. That's roughly 75% faster than earlier versions. Huan Video uses a DT architecture with a 3D causal VAE that compresses spatial dimensions by 16 times and temporal dimensions by four times. [music] It also has a custom attention system called SSTA, selective and sliding tile attention, which cuts down compute by removing redundant key value blocks over time. All of this adds up to an end toend speed up of almost 1.9 times for 720p video generation compared to flash attention [music] 3. It supports texttovideo and image to video. comes with built-in super resolution up to 1080p and integrates with Comfy UI, diffusers, lightex 2V, Wong GP, and a bunch of caching systems like Deepcache, TCH, and Taylor Cache. Tencent even open sourced the full training pipeline along with the Muon optimizer, which helps huge video models converge faster by stabilizing the update steps. The demos show everything from cinematic shots to physicsaware scenes to high motion action sequences. They ran large-scale evaluations with professional reviewers using the GSB method, good, same, or bad. And Hunuan Video consistently scored on top across both texttovideo and imagetovideo tasks. They even ran full speed benchmarks on 8H800 GPUs, and the model maintained high quality across the full 50step process. So yeah, that's it for today. Thanks for hanging out, and I'll catch you in the next one.",
        "start": 613.92,
        "duration": 1475.962000000001
    }
]