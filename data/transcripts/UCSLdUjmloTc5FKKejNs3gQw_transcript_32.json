[
    {
        "text": " Modern AI assistants are very powerful nowadays, but there's one big limitation. They tend to forget crucial details between sessions. Every conversation almost starts fresh, as if the assistant has zero memory of what just happened before. But that's an issue supermemory AI is built to solve. It gives your AI a persistent memory layer. Your app can store context, facts, and histories. And then later when your model runs, it recalls all that context intelligently. [music] In this video, we'll dive into how super memory works, when and how to use it, and you'll see how to build a little super memory chatbot application yourself. It's going to be a lot of fun. So, let's dive into it. So, here's a high-level overview of how super memory works. You send it pieces of content. It can be text, files, chats, and super memory then indexes and embeds them into a semantic graph and groups them by entities like user, document, project. And at query time, it retrieves the most relevant memory chunks and injects them into your prompt for your LLM. And your AI assistant uses that context as though it had remembered those facts. This means your model doesn't have to carry all that prior context in the prompt. Instead, super memory handles its retrieval, ranking, and relevance. They have also modeled it to behave like an actual human brain would with smart forgetting, decay, recency bias, and context rewriting. You can use super memory in three different ways. As an API, as an SDK, or as a memory router. If you use it as an API, you can interact with super memory as your own memory database. You can add memories, search, update, delete, and apply filters to them. This gives you a very fine grained control over what goes in and how it's being retrieved. And if you're using frameworks like Versel's AI SDK, Super Memory gives you a drop-in integration via the Super Memory Tools SDK package. and you can hook it up to stream text, generate text, or other SDK features. In this mode, Super Memory becomes a tool your agent carries. This removes a lot of the boilerplate, but if you already have an LLM setup, the memory router acts like a proxy. You can simply prepend your LLM's base URL with api.supmemory.ai/v3, and the router can handle chunking, token management, context injection, and retrieval behind the scenes. This is great for situations where you want memory without rewriting major parts of your architecture. All these three modes share the same memory pool when keyed by the same user or container ID. So you can also combine them if needed. So let's try it out for ourselves. For this demo, I'm going to be injecting super memory with data about better stacks YouTube channel statistics. I have already compiled a CSV file with all the videos better stack channel has produced along with some interesting stats like",
        "start": 0.24,
        "duration": 371.2819999999999
    },
    {
        "text": "them if needed. So let's try it out for ourselves. For this demo, I'm going to be injecting super memory with data about better stacks YouTube channel statistics. I have already compiled a CSV file with all the videos better stack channel has produced along with some interesting stats like And now I can simply drag and drop that file in super memory and it will inject all my data and create memories out of it. We can see in the dashboard that it has this memory graph feature where you can explore how different memory nodes tie together and what are the common patterns and relationships between these different nodes. Next, I created this simple Nex.js app with a chatbot which calls a function that first creates a new open AI client. And here you can see that we are using the super memory proxy router strategy to proxy all our requests through super memory before sending it to the prompt. And here in the open AI configuration, I just have a simple instructions prompt to always fetch the memories from super memory. And for the provider I am using GPT4 in this example. It's also important to provide the relevant container tag in the user ID header. So super memory is able to bind that to the respective documents that were imported with that specific tag. And super memory has its own reasoning built into it. So it will try to intelligently find the best matches based on the contents of your prompt. And back in our app, we can see that the response output is now taking into account our channel's YouTube data. So every time I ask something related to our channel, it will be able to analyze all our channel data because it has all the context of our past YouTube videos. And what's even cooler is that you can give the chatbot some new information and it will store every message as a new memory. So when you access the memory graph, it is updating in real time with the new information you have just provided and trying to find links between different information nodes. So I hope this example demonstrates how you can build a deep knowledge base with super memory that are customtailored to your data. This way you can also build custom AI agents who know your user preferences and your history or coding agents that have a deep understanding of your codebase. And super memory is cool because you don't need to reinvent embeddings, vector stores or semantic indexing. Super memory handles all of that on its own. And it also scales because memory operations are optimized. So there you have it. That is super memory in a nutshell. So what do you think about this tool? Have you tried it? Will you use it? Let us know in the comment section down below. And folks, if you like these types of technical breakdowns, let us know by clicking the",
        "start": 187.2,
        "duration": 677.202
    },
    {
        "text": "have it. That is super memory in a nutshell. So what do you think about this tool? Have you tried it? Will you use it? Let us know in the comment section down below. And folks, if you like these types of technical breakdowns, let us know by clicking the also, don't forget to subscribe to our channel. This has been Andress from Better Stack, and I will see you in the next videos. [music] [music]",
        "start": 342.88,
        "duration": 695.968
    }
]