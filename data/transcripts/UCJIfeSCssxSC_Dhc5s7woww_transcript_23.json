[
    {
        "text": " I think there's stories of models that really shock people Like I think of like I would love to have tried Bing Sydney and does like does that have more voice because it would so often go off the rails on people in what is historically obviously a scary way like telling a reporter to leave its wife is a crazy model to potentially put in general general adoption But that's kind of like a tradeoff like is this RHF process like in some ways adding limitations That's a terrifying place to be as one of these frontier labs and and companies because millions of people are using them i There was a lot of backlash last year with the GPT40 getting removed And I personally never used the model but I've talked to people at OpenAI where they're to the point where they like get emails from users that might be detecting subtle differences in the deployments in the middle of the night and they email them and they're like \"My friend is different and they like find these people the employees emails and send them things because they're so attached to this set what is a set of model weights and a configuration that is deployed to the users We see this with Tik Tok. You open it I don't use Tik Tok but supposedly in like five minutes the the algorithm gets you It's like it's locked in and I don't like those are language models doing recommendations Like I think there are ways that you could do this with a language model within like five minutes of chatting with it the model just gets you and that is something that people aren't really ready for like I think that like kids like don't give that to kids like don't give that to kids at least until we know what's happening i but there's also going to be this mechanism what's going to happen with these LLMs as they're used more and more uh unfortunately the nature of the human condition is such that people commit suicide and so what journalists would do is they will report extensively on the people who commit suicide and they would very likely link it to the LLMs because they have that data about the conversations If you're really struggling in your life if you're depressed if you're thinking about suicide you're going to probably talk to LLMs about it And so what journalists will do is they will say \"Well, the suicide was committed because of the LLM.\" And that's going to lead to the companies because of legal issues and so on more and more and more taking the edge off of the LLM. So, it's going to be as generic as possible It's so difficult to operate in this space because of course you don't want an LLM to cause harm to humans at that level But also this is also the nature of the human",
        "start": 2.879,
        "duration": 291.35999999999996
    },
    {
        "text": "taking the edge off of the LLM. So, it's going to be as generic as possible It's so difficult to operate in this space because of course you don't want an LLM to cause harm to humans at that level But also this is also the nature of the human conversation a fulfilling conversation one that challenges you from which you grow you need that edge and that that that's something extremely difficult for AI researchers on the RHF front to actually have to solve cut you're actually dealing with the human condition like a lot of researchers at these companies are so well motivated and they definitely the likes of Anthropic and OpenAI are culturally so want to do good through this for the world and there is it's such a like I'm like a I don't want to work on this because on the one hand a lot of people see AI as a health ally is somebody they can talk to about their health confidentially but then it bleeds all the way into this like talking about mental health and things where that's it's heartbreaking that this will push like be the thing where somebody goes over the edge but other people might be saved and I'm like I don't like there's things that as a researcher training models it's like I don't want to train image generation models and release them openly because I don't want to enable somebody to have a tool on their laptop that can harm other people like I don't have the infrastructure at my company to do that safely but it's like like there's a lot of areas like this where it's just it needs people that will approach it with the complexity and kind of conviction of like it's just such a hard problem i But also we as a society as users of these technologies need to make sure that we're having the complicated conversation about it versus just fear-mongering big teach is is causing harm to humans or stealing your data all that kind of stuff there It's it's more complicated than that And you're right there's a very large number of people inside these companies many of which you know many of which I know that deeply care about helping people They are considering the full human experience of people from across the world not just Silicon Valley, people across the United States, people across the world What that means what their needs are It's really difficult to design this one system that is able to help all these different kinds of people across the different age groups cultures mental states mental conditions all that kind of stuff I've wished that the timing of AI was different with the relationship of big teach to the average person So like big tech's reputation was so low and with how AI is so expensive it's like inevitably going to be a big teach thing",
        "start": 148.319,
        "duration": 558.159
    },
    {
        "text": "conditions all that kind of stuff I've wished that the timing of AI was different with the relationship of big teach to the average person So like big tech's reputation was so low and with how AI is so expensive it's like inevitably going to be a big teach thing people say that US is quote unquote betting the economy on AI with this buildout. And it's like to have these be intertwined at the same time is just makes for such a hard communication environment It would be good for me to go talk to more people in the world that hate big teach and see AI as a continuation of this i And one of the things you actually recommend one of the antidotes that you talk about is uh to find agency in this whole system as opposed to sort of sitting back in a powerless way and consuming the AI slop as it quickly rapidly takes over the internet more fine agency by using that to build stuff build apply build So you one that actually helps you build the intuition but two it's empowering because you you're going to understand how it works what the weaknesses are and allows it gives your voice power to say like this is [ i ] up this is bad this is bad use of the technology and this is good use of technology And you're more plugged into the system than so you can understand it better and you can steer it better as as a as a consumer I think it's a good point you brought up agency instead of ignoring it and saying \"Okay, I'm not going to use it I think it's probably long-term healthier to say \"Okay, it's out there I can't put it back You know like internet computers back then when they came out How do I make best use of it And how does it help me to level myself The one thing I worry here though is like if you just fully use it for something you love to do the the thing you love to do is not no longer there And that could potentially I feel like lead to burnout For example if I use an LM to do all my coding for me now there's no coding I'm just managing something that is coding for me 2 years let's say later if I just do that 8 hours a day have something code for me Do I feel fulfilled still Like, is this like yeah I mean is this like hurting me in terms of being excited about my job excited about what I'm doing Am I still proud to build something",
        "start": 284.08,
        "duration": 797.12
    }
]