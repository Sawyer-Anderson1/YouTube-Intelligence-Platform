[
    {
        "text": " we have had you know now we have Gemini 2.5 outside we are working on Gemini 3.0 which we will release this year and and the progress has been extraordinary and I think the progress in 26 is going to be even more exciting than 25 so I couldn't be more excited and so that was Sundar Pachai talking about the release of Gemini 3.0 Oh, as most of you know, Gemini is currently on 2.5. It's currently state-ofthe-art in many areas. I know most people are using this model and they are pleasantly surprised. And Google's AI efforts have been continually ramping up with all sorts of different releases across an entire suite of products from Nano Banana taking the industry by storm to VO3.1 doing everything that it does. And now we are speculating on when will Gemini 3.0 be released. So one of the first things I do want to talk about when it comes to Gemini 3 is of course the benchmarks. Now some of these are going to be unofficial benchmarks because if you haven't been paying attention or you may not have noticed in the large language model space there is something called LM arena and in this arena there are early models that are constantly put there. So Google or OpenAI may actually decide to release an LLM early so that they can get feedback from users to see how it compares against other users so that they can, you know, fine-tune, tweak it a bit so that when it is actually released, it's the one that most people prefer. And what we can see here is remarkable. go in the benchmarks. And this benchmark, I didn't actually know that this benchmark even existed prior to making this video, but this is called the hieroglyph benchmark. And this is a newly introduced evaluation framework that was introduced in August of 2025, designed to measure lateral reasoning ability in AI models, specifically their skill in finding non-obvious or creative connections between seemingly unrelated ideas. Now, if we look at this benchmark, it does seem that Gemini 3.0 O Pro, which by the way, some people are calling this model, I think it's lithium flow, by the way. So, if you go on Twitter and you search for lithium, you're going to find a variety of different benchmarks and just a bunch of different things that I'm going to cover by the way in this video. So, this is the benchmark that, you know, focuses on how, you know, well, a model can connect abstract clues, analogies, or hidden relationships that require intuitive leaps rather than straightforward logical inference. And this differs actually from most straightforward benchmarks that test linear reasoning or factual recall. And you know therefore alongside other benchmarks like brain teaser or big bench hard it's specifically aimed at out of the box thought processes similar to what humans do in lateral thinking puzzles. So where",
        "start": 0.32,
        "duration": 325.52000000000004
    },
    {
        "text": "And this differs actually from most straightforward benchmarks that test linear reasoning or factual recall. And you know therefore alongside other benchmarks like brain teaser or big bench hard it's specifically aimed at out of the box thought processes similar to what humans do in lateral thinking puzzles. So where see but we can see that Gemini 3.0 Pro is here just under GPT 5i. Now, that's not a bad score, but it does show us that compared to Gemini 2.5 Pro thinking, that is a significant jump in terms of the reasoning capabilities. Now, remember that's just on this benchmark, but we can see here that Gemini 3.0 Pro would essentially if this, you know, lithium flow model is, you know, essentially almost doubling the reasoning efforts of a standard model. And I'm guessing that this is different because this isn't like a thinking model at the moment. I'm sure they'll have a thinking model for of course one that has chains of thought. But comparing this model to you know the other models which you can see here the only thing surprising is that we don't have crocthor but of course we can see that GPT5 high is you know a really really compute intensive model that reasons for a really long time. So Gemini 3.0 pro being the base model that is particularly surprising. Now, if you want to take a look at another benchmark, we have the Kingbench leaderboard. And Kingbench AI is a relatively new AI large language model benchmark. And this was introduced in mid2025 to measure real world AI reasoning, you know, coding and world modeling performance across open and closed weight models. Unlike earlier benchmarks such as MMLU or ARC, Kingbch aims to simulate the dynamic cognitive reasoning situations rather than a static QA data set providing a broader range you know of tests for an LLM's adaptability, robustness and bias resilience. And you know it features you know a few dimensions but what we can see here is that all of the models on you know the Gemini standard we can see Gemini 3, Gemini 3.0 Pro and Gemini 3 which is I think it's thinking. What we can see here is that this model manages to top the board here and Kingbench you know evaluates the models using five main dimensions core reasoning logical task solving capabilities you know factual grounding the ability to handle up-to-ate complex knowledge and respond accurately across domains and coding and problem solving. So, this is super super surprising because not only is it number one on the leaderboard, but surprisingly seems to be outperforming Sonet 4.5. Now, you might think that that's not that crazy, but you have to understand that this kind of thing is incredible because Sonet 4.5 and Sonet 4.5 Max, you know, this is a standard model. This is the thinking model. And Sonet is essentially the number one software engineer in the world. So when we take a",
        "start": 164.4,
        "duration": 624.561
    },
    {
        "text": "that crazy, but you have to understand that this kind of thing is incredible because Sonet 4.5 and Sonet 4.5 Max, you know, this is a standard model. This is the thinking model. And Sonet is essentially the number one software engineer in the world. So when we take a Anthropic's neck in the AI race that is going to be a huge huge shakeup because we all know that Google tend to offer their products at a huge discount because they have so much compute they have so much money and they're able to do things that other companies like Anthropic who are essentially you know it's kind of like still like an AI startup that you know are they I wouldn't say they're strapped for cash but I would say that they can't be as costefficient as others. Now, if we look at the SVG benchmarks, we can see here from one user, CAN064, that Gemini 3 seems to just consistently outperform everything else. I've seen tons and tons and tons of these SVG benchmarks where we can see that lithium flow, like I said before, or Gemini 3 just manages to outperform others in a way that we just simply haven't seen before. Now, I'm guessing that this shows us that, you know, the model has just a real understanding of visual reasoning and other forms of reasoning where GPT5, we can see here, you know, the prompt, if you guys didn't know, was actually an Xbox controller. So, this is super super interesting. GPT5 versus Lithium Flow, we can see that those models, you know, are super super different there. So, super super interesting on this benchmark. Now, the next one, this one is super super important. Now, at first glance, this looks like something simple, okay? Because it's just a clock and the clock reads 2 6. But the point here is that vision models are essentially blind. And I'm going to, you know, show you guys a paper and this is a bit weird and a bit nerdy that I even remember that this paper exists. But there was a paper that I was fascinated with and I'm going to show you guys right now. And so this paper was called vision language models are blind. Now, I remember reading this paper for the first time and it just blew my mind because and they haven't updated it, but at the time it was still very surprising. So, these models that they were using weren't that like it was Gemini 1.5 Pro, Sonet 3.5, GPT40. And you might say to me, well, those models are old. There's no way that you know they would fail those tests now. But you have to understand that like it's a different kind of reasoning that I guess is somewhat intuitive for humans but really hard for AI systems. And I want to show you guys exactly why that is and",
        "start": 316.96,
        "duration": 881.2000000000007
    },
    {
        "text": "no way that you know they would fail those tests now. But you have to understand that like it's a different kind of reasoning that I guess is somewhat intuitive for humans but really hard for AI systems. And I want to show you guys exactly why that is and here, if I just zoom in a bit, we have four different intersections of lines. And I'm going to relate this back to the clock point in just a second. Just bear with me. But we can see here that like if you asked yourself, do these lines have any intersections? Meaning, do they touch? It's easy to say no. Do this does this line have any intersection? That has one, this has one, and this has two. But you would be surprised to find out that for AI systems, they actually don't get this right that often. We can see right here that these models, even these basic models, such a basic question, you could ask them how many times do they intersect? And you can see that the models really really struggled with this. Essentially showing you guys that like when you have these models like trying to do visual reasoning tasks, text reasoning was really good, but visual reasoning was really, really, really bad. Now, I did run a few tests on Google Gemini, and it does seem to be getting some of these questions correct. And I am guessing that you know maybe it might not be in the training data. Maybe it might be you know that's kind of a question of course I'd need a broader range of things to test it on. However the point is is that previously LLMs couldn't really tell the time and it was this thing that was you know floating around on Twitter because you know a lot of people like AJI's just around a corner but these vision models can't even test the time. And most people have put this clock into separate different models. And we finally have a model that is super super accurate when it comes to telling the time based on the clock in the image. So here we can see that Google Gemini fails and says that this is 12:30. You can also see Claude Sonet here says that this is, you know, 12:30 as well. They actually do fail. And yeah, after a bit more reasoning, just telling it 6:30, bro, it says looking at it again, you're right. It is 630. And this is something that the lithium flow, which is Google's model, was able to get correctly. Now most people won't realize this because this is not something that most people are doing but you have to understand that every time vision improves and you know we get more use cases like you know let's say like you know vision was at this level and now it's at this level which is a bit higher",
        "start": 448.319,
        "duration": 1117.599000000001
    },
    {
        "text": "is not something that most people are doing but you have to understand that every time vision improves and you know we get more use cases like you know let's say like you know vision was at this level and now it's at this level which is a bit higher jump from like 70 to like 80% you get a lot more use cases and thus a lot more very interesting products. So this is the kind of thing I don't know like most people wouldn't have cared about this but for me this one was super super fascinating because vision models inherently the way they're designed is just super super tricky. Now if we're talking about you know other things as well for example like you know potentially the other capabilities I can talk about the fact that it managed to you know oneshot this website. This one right here was really really cool. This is something called Apex Alpha. I'm guessing this is like a virtual like website that doesn't really exist. But this doesn't look like your traditional, you know, clawed lovable basic website that looks super super simple. And the point here is that like what we do have is we have a situation on our hands where Gemini 3.0 Pro and this is something I want you guys to understand might be the best coding model to exist. And you know, you can look at the other parts of this website as well. This design looks super super amazing, super super incredible. And there was also some other coding abilities of the model and it managed to code this. And honestly, I apologize for my poor gameplay here, but this is was essentially a version of Geometry Dash, which is a pretty hard game. And honestly, I'm I'm terrible at this. So, you can't take my gameplay for how good the game is. But basically, it was able to code this in one shot, just like thousands of lines of code, and it was able to code this pretty effectively. I'll I'll be sharing everything with you know in the description on you know the games I played like there's community sharing all the prompts and stuff and so it's it's super interesting because this is pretty much confirmed. We're seeing from testing catalog that you know Gemini 3.0 Pro we're going to upgrade you like the lines already there and it's important to know that okay we've spoken about capabilities and stuff like that. When is the potential release date for Gemini 3.0? Most people want to know when is this model going to be coming out. Now, some information on Poly Market says that there is a 62% chance that it is going to be coming out by the end of October 31st. I'm not sure if I believe that that much because historically Google have released their models towards the end of the year. And I do believe that that's probably what",
        "start": 568.56,
        "duration": 1377.1210000000003
    },
    {
        "text": "that there is a 62% chance that it is going to be coming out by the end of October 31st. I'm not sure if I believe that that much because historically Google have released their models towards the end of the year. And I do believe that that's probably what see any reason for them to push this out, especially considered they just pushed an update to VR 3.1. Maybe they will. I'm not entirely sure. But I do know that these poly market predictions do hold a decent amount of weight for you know different kind of worldwide predictions and they usually are pretty accurate. But just me personally I don't see any need to do that unless Google feels like they're falling behind. Of course if there are certain things that happen in the AI space maybe they may release it earlier but I'm not expecting it but if they do release it then I certainly will be there. Now of course this is why I say that you know Google are probably going to release the best coding model. So Amar Resi, this guy, I think he's the lead of Google AI Studio. And someone said, \"Yeah, you can't really vibe code with Gemini 2.5 Pro right now when all the others are far superior at coding. Grock 4 even solves issues 4.1 Opus couldn't solve for me. We definitely need something like a Gemini 3 Pro for vibe coding if they're serious about it.\" And then he says everything sets up for the next thing. So it's quite you know indicative of the fact that they're really trying to solve this coding area for Gemini 3 and Semi analysis which is a specialized research and consulting firm known for its deep technical analysis of the semiconductor and AI industries founded by D Patel they tweeted out something super interesting they said because Google is so bad at tweeting we'll do it for them Gemini 3 is shaping up to be an incredibly performant model especially at coding and multimodal capab abilities. So this essentially means that like coding is going to be one of those areas where we're likely to see a big jump potentially even far surpassing what we currently do have from other AI systems. And additional to that we also do have multimodal capabilities which I've just spoken about which is what I personally do believe will you know happen once we do get that situation on our hands when the model is finally released and we're able to test those vision capabilities. So for me personally I do believe that Gemini 3.0 O once it's here, it's likely going to be a model that is great at coding and multimodal capabilities because realistically the industry isn't focusing that much there and Google can once again make some ground. And this is once again another clip of where Sunda Pachai and once again we can look at where Sunda Pachai references Gemini 3.0",
        "start": 700.56,
        "duration": 1654.6399999999996
    },
    {
        "text": "model that is great at coding and multimodal capabilities because realistically the industry isn't focusing that much there and Google can once again make some ground. And this is once again another clip of where Sunda Pachai and once again we can look at where Sunda Pachai references Gemini 3.0 future model capabilities but it does seem like they're targeting it for the end of this year. the agentic enterprise. I I look at the model trajectory ahead you know as we are working with Gemini 3.0 In 26, we're going to make a you know, we've already had dramatic progress over the past couple of years. The progress ahead is palpably going to be you're going to feel that the these models are going to be really intelligent agents.",
        "start": 840.959,
        "duration": 1704.3209999999997
    }
]