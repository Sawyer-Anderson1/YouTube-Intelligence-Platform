[
    {
        "text": " You can now make things disappear. Summon a tsunami and swap characters with pink Superman. China just dropped a magic wand. Clingo one, a unified video model where you can throw text, images, video clips, all into one prompt, and it just handles everything. Smooth transformations between two images, character replacements, environmental changes, all this just by typing what you want. I tested it with nine increasingly difficult use cases, and the output of the last one shocked me. This isn't just another AI video tool. This could be what changes everything. The crazy part, it's on a 7-day free trial right now on Higsfield. Thank you, Higsfield, for sponsoring this video. More on that later. But before we dive in, I share updates like these daily on my free WhatsApp community. Links in the description. Now, let's get into it. So, very first thing, let's explore Cling 01, the new model. To access it, you just go into video and choose Cling O1 video. And this is where we can do the first frame to end frame smooth transformation magic. So, let me show you how it works. What I'm going to do is upload a start frame. This is the old Instagram logo and an end frame. That's the new Instagram logo. And for the prompt, I'm putting old logo appears as a hologram glitching with digital noise. It pixel warps, scans, and reconfigures into the new logo with neon light trails and subtle distortion effect. So, that's the prompt. Now, we just hit generate. And this this is the sort of output we get. So, say you're a marketer launching a rebrand. You have your old logo and your new logo. Just upload both to Cling01 and it creates this morphing video where the old logo transforms into the new one. Smooth, professional, no animation skills needed. Let's try the same thing with another example. So, I have a start frame of an old iPhone and an end frame of a new iPhone 17. Opening both of those and I'm giving the prompt create a smooth disassembling to assembling transformation between the old iPhone and new iPhone. Same settings, hit generate. And this is the output we get. Pretty cool, right? One more example. I got these two start and end frame images from Nano Banana. Did the same thing. And look at this output. The car is seamlessly disassembled into different parts. So those are some transformation examples. Two images in, transformation video out. That's smooth transform. Now, this is where elements come in. And this is probably one of the most useful features if you're doing any kind of marketing content. To create an element, you just click on add element, then create element. Here you can upload your front image, add different angles if you have them. Give it a name and some description. So I've already generated my element here. You can see I added my basic details and saved it. And",
        "start": 0.16,
        "duration": 314.2410000000001
    },
    {
        "text": "element, you just click on add element, then create element. Here you can upload your front image, add different angles if you have them. Give it a name and some description. So I've already generated my element here. You can see I added my basic details and saved it. And the same time with the element, I'm going to give it a simple prompt. I'm going to ask it to hold an iPhone. Make this element hold an iPhone and promote it. So here's the thing. If you're in marketing and you have a model ready, that model could be AI generated using Nano Banana Pro, you create that model as an element and then just put whatever product you have. We're choosing iPhone here. Hit create and you can see the output is so seamless like that's really good. Let's try something more complex. So instead of iPhone this time, I'm choosing a car and I'm giving a prompt of make drive a car. This is the sort of output we get. Now, as you can see, because it's a very complex video, but multiple layers, movements embedded throughout, we do see a little inconsistency in the facial, but honestly, the output is still very impressive. I mean, in Sora, you have to create a cameo, , give consent and all that. Here, you just create an element and that's it. Way simpler. Now, quick thing. This video is sponsored by Higsfield, which is actually what I've been using for all these generations you just saw. Basically, Higsfield is where I go to generate AI videos. The reason I use it is because you get all the best models in one place. Clingo 1, Hyuo, Google's VO3.1, OpenAI's, Sora, Cance, etc. And they have like 50 plus camera movements, built-in dolly shots, crane shots, all that cinematic stuff, which is super useful when you're trying to get a specific look. And right now, they have a crazy deal going on. Nano Banana Pro with unlimited flux access for a full year, 70% off. They're calling it the biggest price drop in Genai history, but it ends December 5th. So yeah, it's basically last chance if you want it. If you want to check it out, link is in the description. All right, let's keep going. Now, let's try another exciting feature, face swapping. So here I have this stock footage of an athlete. He's doing some stretching exercises in a park. You can see the pine trees in the background. He's wearing this black tank top. Very focused expression. Now, what I want to do is I want to keep everything the same. the movements, the lighting, the background, all of that. But just swap out his face with someone else. In this case, I'm swapping with myself. So, the prompt is super simple. Change the person face in video one with and then I'm referencing my element, which is my photo. That's it. Hit",
        "start": 159.04,
        "duration": 579.0420000000003
    },
    {
        "text": "the lighting, the background, all of that. But just swap out his face with someone else. In this case, I'm swapping with myself. So, the prompt is super simple. Change the person face in video one with and then I'm referencing my element, which is my photo. That's it. Hit mean, look at it. The expression, the lighting, the surrounding, everything is exactly the same. But now it's me doing the stretching instead. And watch this. When I play it, he does the exact same motion, clasping his hands together, pushing them outward. The whole stretching movement, the new face moves naturally with it. And the thing is, there's no glitching, no weird pixels on the face, it stays stable throughout the whole movement. Even when his hands fully extend at the peak of the stretch, face stays solid. So this is videotovideo face swapping. You take existing footage, swap in a different person, and all the motion, lighting, everything transfers over. Pretty powerful stuff. Now let's see if Clingo1 can pull off something simple. So we have uploaded a video here and in this video we don't want the ship. So what do we do? We just prompt remove the ship. That's it. Keep the settings as they are 16x9 ratio and hit generate. This is textbased editing. I'm just removing something from the video by telling it what to remove. And here's the output. What we did is we removed the ship from the right part of the frame. And yeah, it's just gone. The AI filled in what should be there. Think about this. No masking, no rotoscoping, no frame by frame editing. You just say remove the ship and it handles everything. That's insane for anyone doing video work. Now, the next step after removing an element is in painting, aka making sure everything looks seamless after the removal. Let's test it. So, first we pick a video. And here we can add up to four images or elements. So, we'll add four images and see what output we get. I'm adding a lamp, a MacBook, a book and some pens. I've added all the images and the prompt is add the images to the video. So there's four images and one video. Let's generate. So we have received the output and this is the output. As you can see, it has smartly placed the lamp we gave. Then in the pen holder, they have kept the pen and they have also placed the book on the right side. Now I had also added a laptop. But it might not have understood that it's a laptop because the image wasn't clear. So it didn't place that one. So, this was an example of in painting. Basically, adding content into a video using reference images. Three out of four worked. Not bad. Now, let's do something even crazier. Let's change the entire atmosphere of a video. So, what we're going to do now is bring a",
        "start": 292.8,
        "duration": 849.2010000000004
    },
    {
        "text": "that one. So, this was an example of in painting. Basically, adding content into a video using reference images. Three out of four worked. Not bad. Now, let's do something even crazier. Let's change the entire atmosphere of a video. So, what we're going to do now is bring a We're going to take this scene and completely change what's happening in the environment. Same process. Upload your video. Describe what you want. In this case, we want a tsunami in the background. The AI doesn't just paste something on top. It actually rebuilds the scene. And look at that output. The lighting changes, the mood shifts, everything feels like it belongs together. You're not just adding an effect. You're changing the entire world of the video. This is what they mean by unified multimodal. You're not switching between 10 different tools. Upload, prompt, generate. That's it. Amazing. But now, let's try something different. So, I generated this Pink Superman video earlier. Like a character in pink Superman clothes standing at a dock leaning on a barrel. Let me show you what we got. Not bad, right? But now I want to put myself into this. Keep the pink clothes but swap the face with me. To do that, we need to create an element first. So here you have an option to add element. Click on create new element. Here you add the frontal image and then you can add multiple different angles like three different side angles. Fill in the name and some description. So I've already created one element. You can see here I added one front photo and three different side angle photos of myself. Added a little description. Saved it. element is ready. Now I select my element. There it is. And the prompt is replace the pink Superman with the element while keeping the clothes of pink Superman. Prompt is ready. Let's hit generate. Okay, here's the output. Let's play it. All right, so the output is a bit plastic looking. I'm not going to lie. And the lip sync is completely messed up, but it still kept the concept of the pink clothes. That's the important part. So yeah, pink Superman with my face. It's not perfect, but it shows you what's possible. You generate a character, then swap yourself in and keep the original costume. Pretty wild. All right, so let's try one more thing. I have this reference video here. It's just an empty metro train. And I'm placing my element again. So we got the metro video. We got the element and the prompt is place vib in the metro. Sitting on the metro chair. That's it. Look at this. The lighting on the poles, the reflections, the orange seats, and I'm just sitting there like I was actually filmed in that metro. So here's the thing. You just grab some images like take nano banana's help to create multiple angles, put them here, create",
        "start": 430.08,
        "duration": 1103.363
    },
    {
        "text": "That's it. Look at this. The lighting on the poles, the reflections, the orange seats, and I'm just sitting there like I was actually filmed in that metro. So here's the thing. You just grab some images like take nano banana's help to create multiple angles, put them here, create place yourself in any video, any scene, any environment, any location. It's crazy what this thing can do. Okay, now let's try another amazing feature. So, I have this stock video here. It's a model walking through a field, white coat, sunset vibes. But I want to change the angle like I want to watch the same video but with a top angle. So, the prompt is simple. Generate a top angle in video one. That's it. Let's hit generate. And okay, look at this. Same scene, same model, same sunset, but now it's like a drone shot. We're looking straight down at her walking. So, you have a video shot from ground level. And you can just ask for a different angle. Top angle, wide angle, close-up, whatever you want. No re-shoots, just type it. Think of how much production budget this would save. This feature is truly a lifesaver. All right. let's try something fun. What if we could take a normal video and change its entire art style like turn it into anime or cartoon? So, I'm uploading a video here and then I'm going to add an image as a style reference. The prompt is simple. Change the art style of video one to image one. That's it. Let's generate. This is the output we got from taking the art style of Naruto. Look at this. Spiky hair. The whole anime vibe. Same movements, same gestures, but now it looks like an anime character. Now, let's try another one. Let's take the art style of Jiujutsu Kaizen. So I'm grabbing a reference image, uploading it. Same prompt and let's generate. Okay, so this is the output. Different anime aesthetic. Now it picks up the style from whatever reference image you give it. And one more. Let's go full cartoon like 2D tune style, flat colors, cartoon look. And there we have it. The same video in a new style. You can literally take any video and turn it into any art style you want. Anime, cartoon, whatever. Just give it a reference image. Now, what's missing? Let's talk about the elephant in the room. Audio. Cling 01 does not generate audio yet. That's the word they used yet. So, it's coming. But right now, you're getting silent video. Vo 3.1 and Sora 2 both have native audio, voice, sound effects, music generated with the video. That's a real advantage for one-click content. But Clingo1's advantage is everything we just covered. All those editing and transformation capabilities, VO3.1 and Sora 2 don't have them. It's a trade-off for now. Speaking of Sora 2, let's do a quick comparison since everyone's asking. Sora 2 has native",
        "start": 558.64,
        "duration": 1389.6840000000004
    },
    {
        "text": "video. That's a real advantage for one-click content. But Clingo1's advantage is everything we just covered. All those editing and transformation capabilities, VO3.1 and Sora 2 don't have them. It's a trade-off for now. Speaking of Sora 2, let's do a quick comparison since everyone's asking. Sora 2 has native insert yourself into videos, and it does up to 25 second clips versus Cling's 10 seconds. But Cling 01 has better character consistency. Early testers say it holds faces better than Sora's Cameo, and it can edit existing videos, not just generate new ones. Sora 2 can't do that. Price: Sora 2 needs ChatGpt Pro at $200 per month. Cling 01 Pro is $26 per month, almost eight times cheaper, and Cling is available globally right now. Sora 2 is invite only in the US. Which use case are you trying first? Drop it in the comments. I want to see what you create. If this helped you, hit subscribe. I break down every major AI tool so you know what's actually worth your time. I've also created video comparing top five AI tools and compared models like Chad GPT, Gemini 3 Pro, and Claude Neck to neck. Click here to watch",
        "start": 719.44,
        "duration": 1496.0820000000008
    }
]