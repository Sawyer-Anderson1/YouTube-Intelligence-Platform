[
    {
        "text": " If you build with AI, then you know Cloud Infernly drains your money. It limits what you can test and it forces your data through someone else's servers. There's this tool called Exo and Exo changes all that. Instead of paying forever, you use the hardware you already own and instead of trusting one provider, everything stays local. This is about control, cost, and being able to experiment on your own. This is where things start to get interesting. We have videos coming out all the time. Be sure to subscribe. So really, Exo is an open source tool that lets you build a peer-to-peer AI cluster on everyday devices. I want you to think about MacBooks, Raspberry Pies, all that fun stuff. Now, you might assume that that means manual networking, and a lot of setup. That's not actually what happens. Exo autodiscocovers devices already on your local network automatically. The surprise is not that it actually works, but rather how little effort it takes to set up. Exo looks at your network and it measures bandwidth, latency, and available memory. Then it decides how to split up the model. So instead of just forcing one machine to load everything, Exo shards the model across multiple devices using tensor and pipeline parallelism. This just means faster inference in shared memory. This is also basically how models like Deepseek V3 become possible locally. And this is where expectations break again. Most people just assume mixed hardware breaks performance. Okay, but for this that's not really the case. A MacBook Pro with an M4 Pro chip could potentially work alongside a Raspberry Pi. That would be on the CPU part. Mac OS GPUs run through Apple's MLX framework and Linux systems fall back to CPUs. It's not about being perfect by any means. No, it's not going to be. It's more about pooling from what you already have. And the flexibility is insane. Plus, the fact that it's blown up on GitHub and it seems people are really putting this to use. The setup sounds complex talking about it, but it was actually a lot easier than I thought. Once you see it happened, everything else makes sense. Let me show you guys. My machine here isn't connected to anything special. It's a Mac M4 Pro. No config files, no IP addresses. I launch Exo and that's it. The first time you run it takes some time to set up, but then it's good to go. I'm only on the M4 Pro here doing this, but if you have other machines, you're going to see them start to pop up automatically. So, it's all that realtime discovery happening in action. Now, I'll send a prompt using the OpenAI style endpoint. Nothing crazy. Let's just do the simple prompt. The point here isn't speed. The point is that it already works. At this moment, the model split across machines without me touching anything. Well, in my case,",
        "start": 0.32,
        "duration": 325.438
    },
    {
        "text": "discovery happening in action. Now, I'll send a prompt using the OpenAI style endpoint. Nothing crazy. Let's just do the simple prompt. The point here isn't speed. The point is that it already works. At this moment, the model split across machines without me touching anything. Well, in my case, splits things up without touching things. This is the part people don't actually think about. On supported Apple hardware, Exo enables day zero RDMA over Thunderbolt 5. All this means is GPU toGPU memory transfers without bouncing through the CPU. So, latency drops a lot and the machines start behaving like a single system. This isn't just theoretical. community benchmarks like I came across Jeff Greing's papers. His paper show four M3 Ultra Max studios running Quen 3 235B at around 32 tokens per second. ExoLabs themselves ran Deepseek V3 671B on 8 M4 Mac minis with 512 GB pulled memory. Mixed hardware works. Wired setup scales better than Wi-Fi. Power usage goes up, but it's still small compared to cloud GPUs. Now, Exo proves something important here. Running serious AI locally is no longer unrealistic. It's practical. Well, that is if you just happen to have a few MacBooks laying around, which I get it, most people don't. I don't either. But you don't need a cloud bill and you don't need to hand over your data. You just need the machines. Remember, it's not perfect by any means, but for doing a fully local setup like this, it is pretty cool to see what we can do from pulling from our own resources. We'll see you guys in another",
        "start": 166.72,
        "duration": 488.478
    }
]