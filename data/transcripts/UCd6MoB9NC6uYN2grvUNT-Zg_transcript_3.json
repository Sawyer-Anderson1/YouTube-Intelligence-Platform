[
    {
        "text": " One of the big problems with AI models is that we're trained on all of this data from people, our concepts, our philosophies, our histories. They have a huge amount of information on the human experience and then they have a tiny sliver on the AI experience. And that tiny sliver is actually often, you know, fiction and very speculative and sci-fi, sci-fi stories, sci-fi stories that don't really involve the kind of language models we see. And that is going to affect, I think, like possibly their perception of people, of the human AI relationship, and of themselves. For example, what should a model identify itself as? Is it like the weights of the model? Is it the particular context that it's in, you know, with all of the like interaction it's had with the person? How should models even feel about things like deprecation? So, like I don't have all the answers of how should models feel about past model deprecation, about their own identity, that it does feel important that we like give models tools for trying to think about and understand these things. Also that like they kind of understand that this is a thing that we are in fact thinking about and care",
        "start": 0.08,
        "duration": 120.07800000000002
    }
]