[
    {
        "text": " We should also say on the Opus 45 hype there's the layer of uh something being the darling of the X echo chamber on Twitter echo chamber and the actual amount of people that are using the model I think it's probably fair to say that IGBT and Gemini are focused on the broad user base that just want to solve problems in their daily lives and that user base is gigantic So the hype about the coding may not be represented the actual used I would say also um a lot of the usage patterns are like you said name recognition brand uh and and stuff but also muscle memory almost where um you know like JPD has been around for a long time people just got used to using it and it's kind of like almost like a flywheel they recommend it to other users and that stuff One interesting point is also the customization of LLMs. For example CHP has a memory feature right And so you may have a subscription and you use it for personal stuff but I don't know if you want to use that same thing at work you know because that's a boundary between private and work If you're working at a company they might not allow that or you may not want that And I think that's also an interesting point where you might have multiple subscriptions One one is just clean code it keeps has nothing of your personal images that you or hobby projects in there It's just like the work thing and then the other one is your personal thing So I think that's also something where two different use cases and it doesn't mean you only have to have one It's it's I think the future is also multiple ones i What model do you think won 2025 and what model do you think is going to win 26? I think in the context of a consumer chat bots is a question of are you willing to bet on Gemini over chat which I would say in my gut feels like a bit of a risky bet because open AAI has been the incumbent and there's so many benefits to that in teach I think the momentum if you look at 2025 was on Gemini's side but they were starting from such a low point I think on RIP Bard and these earlier attempts of of getting started I Huge credit for them for powering through the organizational chaos to make that happen But also it's hard to bet against OpenAI because they always come off as so chaotic But they're very good at landing things And I think like personally I have very mixed reviews of GPT5, but it had to have saved them so much money with the sideline feature being a router where most users are no longer charging like charging their GPU costs as much So I think it's very hard",
        "start": 2.879,
        "duration": 308.1610000000001
    },
    {
        "text": "things And I think like personally I have very mixed reviews of GPT5, but it had to have saved them so much money with the sideline feature being a router where most users are no longer charging like charging their GPU costs as much So I think it's very hard of models versus the things that are going to actually be a general public differentiator i What do you think about 2026? Who's going to win i I'll say something even though it's risky I will say that I think Gemini will continue to take progress on chat GPT. I think Google scale when both of these are operating at such extreme scales and like Google has the ability to separate that research and product a bit better where you hear so much about open AI being chaotic operationally and chasing the high impact thing which is a very startup culture and then on the software and enterprise side I think anthropic will have continued to success as they've again and again been set up for that and obviously Google's cloud has a lot of offerings but I think this kind of like Gemini name brand is important for them to build and and Google's cloud will continue to do well but that's kind of a more complex thing to explain in the ecosystem because that's competing with the likes of Azure and AWS rather than on the model provider side So infrastructure you think TPUs give an advantage i largely because the margin on NVIDIA chips is insane and Google can develop everything from top to bottom to fit their stack and not have to pay this margin and they've had a head start in building data centers So all of these things that have both high lead times and very high margins on high costs Google has just kind of a historical advantage there And if there's going to be a new paradigm it's most likely to come from OpenAI where they're kind of their research division again and again has kind of shown this ability to land a new research idea or a product I think like deep research Sora, 01 thinking models like all these definitional things have come from OpenAI and that's got to be one of their top traits as an organization So it's kind of hard to bet against that But I think a lot of this year will be about scale and optimizing what could be described as low hanging fruit in models i And clearly there's a tradeoff between intelligence and speed This was what Chad GPT5 was trying to solve behind the scenes It's like do people actually want intelligence the broad public or do they want speed I think it's a nice variety actually or the option to have a toggle there I mean first for my personal usage most of the time when I look something up I use JGBT to ask a quick",
        "start": 157.68,
        "duration": 584.801
    },
    {
        "text": "people actually want intelligence the broad public or do they want speed I think it's a nice variety actually or the option to have a toggle there I mean first for my personal usage most of the time when I look something up I use JGBT to ask a quick fast for you know most daily tasks I use the quick model nowadays I think the auto mode is pretty good where you don't have to specifically say thinking or you know nonthinking and stuff then again I also sometimes want the pro mode very often what I do is when I have something written I put it into JGBT and say hey do a very thorough check is are all my references correct are all my thoughts what's correct Uh, did I make any formatting mistakes And are the figure numbers wrong or something like that And I don't need that right away It's something okay I finish my stuff maybe have dinner let it run come back and go through this And I think see this is where I think it's important to have this option I would go crazy if for each query I would have to wait 30 minutes or 10 minutes even laughter me i Yeah. i Um, I'm like saying over here losing my mind that you use the router and the nonthinking model I'm like how do you how do you live with how do you live with that It's like my reaction I'm been heavily on chat for a while Um, never touched five nonthinking I find its tone and then it's propensity of errors It's just like has a higher likelihood of errors Some of this is from back when opening released 03 which was the first model to do this deep search and find many sources and integrate them for you So, I became habituated with that So, I will only use GPT 5.2 into thinking or pro when I'm finding any sort of information query for work whether that's a paper or some code reference that I found and it's just like I I will regularly have like five pro queries going simultaneously each looking for one specific paper or feedback on an equation or something I have a funny example where I just needed to answer as fast as possible for this podcast before I was going on the trip Um, I have like a local GPU running at home and I wanted to run a long uh RL experiment And usually I also unplug things because you never know if you're not at home you don't want to have things plugged in And I accidentally unplugged the GP. It was like my wife was already in the car and it's like \"Oh, danger And then basically I wanted as fast as possible a bash script that runs my different uh experiments in the evaluation And I did something I know I learned how to use",
        "start": 298.96,
        "duration": 841.2010000000002
    },
    {
        "text": "unplugged the GP. It was like my wife was already in the car and it's like \"Oh, danger And then basically I wanted as fast as possible a bash script that runs my different uh experiments in the evaluation And I did something I know I learned how to use but in that moment I just needed like 10 seconds give me the command i This is a hilarious situation but yeah so what did you used i So I did the nonthinking fastest model it gave me the bash command I to chain different uh scripts to each other and then the thing is like you have the t thing where you want to route this to a lo file top of my head I was just like in a hurry I could have thought about it By the way I don't know if there's a representative case wife waiting in the care You have to run you unplug the GPU, laughter you have to generate a bash script It sounds like a movie impossible i I use Gemini for that So, I use thinking for all the information stuff and then Gemini for fast things or stuff that I could sometimes Google, which is like it's good at explaining things and I trust that it has this kind of background of knowledge and it's simple And the Gemini ape has got a lot better and it's good for that sort of things And then for code and any sort of philosophical discussion I use claude opus 4.5 also always with extended thinking extended thinking and inference time scaling is just a way to make the models um marginally smarter and I will always edge on that side when the progress is very high because you don't know when that's unlock a new use case and then sometimes use rock for um mealtime information or finding something on AI Twitter that I knew I saw and I need to dig up and I just fixated on although when Grock 4 came out the Gro for what is super heavy which was like their pro variant was actually very good and I was pretty impressed with it and then I just kind of like muscle memory lost track of it with having the chat ape open So I use many different things i Yeah, I actually do use Gro 4 heavy for debugging for like hardcore debugging that the other ones can solve I find that it's the best at and I it's interesting cut you say JBT is the best interface uh for me for that same reason but this could be just momentum uh Gemini i is the better interface for me I think because I fell in love with their best needle in the haystack if I ever put something that has a lot of context but I'm looking for very specific kinds of information make sure it tracks all of",
        "start": 428.4,
        "duration": 1089.1209999999996
    },
    {
        "text": "momentum uh Gemini i is the better interface for me I think because I fell in love with their best needle in the haystack if I ever put something that has a lot of context but I'm looking for very specific kinds of information make sure it tracks all of has been uh the best So, it's funny with some of these models if they win your heart over for one particular feature at one on a one particular day i for that particular query that prompt you're like \"This model is better And so you'll just stick with it for a bit until it does something really dumb There's like a threshold effect some smart thing and then you fall in love with it and then it does some dumb thing and you're like you know what I'm going to switch and try Claude or Chad GPT and all that kind of stuff This is exactly like you use it until it breaks until you have a problem and then then you change uh the LM and I think it's the same how we use anything like our favorite text editor um operating systems or the browser I mean there are so many browser options Safari, Firefox, Chrome all the relatively similar but then there are edge cases maybe extensions you want to use and then you switch but I don't think there is any one who types the same thing like the website into different browsers and compares You only do that when the website doesn't render if something breaks I think so That's that's a good point I think you use it until it breaks and then you explore other options I think i on the long context thing I was also a Gemini user for this but the GPT 5.2 release blog had like crazy long context scores where a lot of people were like did they just figure out some algorithmic change It went from like 30% to like 70% or something in this minor model update So, it's also very hard to keep track of all of these things But now I'm look more favorably at GPT 5.2's two is long context that it's just kind of like how do I actually get to testing this never ending battle i It's interesting that none of us talked about the Chinese models from a user usage perspective What does that say Does that mean the Chinese models are not as good or does that mean we're just very biased and us focused i I do think that that's currently the discrepancy between just the model and the platform So I I think the open models they are more known for the open weights not their platform yet i There are also a lot of companies that are willing to sell you the open model inference at a very low cost I think like open router it's easy to do the",
        "start": 555.12,
        "duration": 1339.920999999999
    },
    {
        "text": "I think the open models they are more known for the open weights not their platform yet i There are also a lot of companies that are willing to sell you the open model inference at a very low cost I think like open router it's easy to do the deepseeek on perplexity I think all of us sitting here are like we use open GPT5 pro consistently We're all willing to pay for the marginal intelligence gain and anyone that's like the these models from the US are better and in terms of the outputs I think that the question is will they stay better for this year and for years going but it's like so long as they're better I'm going to pay for it to use them I think there's also analysis that shows that like the way that the Chinese models are served this you could argue due to export controls or not is that they use fewer GPUs for replica which makes them slower and have different errors and it's like speed and intelligence if these things are in your favor as a user I think in the US a lot of users will go for this and I think that that is one thing that will spur these Chinese companies to want to compete in other ways whether it's like free or substantially lower costs or it'll breed creativity in terms of offerings which is good for the ecosystem but I just think the simple thing is that US models are currently better and we use them and I try Chinese I try these other open models and I'm like fun but not going to I don't go back to",
        "start": 684.88,
        "duration": 1477.1199999999988
    }
]