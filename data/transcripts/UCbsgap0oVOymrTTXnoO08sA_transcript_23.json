[
    {
        "text": " Meta and Harvard just dropped an open source coding agent called the Confucious code agent music and it basically proves that the agent scaffold can matter more than the model itself Then Abu Dhabi's TI comes out of nowhere with Falcon H1R7B, a tiny 7B reasoning model with 256,000 context window that's outperforming models way bigger than it And then DeepSeek updates the R1 paper with 60 extra pages like it's some kind of secret technical dump which has everyone thinking the next model release is right around the corner So let's talk about it All right let's start with the meta and Harvard thing They introduced something called the Confucious Code Agent or CCA and it's built on top of a platform called the Confucious SDK. At first glance that just sounds like another AI coding agent right Like a competitor to SWE agent OpenHands. all the other projects that try to solve GitHub issues by running tests editing files and iterating But Confucious is doing something much deeper The key idea behind it is literally a shift in philosophy They treat scaffolding as the main problem And I know scaffolding sounds like a boring word but it matters a lot Most people still think an AI agent is basically a big model with a couple tools attached Maybe it can execute commands Maybe it can edit files Maybe it can do search and then you slap a wrapper around it and call it an agent Confucious takes the music exact opposite approach They're basically saying if you want an AI to survive in a real industrial codename across long debugging sessions across dozens of files across hundreds of actions then the actual system around the model matters just as much as the model itself And honestly I agree with that because coding in real reps isn't one clean prompt It's chaos its tests failing in weird ways dependencies breaking environment issues hidden conventions in the codebase, and small changes causing completely unexpected side effects So, Confucious SDK is organized around three axes agent experience user experience and developer experience That sounds corporate but it's actually pretty practical Agent experience controls what the model sees and how context is structured User experience is about making the agent understandable for humans with readable traces and differ Developer experience focuses on observability and debugging the agent itself so you can tune it like a system Now what makes this thing truly interesting is that Confucious SDK introduces three major mechanisms that make the agent perform better across long tasks The first mechanism is a unified orchestrator with something called hierarchical working memory And this part is important because it solves one of the biggest weaknesses in agents today They forget too easily When an agent works on a real task it's not just one message and one reply It can be 60, 70, sometimes over 100 steps music The agent edits code runs tests reads logs changes strategy",
        "start": 2.639,
        "duration": 341.20099999999996
    },
    {
        "text": "it solves one of the biggest weaknesses in agents today They forget too easily When an agent works on a real task it's not just one message and one reply It can be 60, 70, sometimes over 100 steps music The agent edits code runs tests reads logs changes strategy lands on a fix In that long trajectory old decisions matter old patches matter Test logs from 40 turns ago matter But if you rely on a simple sliding context window eventually that information gets music cut off And once it gets cut off the agent starts acting like it has amnesia It repeats mistakes It loops It breaks things it already fixed music It loses the thread of what it even tried earlier So Confucious SDK tries to solve that problem using hierarchical working memory music Instead of treating the agent run like one giant chat transcript it partitions the trajectory into scopes summarizes past steps compresses music context and preserves the important artifacts It keeps patches error logs and key design decisions accessible later while still keeping the prompt within context limits So basically it's not just bigger context window equals better memory It's actual memory architecture This is a huge deal because when you look at long horizon coding tasks memory is the difference between an agent that slowly improves and an agent music that spirals into a loop Most people underestimate how much performance gets destroyed by bad memory management especially when you deal with large repose Now, the second mechanism in Confucious SDK is honestly one of the coolest things in the whole release Persistent note-taking. They add a note-taking system where a dedicated agent writes structured markdown notes from execution traces And these notes aren't logs They're more like the kind of notes a senior engineer would leave for themselves after solving a tricky bug So it captures rep conventions strategies that worked patterns that caused failures and little gotcha that matter for that specific codebase. Then it stores them as long-term memory that can be reused across sessions This concept is deeper than it sounds because in real engineering you don't just solve a task You build familiarity with the codebase. You learn the style You music understand the test suite You realize which parts of the rep are fragile That knowledge is exactly what makes someone faster the second time they touch the project Confucious basically tries to simulate that music and they tested it in a pretty clean setup They ran the Confucious code agent twice on 151 stench pro tasks music using clawed 4.5 sonnet In the first run it solves tasks from scratch and generates notes In the second run it reads the notes before solving The results show something subtle but meaningful Average turns drop from 64 to 61. Token usage music drops from around 104,000 to 93,000 and resolve at one improves from 53.0 to 54.4. Now, that accuracy improvement is not",
        "start": 173.04,
        "duration": 649.1199999999995
    },
    {
        "text": "generates notes In the second run it reads the notes before solving The results show something subtle but meaningful Average turns drop from 64 to 61. Token usage music drops from around 104,000 to 93,000 and resolve at one improves from 53.0 to 54.4. Now, that accuracy improvement is not but the point is that the note system clearly improves efficiency It makes the agent waste fewer steps and fewer tokens which is literally money in real production environments It also proves the principle that long-term written memory can work even in a benchmark setting So that's mechanism number two The third mechanism is modular extensions for tools Confucious SDK exposes tools as extensions like file editing command execution music test runners code search and the key part here is that each extension can maintain its own state and prompt wiring music Now why does that matter Because most agents treat tools like random calls They do a bash command dump output and hope the model reads it correctly But real tool usage needs discipline It needs structure It needs recovery logic Confucious shows this clearly through ablations where they test tool sophistication On a subset of SWEBench Pro, they show that better tool handling dramatically boosts resolve at one For example with Claude 4.5 Sonnet, a simple tool configuration reaches around 44.0 zero resolve at one while richer tool handling reaches 51.6. That's a massive jump It shows that tool routing and sequencing becomes a major performance lever So when people say agents are just models with tools Confucious basically replies \"No, the tool strategy and system design can move you as much as changing the model And then on top of all of this Confucious adds something that feels like a preview of what agent development will look like in the future A meta agent that designs agents This meta agent takes a natural language specification of what kind of agent you want proposes configurations prompts and extension sets runs the candidate agent on tasks inspects metrics and traces and edits the configuration iteratively in a build test improve loop So instead of a human doing 500 manual iterations trying to tune prompts tool wiring memory format and control flow you get LLMdriven optimization of the agent design itself That's the kind of thing that makes a system scale faster because now the tuning process becomes systematic rather than artisan work Now let's talk results on sEbench pro Confucious code agent with claude 4.5 sonnet hits resolve at 152.7. Meanwhile, claude 4.5 opus with a weaker scaffold sits at 52.0. That's the headline that really matters A mightier model with a strong scaffold can outperform a stronger model with a weaker scaffold That's the first major theme of today Scaffolding can outweigh model size Now, let's connect this to the second article because it reinforces the exact same point just from the model architecture and training side instead of the agent side Technology Innovation",
        "start": 329.759,
        "duration": 966.3979999999995
    },
    {
        "text": "outperform a stronger model with a weaker scaffold That's the first major theme of today Scaffolding can outweigh model size Now, let's connect this to the second article because it reinforces the exact same point just from the model architecture and training side instead of the agent side Technology Innovation Falcon H1R7B. This is a 7B parameter reasoning model that matches or exceeds many 14B to 47B reasoning models across math code and general benchmarks And the reason this matters is because 7B models were traditionally treated as small like cheap models models you use for lightweight tasks Falcon H1R flips that assumption They combine three design choices into one system A hybrid transformer plus Mamba 2 backbone a huge 256,000 context window and a training recipe that mixes long form supervised reasoning with reinforcement learning using GRPO. So architecture wise it's a decoder causal model with hybrid layers Transformers handle attention-based reasoning while the Mamba 2 blocks handle linear time sequence modeling This matters when you push context length into extreme ranges because pure transformer attention scales badly as the sequence grows Falcon H1R supports a default max model length of 262,144 tokens in VLLLM, which corresponds to a practical 256,000 context window That is massive It allows extremely long reasoning chains huge tool logs and multi-document music prompts in one pass And because Mamba 2 scales linearly it music helps control memory and throughput at those lengths Now, training is where it gets even more interesting They use a two-stage pipeline First, cold start supervised fine-tuning on long form reasoning traces across math coding science plus additional non-reasoning domains like chat tool calling and safety Difficulty aware filtering weights harder problems Targets can reach up to 48,000 music tokens meaning the model learns to sustain long coherent reasoning Then they refine it with reinforcement learning using GRPO where rewards are given when the reasoning output is verifiably correct Math answers can be checked symbolically Code can be executed against unit tests That's the cleanest kind of RL because correctness is measurable So you're not training vibes you're training correctness Benchmarks show this model performs shockingly well for 7B. In math it scores 88.1% on time 24 and 83.1% on time 25 with an aggregate math score of 73.96%. In coding and argentic tasks it scores 68.6% on live codebench va and it stays competitive on general reasoning benchmarks like MMLU Pro and GPQA. Now, I want you to think about what this means for a second It means a small model can perform in the same band as much larger systems when the architecture and training pipeline are tuned for reasoning So the second theme becomes clear Parameter count advantage is shrinking when training and architecture get smarter And then there's the efficiency side Falcon H1R7B shows strong throughput in their reported setups reaching around 1,00 to 1,800 tokens per second per GPU depending on the batch settings They also include test time scaling through a",
        "start": 489.759,
        "duration": 1321.0399999999993
    },
    {
        "text": "clear Parameter count advantage is shrinking when training and architecture get smarter And then there's the efficiency side Falcon H1R7B shows strong throughput in their reported setups reaching around 1,00 to 1,800 tokens per second per GPU depending on the batch settings They also include test time scaling through a or deep con The idea is you run many chains of thought in parallel then filter candidates using confidence signals that helps accuracy without burning unlimited computer So between Confucious and Falcon H1R you're basically seeing the same thing System engineering is becoming the main differentiators Okay let's move to the third update because this one feels like the quiet move before a bigger strike Deepseek updated their R1 paper on RXE. No announcement no tweet nothing Just version one becomes version two But the update is huge The paper jumps from 22 pages to 86 pages And suddenly it contains the full training pipeline breakdown expanded evaluation across more than 20 benchmarks and massive appendices It's like they decided to open the entire black box The update includes de i de i de 3 intermediate checkpoints for the training pipeline Dev 1 comes from cold start instruction tuning which improves instruction following but hurts reasoning Dev 2 is designed to rescue reasoning using reasoning focused RL. Dev 3 is the final refinement using rejection sampling for highquality data followed by another SFT stage for stable output This detail matters because people wondered why R1 could do longhair reasoning without devolving into chaotic outputs like R10. The staged pipeline explains how they stabilized it The evaluation expands massively covering benchmarks like SWE bench verified live codebench, male pro GPQA diamond drop if evil and more They also add human baselines in some comparisons which is rare and makes the evaluation more meaningful than simple leaderboards. The appendices are where the real value is Appendix A details GRPO implementation and hyperparameters including learning rates KL coefficients and sampling temperatures Appendices B through F cover reward function design data strategies and evaluation procedures For researchers trying to reproduce R1, it's basically an operation manual Deepseek even includes failed attempts in the paper admitting they tried MCTS and PRM and found that they don't generalize well to openhanded reasoning tasks That kind of transparency is rare in industry AI, which usually only publishes success stories Now, the timing makes this update feel even more suspicious January with marks the first anniversary of R1's release and February music with is Lunar New Year. Deepseek has a tradition of big announcements around spring festival and last year V3 and R1 were released during that window So when the community sees Deepseek quietly dumping 60 plus pages of technical details into the R1 music paper people instantly start asking is this a prelude to V4? And honestly that question makes sense because companies usually don't reveal everything unless the revealed teach is already behind them This could be a defensive open-source strategy to prevent",
        "start": 670.0,
        "duration": 1664.478999999999
    },
    {
        "text": "60 plus pages of technical details into the R1 music paper people instantly start asking is this a prelude to V4? And honestly that question makes sense because companies usually don't reveal everything unless the revealed teach is already behind them This could be a defensive open-source strategy to prevent similar methods or it could be a signal that they've moved on to newer directions and want the old chapter to become public baseline knowledge All right drop your take in the comments I want to see what you think And if you enjoyed this breakdown make sure to like the video subscribe for more AI updates like this and I'll catch you in the next one",
        "start": 844.399,
        "duration": 1702.6389999999985
    }
]