[
    {
        "text": " As soon as we reach AGI, capital is all that matters. The permanent underclass. I don't think because we make AI, we necessarily make class stratification. Do you think that banning the sale of specific chips to China helps the US? I think it's a good idea to then provide previous generation of chips, the generation before because then it makes it challenging. The Intel acquisition 10% ownership. Does Intel want this capital or not? Is it being forced? Like is this socialism? Is this Mauism? As you're looking at the Trump administration and their approach to China's AI, what would you say you disagree with in their approach and what would you say you agree with? Reed, thank you for joining me here today. My pleasure. I appreciate it. So, we're going to get right into the deep topic first. We're going to talk about AGI. I've been seeing more and more technologists, especially younger technologists, talk about the permanent underclass and having a certain amount of time before being consigned to the permanent underclass effectively because AGI as soon as it as soon as we reach AGI, the capital is all that matters. Have you heard this? What do you think about it? If you agree, what can we do to prevent it? And if you disagree, why? So, I tend to think societies that don't allow genuine and fairly broad upward mobility are unstable. And, you know, you could go to the kind of dystopia movie Alisium where you have an upper class with a bunch of, you know, militaristic robots. Great movie, you know. Yes. Exactly. Great movie. You know, excellent. Matt Damon, you know, the whole crew. Yeah. And I think that would be a dystopia and we bad. I don't think because we make AI we necessarily make class stratification. Technology can be used for class stratification. technology can be used against for example on the against it side is if you allow upper mobility then the fact that everyone gets an educational agent to like not just the wealthy people who have educational agents today they're called tutors other things but everybody has that then you have much more potential educational upward mobility so I am I am not worried intrinsically based on technology techology for a permanent underclass for, you know, kind of a shift towards an autocratic society. I'm worried about kind of the political system in terms of getting there. And you could say, well, but the AI could actually in fact further destabilize our destabilized political system. And the answer is yes, our political system is destabilized. And AI can go can can go in a negative direction, but can also go in a positive direction. And so my view is what we should be doing as opposed to like hey how do I get as much capital as possible in the three years is how do we get to a political system that enshrines",
        "start": 0.08,
        "duration": 372.48
    },
    {
        "text": "a negative direction, but can also go in a positive direction. And so my view is what we should be doing as opposed to like hey how do I get as much capital as possible in the three years is how do we get to a political system that enshrines that is broadly you can make progress and that you have upward mobility and frankly geographic mobility for people. Yeah. Is that going to be facilitated with AI? Do you think governments should be adopting it much more quickly than they are right now? I I just I'm I'm wondering how we can actually achieve that. Well, it can be facilitated. you know, educational to one thing I do think governments I mean roughly speaking part of the reason why I wrote super agency early this year. Roughly speaking, we're in a cognitive industrial revolution. Britain did not invent the industrial revolution but embraced it most fully early. that led to a centuriesl long empire of otherwise this kind of small island right you know kind of you know kind of as part of Europe and I think that we as society need to do that too and I think it's the how do we make our industries you know kind of AI enhanced and robust how do we help the transitions with workers because by the way it will be lots of job transitions it's not going to be easy like the whole lite movement in, you know, kind of powered looms in England. That's just kind of the beginning of the kind of adoption issues that we'll have. But by the way, part of like when people say, well, why should I bother like I I'm happy with my job the way it is. Why shouldn't I just resist the use of AI and not have it? And the answer is well because your children your grandchildren will be the kind of the the non-industrial rural children that the others countries had. You don't want that for your children. Two what government should be doing is saying how do we get you the benefits of AI as much as possible. Yes, there's going to be job transition, but a medical assistant, I mean, think if everyone who has access to a smartphone has access to a, you know, kind of better than a GP today in terms of navigating health health issues. say that you get into kind of the issue of triaging what goes on with emergency rooms or other things and say hey I don't just have to go to emergency room because I don't know what to do but actually that's triage to the people whose lives need saving versus the take two ibuprofen and you know check in again in a couple hours that kind of thing those are all things but also an education assistant a legal assistant etc you bring those benefits to people",
        "start": 186.08,
        "duration": 666.0799999999999
    },
    {
        "text": "do but actually that's triage to the people whose lives need saving versus the take two ibuprofen and you know check in again in a couple hours that kind of thing those are all things but also an education assistant a legal assistant etc you bring those benefits to people going to be in transition that'll be scary. That'll be difficult. I'm not going to enjoy it. But but I understand that that's what's important for, you know, my family, my community. Yeah. So, we I'm going to get into education and health and integrating AI into that, but I want to zoom in a little bit and let's talk about agents specifically. , you know, a agents increasingly are browsing the web on behalf of humans. , but the web still is made for humans. Do you think that there are or what are the major fundamental changes that need to be made to have the web be more agentfriendly? Well, we want the web to continue to be also very human friendly as it is because you know humans use even as we get mediated even as we have agents for us and agents in the other people and other organizations that we're talking to. so I think that that will be that'll be important. Now anthropics MCP protocol will be key like you know agents talking to agents. So it'll be less I think the web per se becoming agent friendly as much as a as an entire new channel of communication on the internet which will be agents talking to each other calling APIs and services and other kinds of things. And I think we're already beginning to see some of that. Yeah. Now, of course, part of when you get to, you know, kind of agentic mode, , and what you see kind of working as agents, whether it's, you know, anthropic, open AI, others, , is like we're already like, you know, part of the thing I've already started using is I go, hey, just as I use, you know, deep research to to research things, when I go, hey, I would like to get an an understanding of what's going on, say, for example, in social media, like I will actually use you know agentic mode in order to do that. I'll say hey what like what's trending as opposed to the kind of the the kind of call it polluted melee that is Twitter when I want to understand stuff on Twitter I ask the agent to give me a summary. Yeah. Okay. I'm glad you brought that up because I've been thinking about this a lot. As AI creates a kind of skewed ratio between signal and noise on the web. It just seems like your agent is going to have to filter the web and provide you with information as you said, but like what does that look like? If you're entrusting a single AI to essentially",
        "start": 336.0,
        "duration": 978.2379999999997
    },
    {
        "text": "creates a kind of skewed ratio between signal and noise on the web. It just seems like your agent is going to have to filter the web and provide you with information as you said, but like what does that look like? If you're entrusting a single AI to essentially isn't there going to be this risk of inherent bias or some of the you know your founder of LinkedIn, social media went through echochamber issues and amplification of fear and hate. Doesn't AI or an agent on your behalf filtering information from the web potentially suffer from those same issues? Well, it can. but by the way, there's biases everywhere. There's, you know, you have biases, I have biases. And so the way you navigate biases is by getting multiple, you know, kind of angles and points of view. It's part of what's made the scientific method. It's part of what you know the reason why you know things like you know understand germ theory of disease because we actually have multiple people reproducing experiments and so forth. So what I think the most natural lazy course for people to do is to have one agent that they use they'll have some risks from that. Does that agent make mistakes? Does that agent reflect the bias of the whatever organization you're is you know technologists talk about well it should be agent I'm paying so it's responsible to me responsive to me versus responsive to advertisers. I think that's going to be far more complicated because if you have human nature people prefer free or cheaper with advertising it's it's been demonstrated for you know over a century in terms of how this works. So I don't think that I think that you have to you have to presume that advertising is actually in fact going to be part of it. but but like for example today when I do deep research on something I actually in fact generally speaking do deep research not just on chat GBT but also on Gemini and also anthropic and actually I'll take the outputs and I'll freed the three of them into sometimes one of the models or three of the models and compare and contrast and and so forth and get some understanding of where those biases are across them as a way of operating. I think that's what's going to happen in agents, too. I don't think we're just going to have one agent that says la, it's only through my one agent in the web. I think we're going to have a couple agents. And what's more, part of I think the people who are the at the edge of learning how to use these are using saying, hey, it's not just generic like, you know, like the the 1.6 six words I type into, you know, pick your favorite search engine. But actually, in fact, , you know, part of what I do when I'm prompting an",
        "start": 494.639,
        "duration": 1263.679
    },
    {
        "text": "edge of learning how to use these are using saying, hey, it's not just generic like, you know, like the the 1.6 six words I type into, you know, pick your favorite search engine. But actually, in fact, , you know, part of what I do when I'm prompting an I'm looking at this, I'm particularly interested in this from an a global perspective. So, can you make sure that you're representing Asian, African, European, you know, perspectives in your in reflection to me. And by giving it a role, you're actually prompting it to try to see around blind spots and all the rest. And I think all of that will be part of it. So actually, in fact, I think ultimately this kind of agent this kind of a trusted agent mediation should help me relative to biases from today rather than hurt me. Yeah. Okay. So you you mentioned publishers or or you mentioned like as as agents are browsing the web on your behalf, you're not necessarily giving the eyeballs to the publisher, your human eyeballs, the monetization model might have to change. Have you had any thoughts as to what that new monetization model might look like as more agents are browsing the web? So I've given some thought to it. you know what my hope is like I think there will be various forms of advertising advertising you know you know has a whole bunch of things where Google's adwords is the current you know kind of leading contender although you know meta with its you know kind of social networks is also doing quite well and I think those two will be baseline but I think we will invent new ones it's like when Google launched its website it thought it going to be selling ads or sorry it started it thought it was going to be selling enterprise services then it thought it was going to be doing doubleclick and then there was a there was a drop in the in like the whole advertising internet market dropped out and it had to invent adwords by you know using inspiration from overture and other things right I actually think there will be a unique new model that one of the leader leading contenders will figure out and that will that will essentially create the equivalent of the the Google power for how that works. And and what I hope for the characteristics of that new model are that it has some of the characteristics that like Adwords had which is well be clear in distinguishing what is advertising be clear in in kind of saying hey here is what your participation in this advertising ecosystem does in terms of here's where your your data and your agency plays into it. But my guess is there's going to be something new. And that's one of the things I've been paying attention. One of the things I like as a as a",
        "start": 639.44,
        "duration": 1594.08
    },
    {
        "text": "participation in this advertising ecosystem does in terms of here's where your your data and your agency plays into it. But my guess is there's going to be something new. And that's one of the things I've been paying attention. One of the things I like as a as a of entrepreneurs who are all thinking about this. And so I might come up with an interesting idea, but I think it's much more likely that some really interesting, bold entrepreneur will do it. I hope to see it. Yeah, me too. I I just you know, if it were up to me, I would just simply pay Chachi PT and and they wouldn't have advertising in it. But I understand the vast majority of the world probably prefers a free product and service. So advertising makes sense. Do you foresee model providers integrating advertising into their model responses? I think all of them will. Yeah, there's going to be a free tier for all of them. Yeah, exactly. It just it just makes economic sense. Okay. I want to talk about agent memory for a minute. it's something I've been thinking a lot about. It's really it brings these models value to really an incredible level but it also provides a moat for the model providers right you're building up a memory with chachi pt it gets better it develops a shorthand with you but if you want to go somewhere else you essentially have to start over do you think there's a need for kind of a open protocol like MCP but for agent memory do you think like h how much investment should companies be making into making the agent memory more portable since it goes against what their incentives are. It does indeed. I think that the companies will naturally basically try to make portability of memory hard. They're they're not going to invest in making portability of memory easy. it's the natural economic incentive and the balance is to some degree it's like well like say for example I like and and and they there will be good call it reasons and rationalizations which are like wow but my memory format is special and and and and I can't like I don't want to export my my secret sauce by which I'm providing such amazing you know kind of persuasion or engagement or or creativity. , and so I, you know, I I I I I'll say sure, I'll export, you know, Reed Hoffman as a name. I'll export, you know, kind of biographical details, you know, kind of things. I'll I'll export a summary of things we've talked about like, oh, you seem to have been doing a lot of a lot of relationship between AI and art and AI and creativity. So, well, say and Reed really likes AI and creativity. Great. great, you know, new model, you should use that as your prompt and go.",
        "start": 806.0,
        "duration": 1913.5209999999997
    },
    {
        "text": "talked about like, oh, you seem to have been doing a lot of a lot of relationship between AI and art and AI and creativity. So, well, say and Reed really likes AI and creativity. Great. great, you know, new model, you should use that as your prompt and go. by the way, if one if one particular agent becomes dominant, governments will then start trying to poke it to to make the the the the make the memory portable. But then the question of course will be well like well what do you mean by portable? Like like for example, I will have gone to you and I'll have said as an AI as an agent provider and I'll have said hey you know would you like me to keep exact records of all your chats or would you like me to to to to to delete them and you'll probably say well actually I'd prefer privacy so I'll delete them. Well then hey there's nothing to port right we we're remembering what kinds of things we have in the conversation but there's nothing to port anyway. So that will be an ongoing challenge. Now what I hope is that we already see multiple significant AI agent providers. so it's obviously ever like like chatb is the most known one but you know there is anthropic and claude there is copilot there is gemini there will be others and so when you have a lot of competition and contention between them the the needs to enforce portability is somewhat less. Now, by the way, part of portability, like this is where you get to the really advanced game is like say, well, can like for example, what I'm doing today is I'm using two agents. Well, could one agent, you know, help remember the things from the other a like both agents are are doing memory by syncing all of them? Very difficult. So, and then it gets well but then it gets to as that happens then I have more ability. I already have multiple agents and I have some portability already because I have multiple agents. Yeah. Yeah. And it seems like if you're competing with these top model companies, but you're not necessarily in first place, you're probably going to be a much bigger proponent of open standards. It's same thing we saw with Llama and Meta. Yes. Like you know, Scorched Earth, open free. Yes. And so, hopefully we do see that. we'll see some some parts of it for sure because of that. Yeah. So, so let's let's continue down the path of memory. When you think about like personal memory versus work memory, this is also something that I've thought a lot about. , the agents are going to be best when they have both. When they have the context of you as a person, how you like to work, maybe even, you know, your your personal life information. And",
        "start": 968.56,
        "duration": 2219.8409999999994
    },
    {
        "text": "versus work memory, this is also something that I've thought a lot about. , the agents are going to be best when they have both. When they have the context of you as a person, how you like to work, maybe even, you know, your your personal life information. And you're starting to develop all of the memories around the context within the company. , but now we start to get into complex IP issues. like the company is certainly going to own anything you build there. So, does that mean the company is going to be less likely to want you to bring your personal agent to work? How do you see that playing out? Well, this is great question. I think relatively too few people are thinking about this right now. you know I from my early days you know 20 plus years ago with LinkedIn I'm a huge proponent of individuals kind of in the 20 year ago like you own your address book whereas a company would tend to say I own your address book because it's on my computer right right and it's like no no individuals own the relationships own their address book you know company might have a copy or something but like the individuals do it I think we're going to want to be to allow how you know kind of freedom of economic and talent mobility. We're going to want as much you know like that just that history of LinkedIn that that transitivity following the individual and as much on the individual side as possible. Now that of course will be you know have some challenges because the the most expensive AI agents will generally be provisioned by the company will be interled with a whole bunch of different company IP right and so the question is when I leave what kinds of things can I take with me right because today we basically say you can't take anything that's the company IP with you so Sure. Understandably. Yes. Understandably. So, what's the interface point between these two? And I go, and you already see little points of it. Like, for example, well, if I'm in a work meeting, can I have my personal notetaker be taking notes? That seems to be not okay because the other people are working for the company. The question is, is we're doing this in a shared company space. So, what will be the things that will be part of the the personal space versus the company space? And I think it's a unknown as yet. Yeah, it it's definitely going to be difficult to figure out because all of these things are coming together in the same place and the companies are incentivized both to have your personal AI in the company and not to at the same time. , if you're looking at a team that is using AI agents and using AI heavily, how do you tell when they're outsourcing",
        "start": 1124.48,
        "duration": 2534.641
    },
    {
        "text": "coming together in the same place and the companies are incentivized both to have your personal AI in the company and not to at the same time. , if you're looking at a team that is using AI agents and using AI heavily, how do you tell when they're outsourcing specifically within the context of the enterprise? So part of what you need to to generally kind of have people have is is make sure and this is part of the you know AI work you know AI work in the age of AI is there's a lot of metacognition that we provide as human beings even with these amazing agents today. So, and part of metacognition is the context, awareness and judgment of you like is this answer does it make sense? Are there things there are there things that you're missing? Right? Because the agent don't really have common sense right now. like if they if if they don't they don't necessarily realize what like we're like for example you know and agents are getting better on this but you go hey give me the answer to this prime number problem and and you know the agent gives you a bad answer and you say nope that's wrong and goes oh I'm sorry I was wrong here's another end you go nope that's wrong a human by the third time goes okay I'm this up I will figure it out the agent just keeps giving you wrong answers as they're going and So you got to have you have to own that kind of metacognition. You have to kind of own that context awareness. And so part of what you want to be doing is you go sure I'm using AI to accelerate to parse information to to work more quickly to to do a bunch of things, but I still have to own the result. And like for example, when you look at good people using coding agents today, they don't just plug in the coding agent and go to town or whatever, right? They look at the results, right? and they kind of they're they're applying that metacognition to it and I think you know you asked how do I check it's like okay so like part of what I do is say how did you use AI in doing it now if they said I didn't use it at all he's like well that's foolish right because you can be greatly accelerated in how you work I push people into using AI but then they say oh I used to do a whole bunch of it great how did you cross-check the work how did you how did how how were you thinking about making sure the work outputs were right. And if they just said, \"Well, I just trusted you.\" Like, well, okay, you got to do more than that. Now, you could cross-check with different agents. You",
        "start": 1282.88,
        "duration": 2816.642
    },
    {
        "text": "you cross-check the work how did you how did how how were you thinking about making sure the work outputs were right. And if they just said, \"Well, I just trusted you.\" Like, well, okay, you got to do more than that. Now, you could cross-check with different agents. You some work. I had agent two cross-checking it. I asked agent two the following kinds of questions in terms of cross-checking it.\" Great. Now, you're in much more in the metacognition area. Yeah. Okay. , I want to move on to global competition obviously. , let's let's talk about China a little bit. , you've said that the AI race with China is game on. You've also said AI should remain American intelligence, both of which, yes, I agree with. , do you think that banning the sale of specific chips to China helps the US or is that putting us in a position where US infrastructure isn't the default, isn't the standard at that point? So ultimately, what matters is the software. So slowing down the provisioning of scale chips that matter for the training of this does help maintain something of our investment edge in the software. And so I think playing to to saying hey we as the US are going to provide less numbers of scale chips to people who we are in direct kind of geopolitical competition with. Yeah. You know and you could you could say China but you could also say well the extreme is like Russia and all the rest we're we're in we're in competition with. And so we will on the the top leading chips provide fewer slashzero. That being said, I think it's a good idea to then provide the the the previous generation of chips, the generation before because then it makes it challenging for competitive chip industries to get created, right? That's so you want to say not the latest but the old ones. That's okay. Yeah. But doesn't that incentivize China to invest? I mean, they're going to do it anyways, I suppose, but like they're they're looking and they're getting previous generation chips and they're they're thinking, okay, well, not only do I have to build the chips, I have to build the software ecosystem as well. Maybe they're going to do it either way, but it seems like if we're giving them the latest chips, they may be less inclined to do so. Well, this it's a software race, not a hardware race. Mhm. So other than hardware enabling software. So and yes they may eventually end up creating their own and you know for example they've they've created some they're exploring different chip architectures in China because yeah of limitation. By the way ultimately this is one of the benefits of competition. That might be a good thing that that other chip architectures come out. but I think that that gives our software an edge over some number of",
        "start": 1429.76,
        "duration": 3171.1220000000003
    },
    {
        "text": "they're exploring different chip architectures in China because yeah of limitation. By the way ultimately this is one of the benefits of competition. That might be a good thing that that other chip architectures come out. but I think that that gives our software an edge over some number of that matters is the next two to five years. But if you keep that edge, that's the kind of thing that that can be very helpful for the cognitive industrial revolution. So I'm in favor of of that. Now that being said, the reason it's not just block all chips is because you say, well, are they going to then go create their own leading edge chips? They're gonna try, but by the way, it's harder for them to do that when they can also buy the last generation of chips. Right. Right. So, it's like the sure they're going to try to create that leading generation anyway. , you know, you'll hear, you know, our chip manufacturer saying, \"No, no, we should just provide them all because that's what's good for their business.\" But the thing that matters is is which AI software will be defining the future of h of human society, human work, you know, kind of human industries and you know part of what we want as western democracies is we want that to be our AI. Yeah. And so then as you're looking at all of these incredible open source models coming out of China and then you look in the US where it seems like the trend is Frontier Labs are going to have their Frontier model closed source and then maybe previous generations open source. How do you compare and contrast those approaches? Is it more of well if you're trailing you're going to do Scorched Earth open source as we talked about earlier? do you think the US's US Frontier Labs posture towards kind of closed source frontier and then everything else can be open source is the right one? I do for a couple reasons. So, first I mean I thought you know OpenAI's release of its two open models this month was epic relative to like all other open source models including the Chinese ones like the the the the open AI models are I think in many facets the best and that's that's interesting and useful. Now, part of the reason I've been a proponent of large scale proprietary models is because of alignment and safety issues. So, for example, part of what you want to do is say, hey, , let's not provision the best models for cyber crime, for bioteterrorism, you know, these kind of things. And and and aligning them is really important. And open AAI has been doing that for for years investing in you know kind of safety groups by the way Anthropic is as well Google is as well Microsoft is as well and those are on a proprietary models",
        "start": 1608.96,
        "duration": 3483.2819999999997
    },
    {
        "text": "of things. And and and aligning them is really important. And open AAI has been doing that for for years investing in you know kind of safety groups by the way Anthropic is as well Google is as well Microsoft is as well and those are on a proprietary models and when open gets to open source its models it's trying to do with as much safety and alignment as it can in those open sourcing and so that's what's that's that's why I think this combination in as much as people have any worries about regulating the front of AI prot proprietary models are lead to much more easy regulatory conversations with governments. Well, problem is once you open source a model everybody has access to it and that includes the criminals that includes the rogue states that includes the terrorists. So you have to be careful about that. Doesn't mean like I'm pro open I was on the board of Mozilla for 11 years. I'm pro open source in all kinds of different ways. I think LinkedIn has open sourced more kind of fundamental technologies that have led to new public companies than than many other companies. And so I like I'm very positive on it, but you have to be careful about how you're doing it. Yeah. I mean there's the argument that opensourcing the models will get more eyeballs on it. And there's been a history of technology that has been open source. The more eyeballs you get, the hard the more hardened it becomes. And so h how do you counter that example? Is it Yeah. How do you counter that example? But that's actually for open source, that's why open source and open weights actually makes a very big difference. So for open source, that's absolutely the case. It's part of the the the central way that you know Linux and Mozilla and other folks have played to being very useful because cyber security hardening is useful when you see the code and you can do stuff. The problem is when you release open weights that doesn't kind of go back into the common pile. Even as you you you might discover a vulnerability with an open weights model that doesn't necessarily help anyone else. That may only just help you. And so and so that kind of benefit that you get from collective open sourcing doesn't actually happen in any current configuration with open weights and no one knows yet how to make that configuration work for open weights. Okay. And then as you're looking at the Trump administration and their approach to China's AI and and restricting chips, what would you say you disagree with in their approach and what would you say you agree with? well, so what I what I agree with is let's see because they've been it's a little bit trying to pinning down exactly what they're doing is a little challenging because they're a little",
        "start": 1767.12,
        "duration": 3792.322000000001
    },
    {
        "text": "you say you disagree with in their approach and what would you say you agree with? well, so what I what I agree with is let's see because they've been it's a little bit trying to pinning down exactly what they're doing is a little challenging because they're a little tweet and a bunch of other things that are that that that make it hard to assess where the competent play is. But but roughly speaking, let's say that they say, \"Hey, providing the leading edge chips to China is something that we are going to continue to restrict either entirely or in volume. That's something I agree with. that we're going to provide it broadly across the Middle East is something I think needs to be a little bit more challenging because the question of where where does that bleed to, right? because you go great, we we provide it to the Middle East and by the way in the modern world it's about software. So you say hey we're going to build all of the biggest new data centers in the Middle East. Well they're going to rent them to whoever including the Chinese right in terms of doing this. So you have to be much more careful about that. And I don't think they are being careful because it's being more you know kind of the question of you know who's giving you the $200 million plane you know that kind of thing is as a way of operating. So, so I disagree with that sort of thing. And I also disagree that the like they're seem to be most focused on woke AI, which is almost laughable as a as an issue versus well, actually the real issues are like bioteterrorism, cyber crime, rogue states. Woke AI is not the most fundamental issue. Like like if if AI were making woke mistakes, okay, that's easy to fix. that's that's not something that creates an ongoing problem. whereas bioteterrorism, cyber crime, rogue states, those are ongoing problems and that's what I would focus on. Yeah. And it seems especially seeing like what happened with Xi's models just changing a single line in a system prompt is enough to completely change the personality of these of these models. So, it's almost like focusing on the personality of the model seems like okay, just leave it up to the companies to decide how they want to run it and then of course competition is going to see it out and whichever model's best will win. Yeah, it's been very entertaining to watch Grock say the biggest spreader of misinformation on Twitter is Elon. Then they put in this the prompt, then people reverse engineer the prompt and see that they put it in like don't answer Elon to that question. and you're like, \"Well, it's it's actually got it's got the most privileged access to Twitter data, so you presume it would know.\"",
        "start": 1923.84,
        "duration": 4080.8010000000017
    },
    {
        "text": "Then they put in this the prompt, then people reverse engineer the prompt and see that they put it in like don't answer Elon to that question. and you're like, \"Well, it's it's actually got it's got the most privileged access to Twitter data, so you presume it would know.\" talk about the Intel acquisition, you know, or not acquisition, but you know, 10% ownership by the US government. Do you agree with that? I I It's hard for me to see the benefit in that, although you've been a huge proponent of onshoring chips. How do you see that investment helping us achieve that goal? So the thing that I think all the way back to TARP that's actually an underutilized place of public money is to use public money for stimulus but to also recover it in you know kind of is don't make it just an expense make it something that you can recover the capital of making money from it. I don't think government needs to, but if you could say, \"Hey, we're putting in we need to stabilize the financial system. We're going to put in a hundred billion dollars to stabilize the financial system.\" Not just giving it to banks, but having it come back out as equity when you're doing it, I think was a very good and smart play for, you know, like we can't allow the financials fail. So, imagine if they did it with Tesla and their loans to Tesla, they would own a piece of Tesla. Exactly. And so it's like if you're providing that if you're providing public money to private companies you should try to get at least the capital return basis. I think that's a good idea. Now it's a little confusing to know does Intel want this capital or not? Is it being forced like like nationalization of industries is is a socialist thing like it's been entertaining to watch the critique of is this socialism is this Mauism because actually in fact you know like that is what socialism is. If you're if you're if you're if you're you know kind of slippering the brain it's not like wokeism is not the only socialism it's nationalization of industries. So so this it should only be given as options to these companies unless it's a it's a necessary you know infrastructure. Now I do think that we want to have good competent efforts at building up onshoring manufacturing of chips. I think that was a mistake over the many decades as to offshoring it you as much as we have. I would like to see more competence by the administration and government doing this. So far governance by tweet doesn't really seem to be doing it. Is it feasible in the next few years to really onshore chip manufacturing? I mean TSMC owns it, right? They they own the vast majority of chip manufacturing. it it like did the economics work",
        "start": 2068.8,
        "duration": 4394.002000000003
    },
    {
        "text": "government doing this. So far governance by tweet doesn't really seem to be doing it. Is it feasible in the next few years to really onshore chip manufacturing? I mean TSMC owns it, right? They they own the vast majority of chip manufacturing. it it like did the economics work to 5 years not without some very directed and smart governance. So for example you know in prior administrations I had been kind of recommending hey can we set up partnership deals in this with Mexico and Canada for doing this. I actually think that would be a very good way of doing this. Now, you might do special economic zones in places like you know, New Mexico or or Texas or other things and making that kind of partnership work. Now, under this administration, which seems to be trying to declare war on everyone that was our previous, you know, friends and allies, I'm not sure that will work, right? Because you know it doesn't work when you're going, \"Hey, we're going to try to extort as much from you in a trade agreement as we can versus let's how do we make 1 plus 1 five and then you know sure maybe in five you get two, we get three, right?\" As a as a way of of kind of playing this out. That would be the kind of way to do it. I think you know because you're going to have to answer the questions around do you have the competence in the workforce? Do you have the the ability to do the the cost-effective manufacturing? Do you have the technology and knowhow to do this? There's a whole bunch of problems you're going to need to solve which is going to grow higher competence in doing this. And that's not, you know, like that's not governance by tweet. Yeah. Well, let's let's continue on the topic of the economics of artificial intelligence. I don't know if you've seen this meme. it was going around X in the last few weeks, but essentially hundred million in ARR, take off the mask, $120 million anthropic bill, and then in the third pane, take off the mask, $150, $150 billion Nvidia bill. is this part for the course? Is this like analogous to Uber and VCs subsidizing Uber in the early days until they became profitable and you know, a great company now, or is there something else going on? No, I think that's broadly the case. and you know it's one of the privileged positions that Nvidia has which is it is it can command great margins because everyone needs the they have amongst the the best chips in the business and CUDA the software. Exactly. Exactly. And CUDA. , and yes, , if you factor in hardware cost and amortization, training cost and amortization, things are being priced at at below their costs. Yeah. But by the way, that's part of why",
        "start": 2229.92,
        "duration": 4709.123000000003
    },
    {
        "text": "amongst the the best chips in the business and CUDA the software. Exactly. Exactly. And CUDA. , and yes, , if you factor in hardware cost and amortization, training cost and amortization, things are being priced at at below their costs. Yeah. But by the way, that's part of why Silicon Valley has, you know, kind of call it depends a little bit on how you parse it, but I think a good parsing is like five million people in it, which is just a little bit bigger than Ireland and over half NASDAQ. And part of the reason why Silicon Valley does this is because they've learned to say, hey, revenue today is not what matters. Revenue in the future is what matters. And so how do you get to strategic position for that revenue in the future? And it's especially true in network effect businesses. And that's part of the reason why I wrote Blitzcaling because that that was kind of lessons from this in terms of how to play out. So like people going that's that's that's just that's just you know like this will never be profitable. It's like no of course it'll be profitable, right? But investing in that future market because revenue in the future is what really matters is the thing that everyone's doing intelligently. Yeah. So this just feels normal. We're going to get that. Yeah. Okay. and then you know the anthropic founder a few weeks ago Daario said you know he talked about the white collar blood bath coming in the next three to five years and it that that quote went viral obviously. blood bath quotes always do of course. Yeah we should just put that in everything. do you like where do you fall on the spectrum for white collar jobs? Are you more towards the pessimist bloodbath or you more towards the optimist utopia? Where where do you fall and why? So I'm generally speaking more optimistic, not just you know super agency possible podcast etc. But also like I would be willing to take the bet that we don't have a white collar blood bath in you know even 24 months from today let alone when he did that that was eight that was eight that was six months ago. So it would be 18 months from today that would be even an easier bet. And the reason is because I actually think we're going to be finding a lot of different forms of human amplification. Now with that, I do think the transitions will be difficult. So I think Eric Benolson at Stanford has done some good work saying, \"Hey, we're seeing some slowdowns in junior hiring positions for customer service jobs and software engineering jobs. Right? I think the software engineering jobs will get refixed because I think there's infinite demand for software engineering. But I think customer service is actually one of the ones where will be most most replaced, right? And could",
        "start": 2391.76,
        "duration": 4986.4820000000045
    },
    {
        "text": "junior hiring positions for customer service jobs and software engineering jobs. Right? I think the software engineering jobs will get refixed because I think there's infinite demand for software engineering. But I think customer service is actually one of the ones where will be most most replaced, right? And could blood bath for customer service jobs in the next 18 or 24 months, I think that's that's very possible. And so so there's going to be a lot of job transitions and that transition I don't mean to to to sugarcoat it, that's going to be difficult, but I do think just as we were talking about metacognition and talk about like what do I like what do I see happening? I don't I think that the notion of an individual contributor who doesn't deploy with a swarm of AI agents that's the disappearing job. Yeah. Right. And so it isn't so much that the job will disappear as much as the job without AI agents. It'll be a person plus AI agents that will be doing the jobs. Well, but then there will just be fewer jobs. It's like, well, not necessarily. A lot of jobs are competitive. like my marketing against your marketing. It's competitive. And so we want to be like if if we just say, \"Hey, we're just gonna all outsource our marketing to chat GBT.\" Well, like I would like you to outsource your marketing chat GBD. I'm going to use ChatgBD plus humans and I'm going to try to win and beat, right? And there's a lot of jobs like that. And then for software engineering, for example, there's infinite demand for it. There's so much infinite demand that this is part of what I see as the future of of information and knowledge work. You even doing podcasting are going to have a software co-pilot. You're going to be using software, custom software for things that you're doing for your own competitive edge that you will be doing yourself. That's the reason why there's a lot of demand for it. And we're all going to have software co-pilots in terms of how we operate. So, I'm also an optimist and I I made this video a few months back where I talked about infinite demand. I'm so glad you mentioned that. , a lot of the comments I got said, \"No, actually there is not infinite demand, especially if you look at the market more broadly, maybe not part like specifically software.\" , when a company has hyper productivity and there isn't infinite demand, then they're not going to be hiring as many humans. what what is your counterargument to people who say there is not infinite demand? Well, I think they're right that a company will not have there'll be some paro curves even on the hypers scale companies behind management costs and hiring you know kind of new engineers. So for example if you said look I'm an",
        "start": 2532.56,
        "duration": 5282.403000000004
    },
    {
        "text": "people who say there is not infinite demand? Well, I think they're right that a company will not have there'll be some paro curves even on the hypers scale companies behind management costs and hiring you know kind of new engineers. So for example if you said look I'm an hyperscalers will be employing 20% less software engineers than they are today. trust me on this. And I'll go fine, other people are going to hire them, right? There's a competition, right? That that was always my argument. Yes. And so, and by the way, like and now with vibe coding and everything else, there's going to be a lot more like like essentially software for just this task. Right. Right. That's part of where the infinite demand comes from because it's like no no actually this t like for example I want to create like for example when you think about create creativity like creating software for CG for movies well that was super expensive and so only a small number of very high-end very big budget Hollywood films can do that now the like for example documentaries are going to have access to all this because of vibe coding and so it's it's it's there's a ton of of of kind of software everywhere, right? Like like there's going to be like how do we make these mics more intelligent? Like as opposed to, oh, we have to be exactly this distance away from the mic in order to get the sound. It's like no, it's actually going to be having software that's going to be like I move a little bit away and it goes, oh, I pick it up a little bit. Right. Right. Like there's going to be software everywhere. Yeah. You talked about smart toothbrushes and super agency. Exactly. Yeah. Yeah. Totally. and and there's this long curve of problems that weren't economically viable before, but when all of a sudden the productivity per human skyrockets, all of those problems become economically viable. And then also as you build out the world, you have new problems. And I there was a CEO I was speaking with a week ago who said, \"Is the world in a perfect place? Is it perfect? Is it ideal?\" And if the answer is no, there are more problems. And and I really liked that framing of it. And the world's never perfect. There's always more problems, right? And especially when we go out to the stars and start exploring space even. Yeah. So, it's I'm I'm definitely an optimist like you. , all right. Let's talk about companionship. You have spent a lot of time thinking about this. You have Reed AI, , digital twins, AI companions like Meta's Celebrity AI, Gro's, Annie, and Valentine. , what specifically should platforms do to make sure that these AI companions are augmenting human relationships and not replacing them? There will always be some humans who will naturally head towards replacement.",
        "start": 2682.56,
        "duration": 5571.683000000002
    },
    {
        "text": "You have Reed AI, , digital twins, AI companions like Meta's Celebrity AI, Gro's, Annie, and Valentine. , what specifically should platforms do to make sure that these AI companions are augmenting human relationships and not replacing them? There will always be some humans who will naturally head towards replacement. invalidate the overall technology. It's just like there's some human beings that will get in a car and go deliberately hit somebody. And that doesn't mean no cars for anybody, right? We try to make it so that as few humans do that and then make it hard for using cars as weapons etc. Similar kind of thing there will be some individuals who will go I'm engaging with the agent I'm doing something like suboptimum for me suboptimum for other people that will happen. The fact that there is some doesn't invalidate it. Now that being said, part of what want to say is like on the macroeconomic you know basis there will be a set of different companies that will go hey I just want you to orient and talking to me and my bot and not going out being an a human in society in your community and everything else and I want to make that happen and that's actually in fact a bad thing whether that's you know the most obvious example as Twitter and Elon and you know creating you know kind of AI girlfriends and you know kind of pornography through the agent and all the rest of this as ways of doing which they are the leaders in in terms of doing and and frankly actually in fact I think what's see what's happening is a lot of of the other labs are going well look if you go work there we think you don't care about human welfare and so we don't necessarily want to hire you to our to our companies. It's almost becoming like a litmitness test is if you go work there that means that you're you're you're doing you don't have the right human alignment values that we want to create for folks because for example you should of course create AI companions. AI companions can be useful for all kinds of things and not just hey help help me figure out this medical condition help me figure out this legal issue. Help me learn things. Of course there's all that but by the way it's 11 p.m. I'm lonely. I'm unhappy. Hey, talking to the companion can be a perfectly good thing, right? It might it might help stabilize me. Now, you want it to be if you're think I'm thinking about self harm, it says, \"Hey, let's let's try to help you find people and and talk about like why you shouldn't do health self harm.\" and so, so I think a broad range of AI companions is a very very good thing, but it should be with a theory of how do",
        "start": 2829.119,
        "duration": 5851.9220000000005
    },
    {
        "text": "it says, \"Hey, let's let's try to help you find people and and talk about like why you shouldn't do health self harm.\" and so, so I think a broad range of AI companions is a very very good thing, but it should be with a theory of how do do you connect with other people around you? How do you connect with your community? And I think that's the kind of standard that we're going to want to see. Yeah. So, I mean, let's use your last example, , of of like, hey, it's 11 p.m. I'm lonely. Let me talk to an AI companion. It's hard for me to see the the the broad benefit given global population decline, loneliness epidemic, which I don't, you know, maybe it helps in the short term to have an AI companion, but it's certainly or or close to certainly going to take away from human companionship. How how do you kind of like u align those competing incentives? Well, so the thing you have to track it will take away from some human companionship, but there's a lot of times where people the reason I said 11 p.m. is a lot of times where people don't have that human companionship on tap. It's like if I'm sitting at my in my apartment by myself, it's 11 p.m. Would I be waking you up as I call you? Will you be, you know, is that going to be okay? I'm not quite sure if I'm ready to do that yet, etc. A companion can fit a very good night. Now, part of what we did in inflection and still does it. If you go to inflections pi, personal intelligence, and say, you know, you're my best friend. It says, no, no, no, I'm your companion. Let's talk about your friends. Have you seen your friends recently, etc.? So you like when you'd want that companion that you might start with you might start with pi at 11 p.m. and goes hey you know are your friends like in a time zone like maybe you're on the east coast and maybe you can call one of them who's on the west coast and it's only 8:00 p.m. and and and make that connection that might be helpful to you. Like you want the agent to be doing that like a lot of times like a teenager might go, \"Oh, I have no friends.\" Hey, actually, in fact, maybe you do. Let's help you think about who you're on the path to friendship with, who you could be building that with, etc. That's what you want the companions to be doing. And so it's not just the hey look I I had my best friend you know you know Mike or Sarah and I'm not going to call them because I'm going to call the agent. It's like no no no you want the agent kind of provoke gonna say hey have",
        "start": 2971.04,
        "duration": 6101.923999999998
    },
    {
        "text": "so it's not just the hey look I I had my best friend you know you know Mike or Sarah and I'm not going to call them because I'm going to call the agent. It's like no no no you want the agent kind of provoke gonna say hey have like that kind of thing and and that's a useful think of it as a set of different interface services. Those are useful interface services. doesn't mean it won't go wrong sometimes, but you want it to go right much more often than it goes wrong. Yeah. Yeah, that makes sense. And but I think to the movie Her, if if you saw that right, and when you're speaking with an AI that is literally built to reflect the exact type of personality that you want, the exact type of companion you want, you you might get sucked into that and you might start foregoing the human companionship aspect and maybe not all companies are going to be as responsible as inflection. And so, , Should there be maybe governmental oversight or or like how do how do you think about that? So part of the thing I advocate on regulation is to start with measuring like too often what happens like let me tell you how to design this like none of us know how these things are going to be designed even the leading experts don't know how to design it perfectly for the 5 10 year future which is what you're looking for in kind of regulation but you say hey I'd like to have a set of measurements on is your agent like for example this is one of the things you could do as a government regulatory agency today, you'd say, \"Hey, , we want you to propose to us some measurements of are your agents taking away from quality human interaction. We would like you to be running those measurements, right? And we'd like to know if those measurements are going up or going down. And if they're going up, then we're going to start engaging with what's going wrong, right? And by the way, we'll rei we'll re revisit every couple years, are these the right measurements or should there be other measurements for this particular challenge. And once we see them starting to go up and think the measurements are real, then if you're not fixing it, we're going to start imposing regulation. Yeah. A lot of what we're talking about requires a a severe amount of AI literacy, right? And so I I want to talk about that a little bit because schools are changing. I have you know two two young kids in in school right now and I'm I'm conflicted and so as a parent like what would you tell me are the best skills for me to tell my kids to learn and to help them learn whereas maybe it was something different",
        "start": 3097.76,
        "duration": 6388.082999999999
    },
    {
        "text": "have you know two two young kids in in school right now and I'm I'm conflicted and so as a parent like what would you tell me are the best skills for me to tell my kids to learn and to help them learn whereas maybe it was something different writing and and math and so how how what would you tell me what would you tell parents out there to have their kids focus on? Well, one AI literacy is going to really matter. So forbidding the use of AI is a bad mistake, right? And so engaging whether you're a school, a parent, etc. I think getting them to engage with AI is part of how learning now chat GBT already has a learning mode which is I think I think I don't know might be as simple as or study mode or whatever they call it as don't give the answer, help them find their way to the answer. And if you do that as a metapar prompt, all of a sudden even that basic thing makes these things into amazing learning agents. Yeah. And so, , that's the kind of process that we're going to want to be on. And yes, kids will figure out how to hack it, be lazy, say, just give me the answer and so forth. But by the way, look, here is one of the things that I think is an inevitable part of our human future, which is the way that education is going to be assessed is essentially by examination by AI. So you're going to go into kind of an AI booth and it's going to say like say for example, you produce a paper, great. What's going to happen is the AI is going to ask you about the paper. The way you're going to be exam is the AI is going to say great, you produced this paper, so why did you think this? And you're going to have to say why. oral assessments. Yeah. Right. And you're going to have to say why. And so actually in fact I think the standard for learning and the ability to learn in personalized customized way much higher ways as opposed to hacking the exam is now going to get much deeper because of AI. There may be some transition issues but the end state is going to be much much better for human cognition. Yeah. Okay. Okay. So, that's that's really interesting because take-home homework is, you know, there's a lot of students right now that are just using AI to write it up for them. And so, you're saying oral assessment is probably going to be a gold standard for seeing how a student is doing. Okay. Which we get infinite and free from AI. Right. Right. And then I'm conflicted. I I would prefer that my child's school don't use doesn't use screens or uses technology less. Yeah,",
        "start": 3242.079,
        "duration": 6657.204
    },
    {
        "text": "oral assessment is probably going to be a gold standard for seeing how a student is doing. Okay. Which we get infinite and free from AI. Right. Right. And then I'm conflicted. I I would prefer that my child's school don't use doesn't use screens or uses technology less. Yeah, screens all day every day. It's literally my job. And and so I'm quite conflicted on this and like I want my children to learn about artificial intelligence in the school. How how do you square these two different conflicting ideas that I don't want them using screens a lot, but also I do want them using AI in the context of the classroom. Well, if people haven't started using audio with AI, they must like it. It is it is really key. So, it's very easy to actually in fact have both devices but also interface points or that say H audio, right? So, as opposed to being locked into your screen, you know, you've got various kind of, you know, audio audio format. And so, that's that's one kind of hack that can do it. But also, I think that part of it is also the design of AI where the AI is saying, \"Hey, we're trying to like I think we want AI that says, \"Hey, we're trying to nudge you into productive interactions with other human beings.\" That's the that's the humanist design principle we have. I gave some speeches in in Italy and Bolognia and Peruia on this. I actually had Reed AI give that the Perua speech in Italian and Hindi and Chinese because it's about human connection. Yeah, I want to think about what might change your mind as an optimist. So, you're an optimist now. What would you have to see? Is there concrete ev evidence, technical, social, political, that would just say get you to say, \"Okay, we need to pause even if it were were possible.\" , what would that look like where you would change your optimism? Well, the change of optimism would probably be that we can't get the right kinds of leading groups building AI to be putting most of the right values in it. Right? So for example, if you said, hey, like I would get very alarmed if it was like Russia leading AI things or other kinds of entities doing that. The reason I'm like I'm not an optimist because I'm just like, oh, I'm just, you know, I was born an optimist and it's, you know, it's just my character. It's a rational reasoned optimism. It's a question of you know I've spent the last you know over a decade talking to a lot of the heads of the kind of both the western democracy labs and also some of the Chinese labs and it's like okay there is a humanist focus in many of these that's good it's the shift of that that would make me more alarmed and that doesn't",
        "start": 3380.72,
        "duration": 6989.522999999999
    },
    {
        "text": "a lot of the heads of the kind of both the western democracy labs and also some of the Chinese labs and it's like okay there is a humanist focus in many of these that's good it's the shift of that that would make me more alarmed and that doesn't think there aren't commercial blind spots and things to do. But but by the way, the way that you prevent bad AI is creating good AI, not by, you know, you know, trying to drive down the highway at 3 miles per hour. Yeah. , in your book, Super Agency, you talk a lot about iterative deployment, which is the approach that Open AI took. You're very early backer. I do you think there's potential for kind of a singular catastrophic misuse of artificial intelligence or or are we on this kind of progressive curve that it's just going to get it iteratively better over time? So there's certainly possibilities of bad use, bioteterrorism, you know, other kinds of things that you want to try to maximize the the the or minimize the possibilities of, right? Maximize the the defense against. Yeah. but I think iterative deployment is the best way to get there. Like the normal instinct is to say like for example, you say I am building a car and I want to make sure that there are zero highway fatalities before I launched my car. You'll never launch a car. you'll create this thing that's like a five-tonon monstrosity with a with a 10-ft steel bumper, you know, that only goes 10 miles an hour, you know, etc. And I was like, okay, that just just doesn't work. You have to do it in deployment to go, oh, that's what gets us to airbags. That's what gets us to crumple zones. That's what gets us to and and you'll have some accidents. You try to have just less of the really catastrophic ones, right? But I think the point is if you are doing iterative deployment and you make one mistake where catastrophic instances happen then it's like maybe iterative deployment wasn't the right way. Not saying I personally disagree but I'm just wondering how but the problem is is is there's no way some people argue I disagree with them to to go into the future without some taking of risk. But like here here's so one of the mistakes that people make and this is called existential risk exisk they make is they say can you guarantee me that humans or robots of themselves won't create like a terminator-l like robot of some sort. It's like nope can't guarantee that neither humans nor lower accidents where robots will do that. They say well then shouldn't you just stop? You say, \"Well, your intellectual mistake is looking at existential risk as a singular thing about an increase of probability that there could be a Terminator-like robot.\" Yep. In AI, there's that increase. But",
        "start": 3549.44,
        "duration": 7299.044000000001
    },
    {
        "text": "nor lower accidents where robots will do that. They say well then shouldn't you just stop? You say, \"Well, your intellectual mistake is looking at existential risk as a singular thing about an increase of probability that there could be a Terminator-like robot.\" Yep. In AI, there's that increase. But think of of defending against both natural and mad man-made pandemic, right? That's another exential risk. I can see it really improving like issues with climate change. I could see it tracking asteroid and asteroids like identifying the asteroids like in a time that we could do something about it. So I think the existential portfolio existential risk portfolio gets substantially improved with AI even when you have the terminator risk. Yeah. Right. And that's the reason I'm bullish on it. And that's the reason why I'm like, \"No, no, creating AI reduces overall existential risk as we steer it, even if I cannot res remove the terminator risk to zero.\" And that's part of the reason I'm positive on creating AI and positive on taking that existential risk because it decreases our overall existential risks. Well, Reed, thank you so much for joining me. This has been awesome. Really appreciate it. Pleasure. I look forward to it. , and fun doing this here in Seattle. Yeah, thank you very",
        "start": 3705.52,
        "duration": 7419.284000000001
    }
]