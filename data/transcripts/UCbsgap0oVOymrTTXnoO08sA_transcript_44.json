[
    {
        "text": " Manus just pushed its agents closer to real autonomy with a major new release and Nvidia moved deeper into open source by securing critical AI infrastructure and launching new agent focused models. All right, let's start with Manis because this update is about what happens at the task level when an AI agent is actually asked to do something endtoend. So they just rolled out version 1.6 and the core idea behind this release is pretty simple. Finishing a task is easy to demo. Finishing it correctly, autonomously, and with a high success rate is the hard part. This update is focused on closing that gap. The company reworked the core agent architecture so it can plan better, reason better, and complete more complex workflows with significantly less human supervision along the way. The release introduces three major changes, and they're not small tweaks. The first is Mayus 1.6 Max, which is now their most powerful agent. The second is full mobile development support which expands manis beyond just web- based projects. And the third is something called design view which adds an interactive visual layer to how images are created and edited. The center of gravity here is manis 1.6 max. This is now the flagship agent and it's built on a more advanced architecture specifically designed for planning and problem solving. What that means in practice is fewer interruptions, fewer corrections, and fewer cases where a task stalls halfway through and needs manual fixing. One of the biggest improvements they're highlighting is oneshot task success. More tasks now finish autonomously from start to finish without needing human intervention. That's a big deal, especially for people using agents for real work instead of demos. This jump comes directly from improved reasoning and better internal planning, not just faster execution. They also ran double blind testing around user experience and the results showed user satisfaction increased by over 19.2%. That number comes from people rating the quality of outputs, the accuracy of results, and how reliably the agent used tools without breaking workflows. In other words, it's not just smarter, it's more dependable, which matters a lot more once you move beyond experimentation. Another area that got a meaningful upgrade is wide research. In Mayus, wide research uses multiple sub aents running in parallel to explore different angles of a topic. With this update, all of those sub aents now run on the max architecture. So instead of having one strong planner and weaker parallel workers, every branch of the research process operates at the highest level. That leads to deeper analysis, better cross-checking, and fewer weak links in the final output. Spreadsheets are another place where Max shows a noticeable jump. Manis is now much stronger at handling spreadsheet heavy workflows, including complex financial models, advanced data analysis, and automated report generation. These are tasks that usually require a lot of manual expertise and constant checking. With Max, those workflows become more reliable and less fragile, especially",
        "start": 2.639,
        "duration": 342.3999999999999
    },
    {
        "text": "shows a noticeable jump. Manis is now much stronger at handling spreadsheet heavy workflows, including complex financial models, advanced data analysis, and automated report generation. These are tasks that usually require a lot of manual expertise and constant checking. With Max, those workflows become more reliable and less fragile, especially calculations or large data sets. Web development also got a visible upgrade. The improvements here go beyond just generating code. Max produces better UI aesthetics, more functional layouts, and smoother interactive experiences. That matters because many internal tools live or die based on usability, not just whether the code technically works. With this update, you can build things like internal apps that parse uploaded invoices, automatically generate editable forms, and surface summary statistics, all with interfaces that feel polished instead of hacked together. Manis is also now creating detailed multi-dimensional feature comparison matrices for competitor analysis. These aren't shallow sideby-side tables. They're structured comparisons that capture nuance across multiple dimensions, which makes them much more useful for strategic decisions. Internally, Manis benchmarks its agents using scenarios designed to mirror realworld usage, not abstract test cases. According to those benchmarks, Manis 1.6 Six Max shows performance gains across the board with the biggest improvements showing up in complex multi-step tasks that require both accuracy and sustained reasoning. That's the type of work where weaker agents tend to fail silently or drift off course. Max holds the thread much better. All of this leads to a simple but important outcome. Tasks that previously took multiple attempts can now finish successfully in a single run. that saves time, reduces friction, and makes AI agents feel more like actual collaborators instead of tools that constantly need babysitting. The second major addition in Mayus 1.6 is mobile development. For the first time, Mayus can now build mobile applications, not just web apps. You describe the app you want and the agent handles the endtoend development process. Combined with the improved web development stack, this massively expands the range of projects you can realistically attempt with the platform. This matters because many products never live purely on the web. Mobile apps are often the primary interface and being locked out of that space limits what AI agents can actually deliver. With mobile development now supported, Manis becomes much more flexible for startups, internal tools, and product experiments that need to ship across platforms. The third piece of the update is design view. And this one changes how visual work is done inside Manis. Instead of relying only on text prompts, Design View gives you an interactive canvas for image creation and editing. You can make precise local changes to specific parts of an image using point-and-click controls. You can add or modify text directly inside images with highquality rendering. You can also combine multiple images into more complex compositions. What's important here is control. Design view gives users fine grained control similar to traditional design software, but it's still powered by modern image generation",
        "start": 174.16,
        "duration": 680.561
    },
    {
        "text": "using point-and-click controls. You can add or modify text directly inside images with highquality rendering. You can also combine multiple images into more complex compositions. What's important here is control. Design view gives users fine grained control similar to traditional design software, but it's still powered by modern image generation makes it much easier to iterate visually without constantly fighting the prompt loop. All of this is available now. Manis 1.6, including the Max agent, mobile development, and design view is live for all users. When starting a new task, you can select your preferred agent, including Max. As part of the rollout, the Max agent is also available at a 50% reduced credit cost for a limited time, which lowers the barrier to actually testing these improvements in real workflows. That's Manis. It's an update focused on autonomy, reliability, and expanding what agents can actually build. Now, let's talk about Nvidia because they just made a move deep in the AI stack, and it carries some serious implications. So, Nvidia announced that it has acquired Skd, the company behind Slurm, one of the most widely used open-source workload management systems in high performance computing and AI. Slurm has been around since 2002, and Skedmd was founded in 2010 by Morris Jet and Danny Aubel, the lead developers behind Slurm. Able is currently the CEO of Skedmd. If you've ever worked around large-scale compute clusters, slurm is foundational infrastructure. It's the system that decides how jobs are scheduled, how resources are allocated, and how massive workloads are coordinated across thousands of machines. NVIDIA has been working with SCEMD for more than a decade, and it considers SLURM critical infrastructure for generative AI. NVIDIA says SKED MD will continue operating SLURM as open-source vendorneutral software. That's a key detail. The company plans to keep investing in the technology and accelerate its ability to integrate with different systems. The terms of the acquisition were not disclosed and Nvidia declined to comment beyond its official blog post. This move reflects how seriously Nvidia is taking the software layer around AI, not just the hardware. GPUs matter, but orchestration matters just as much once you scale. Slurm sits right at that intersection. Alongside the acquisition, Nvidia also released a new family of open AI models called Nvidia Neotron 3. The company describes Neotron 3 as the most efficient family of open models for building accurate AI agents. These models are designed with agentic systems in mind, not just single turn chat. The Neotron 3 family includes three models. Neatron 3 Nano is a small model intended for targeted tasks where efficiency matters most. Neatron 3 Super is built for multi- aent applications where multiple AI systems coordinate or interact. and Neotron 3 Ultra is designed for more complex tasks that require deeper reasoning and broader capabilities. In the press release, Nvidia CEO Jensen Huang emphasized that open innovation is foundational to AI progress. He framed Neotron as a way to",
        "start": 344.96,
        "duration": 1001.2
    },
    {
        "text": "aent applications where multiple AI systems coordinate or interact. and Neotron 3 Ultra is designed for more complex tasks that require deeper reasoning and broader capabilities. In the press release, Nvidia CEO Jensen Huang emphasized that open innovation is foundational to AI progress. He framed Neotron as a way to that gives developers transparency and efficiency when building agentic systems at scale. This release fits into a broader pattern. Over the past several months, Nvidia has been steadily expanding its open source and open AI offerings. Just last week, the company announced Alpameo R1, an open reasoning vision language model focused on autonomous driving research. Around the same time, Nvidia also added more workflows and guides for its Cosmos world models, which are open- source under a permissive license and aimed at helping developers build physical AI systems. Physical AI is where Nvidia is clearly placing its bets. Robotics, autonomous vehicles, and embodied systems all require massive amounts of compute, robust scheduling, and efficient models that can operate reliably in real environments. NVIDIA wants to be the default supplier, not just for GPUs, but for the entire AI stack that powers these systems. That's it for this one. Thanks for watching. Leave your thoughts in the comments, and I'll catch you in the next one.",
        "start": 507.12,
        "duration": 1119.3610000000006
    }
]