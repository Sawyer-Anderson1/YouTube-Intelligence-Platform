[
    {
        "text": " [Music] Unit's G1 humanoid robot has just done something nobody expected. It played table tennis fully autonomously. Not a pre-programmed arm swing, not a stage trick, but realtime rallies powered by advanced AI and whole body coordination. This moment marks one of the biggest leaps yet in humanoid robotics. And the system behind it called Hitter is proving that robots are finally learning how to master human level agility in one of the world's fastest sports. Now, why is this such a big deal? Because table tennis isn't just a casual game. At a competitive level, it's one of the most demanding sports on the planet, pushing human reflexes to the limit. A ball can travel at speeds of up to 70 mph with spins that make it curve unpredictably. Players often have less than 200 milliseconds to react. For a humanoid robot to return shots at this speed while keeping its balance, moving fluidly, and coordinating its entire body is something researchers have been chasing for decades. And now, thanks to Unitry's G1 powered by the Hitter system, we've finally seen it happen. So, let's break down how the G1 pulled this off. The magic lies in the hitter system, which combines two key elements. a model-based planner and a reinforcement learning RL whole body controller. The model-based planner is basically the strategist. It receives information about the ball's position and movement, then predicts where it's going to be in the future. With that prediction, it decides the optimal strike point, the speed of the swing, and the angle needed for a successful return. Think of it as the robot's internal coach, telling it how to set up the perfect shot. Once that plan is made, the RL controller takes over. This is the part trained through reinforcement learning, a process where the robot practices thousands of movements in simulation, learning through trial and error what works and what doesn't. By the time it steps onto the table tennis court, it has developed a natural sense of timing and coordination. The result is shockingly humanlike motion. The robot bends its knees, shifts its torso, rotates its waist, and swings its arm in one fluid motion. It's not stiff or robotic. It actually looks like an athlete making a calculated return. Now, the G1 isn't using its own built-in vision just yet. Instead, it relies on a 9 camera optit motion capture system. This setup covers the entire play area, tracking the ping pong ball in three dimensions using reflective markers attached to it. Every movement of the ball is captured in micros secondsonds, and that data is sent to the planner. With this setup, the robot can predict exactly where the ball will bounce, how fast it's moving, and the perfect position it needs to reach to hit it. Some might call this a crutch. And yes, it's not the same as onboard vision, but it's still a critical step forward. Why? Because the",
        "start": 2.33,
        "duration": 361.99
    },
    {
        "text": "the robot can predict exactly where the ball will bounce, how fast it's moving, and the perfect position it needs to reach to hit it. Some might call this a crutch. And yes, it's not the same as onboard vision, but it's still a critical step forward. Why? Because the just seeing the ball. It's moving the body fast enough and precisely enough to make contact with it. That's what this demo proved. The G1 has the mechanical skill, balance, and reaction time to handle a game as fast and unpredictable as table tennis. You might wonder, why choose table tennis as the test case? It's simple. It's one of the toughest possible challenges for embodied AI. Think about what the sport demands. You need lightning fast reaction times since the ball can go from one side of the table to the other in less than half a second. You need trajectory prediction because spin changes everything. Sometimes the ball curves toward you. Sometimes it dips suddenly after the bounce. And you need full body coordination since you're not just swinging an arm. You're moving your feet, shifting your hips, bending your knees, and maintaining balance the entire time. For decades, experts believe that humanoid robots couldn't handle this level of complexity anytime soon. But now, Unit's G1 has shown that with the right system, it's possible. Table tennis isn't just a game here. It's a proving ground for high-speed, highstakes embodied AI. So, what does it actually look like when the G1 plays table tennis? When the ball comes over the net, you see the robot shift into position almost instantly. Its legs adjust to get the right stance. Its waist rotates and then the arm comes through with a clean, controlled swing. The paddle connects with the ball at just the right moment, sending it back across the table with the correct angle and velocity. What's most striking is how natural it looks. The motions aren't jerky or robotic. They flow in a way that's unmistakably humanlike. In some clips, if you didn't know better, you might even mistake it for a person in a robotic suit. That's how fluid the coordination is. And this isn't a one-off stunt. The system is able to rally consistently, not just hit a single return. That's what makes this such a game changer. Of course, we should talk about the limitations. The most obvious one is the external camera system. In real world conditions, you can't attach reflective markers to every ball or set up a nine camera motion capture array around every table. For robots to truly master table tennis or any high-speed sport, they'll need onboard vision systems that can track the ball themselves. That means equipping humanoids with fast cameras, better depth perception, and more powerful onboard processors capable of handling the enormous flood of visual data in real time. It's a massive challenge, but the foundation is already here. Another limitation is decision-m.",
        "start": 183.2,
        "duration": 683.9100000000004
    },
    {
        "text": "onboard vision systems that can track the ball themselves. That means equipping humanoids with fast cameras, better depth perception, and more powerful onboard processors capable of handling the enormous flood of visual data in real time. It's a massive challenge, but the foundation is already here. Another limitation is decision-m. returning the ball successfully. But in human table tennis, there's more to the game. Deciding whether to play offensively or defensively, adjusting strategies mid rally, and even anticipating your opponent's tactics. Those higher level decisions are still beyond robots for now. But as we've seen with other reinforcement learning breakthroughs, once the physical capabilities are proven, the strategy layer comes next. So, it's only a matter of time before humanoid robots don't just return the ball, but play intelligently. Here's the real reason this breakthrough matters. It's not just about robots playing pingpong. It's about proving that humanoid robots can perform fast, dynamic, realtime actions in environments that require both agility and precision. If a robot can track a ball flying at 20 m/s, predict its path, move its body into the right position, and strike at exactly the right moment, all while staying balanced. Imagine what else it could do. Think about factory work, where timing and precision are everything. Think about assisting in hospitals, where robots might need to react quickly to a falling object or a sudden patient movement, or even disaster recovery, where a robot needs to move through unstable terrain while making real-time adjustments. The skills needed to play table tennis, perception, prediction, and coordinated movement are the same skills that will allow robots to thrive in countless real world environments. That's why this moment is so important. Now, it's worth comparing this to what came before. Industrial robots have been able to hit ping-pong balls for years, but those setups were stationary robotic arms bolted to the ground with pre-programmed swings. They weren't humanoid, and they weren't autonomous in the sense of reacting naturally to the game. On the humanoid side, we've seen robots hold paddles and swing once or twice, but those movements were slow, clunky, and nowhere near the speed of real play. They were more like demonstrations than actual games. The G1 with hitter changes everything. This is the first time we've seen a humanoid robot actually rally in real time, moving its entire body in coordination to hit shots with humanlike fluidity. It's a milestone that sets a new bar for humanoid robotics. One of the most exciting parts of this system is the reinforcement learning controller. Reinforcement learning is all about trial and error. The robot tries different movements in simulation, gets rewarded for successful actions, and gradually learns the optimal way to move. For the G1, this meant learning how to swing its arm, adjust its torso, bend its knees, and time everything perfectly to hit the ball without losing balance. Over time, it developed motions that look surprisingly athletic, even though the robot doesn't move exactly",
        "start": 346.16,
        "duration": 1027.4280000000003
    },
    {
        "text": "learns the optimal way to move. For the G1, this meant learning how to swing its arm, adjust its torso, bend its knees, and time everything perfectly to hit the ball without losing balance. Over time, it developed motions that look surprisingly athletic, even though the robot doesn't move exactly That's the beauty of RL. It doesn't just copy human motion. It creates movement strategies optimized for the robot's own mechanics. In some cases, that might even make it better than humans at certain tasks. Looking forward, the big step will be making this system work without external motion capture. Onboard vision is the holy grail here. And if unitry manages to integrate it successfully, we could see robots rallying under fully natural conditions. From there, strategy will come into play. Imagine a humanoid robot that doesn't just return the ball, but chooses between a defensive block, a top spin counter, or even a smash depending on the situation. That would be the next level of impossible. And beyond table tennis, unitry could adapt the hitter system to other high-speed activities. Basketball, baseball, or even martial arts. Anywhere quick reactions and whole body coordination are needed, this system could shine. This isn't just about table tennis. It's about the future of humanoid robotics. Because if a robot can rally at the speed of pingpong, it can handle countless other challenges in the real world. The line between human agility and robotic skill is getting thinner by the day. And if this is what we're seeing in 2025, just imagine what the next few years will bring. But Unit's G1 isn't the only robot shocking the world right now. While humanoids are proving they can handle lightning fast sports like table tennis, quadripeds are rewriting the rulebook in their own way. And the latest updates from Boston Dynamics and Deep Robotics just proved it. Robot dogs are stepping into a whole new era of agility and power. These aren't just small tweaks or minor upgrades. We're talking about fundamental breakthroughs that completely change how quadriped robots move, adapt, and perform in the real world. Spot is pushing the boundaries of agility and control, showing off aerial moves that look like something out of an athletes playbook. Meanwhile, Lynx M20 takes a bold design step that redefes versatility for industrial robots, proving that quadripeds are now capable of tackling environments that once seemed impossible for machines. Together, these two announcements signal something big. Robot dogs are moving beyond research experiments and flashy demos. They're becoming serious, practical machines with realworld applications that go far beyond novelty. Let's break down exactly what's happening here. Starting with Boston Dynamics's jaw-dropping update. Spot has been around for years now, and it's arguably the most famous robot dog in the world. It's danced to music, climbed stairs, carried loads, and performed warehouse inspections with ease. But as impressive as those abilities were, everything we'd seen until now was grounded. Literally.",
        "start": 520.399,
        "duration": 1380.2290000000003
    },
    {
        "text": "Dynamics's jaw-dropping update. Spot has been around for years now, and it's arguably the most famous robot dog in the world. It's danced to music, climbed stairs, carried loads, and performed warehouse inspections with ease. But as impressive as those abilities were, everything we'd seen until now was grounded. Literally. creature of the floor. That changed when Boston Dynamics revealed their latest research. Using reinforcement learning, an AI method where the robot learns behaviors through trial and error. Spot has unlocked stunning aerial capabilities. Suddenly, this groundbased quadriped is performing midair moves with control and precision that nobody thought possible. Spot can now leap higher than ever, stabilize itself midair, and land with an accuracy that looks more like an athlete or gymnast than a robot. Engineers describe it as acrobatic locomotion. And when you watch it in action, it feels almost surreal. For the first time, Spot isn't just adapting to the ground. It's mastering the air in brief, controlled bursts. This is more than just a flashy stunt. Controlled aerial motion gives Spot access to environments and scenarios that would have been impossible before. Imagine a disaster zone with gaps, rubble, or collapsed terrain. A robot that can leap, twist, and land safely can handle challenges that wheels, or even traditional quadripeds simply can't manage. What's remarkable here is that engineers didn't sit down and manually code every move. Reinforcement learning allowed Spot to teach itself how to optimize its movements for stability and control. The robot essentially experimented until it figured out physics-based strategies that even its creators wouldn't have thought of. That's the magic of RL. It produces solutions that are both creative and practical, giving robots a level of adaptability that feels almost human. Boston Dynamics has long been known for attention-grabbing videos, but this time it feels different. These aerial behaviors aren't about making people laugh or gasp at a dance routine. They're about giving Spot the ability to perform tasks that make it more useful, more reliable, and more versatile in the real world. To understand why this breakthrough matters, it helps to look at what reinforcement learning actually does. Instead of following rigid instructions, Spot is guided by a simple system of rewards and penalties. If a move brings it closer to success, say landing on its feet or maintaining balance, it gets rewarded. If it fails, it gets penalized. Over thousands of repetitions, Spot gradually learns what works best. This is the same type of machine learning that's behind some of the most advanced AI systems in the world. From game-playing bots to autonomous vehicles, in robotics, reinforcement learning is especially powerful because it allows machines to adapt in ways that can't be pre-programmed. For Spot, it means discovering leg and body movements midair that maximize control. Engineers might not know the exact angles or timings required for perfect landings, but Spot can figure it out through practice. The result is a robot dog performing moves that look like instinct",
        "start": 698.8,
        "duration": 1726.5489999999998
    },
    {
        "text": "that can't be pre-programmed. For Spot, it means discovering leg and body movements midair that maximize control. Engineers might not know the exact angles or timings required for perfect landings, but Spot can figure it out through practice. The result is a robot dog performing moves that look like instinct AI training. This development could ripple far beyond spot itself. If reinforcement learning can unlock aerial dynamics for a quadriped, what could it do for humanoids, warehouse robots, or industrial arms? The idea that machines can teach themselves to handle physics better than humans can program them opens the door to a new generation of adaptive robots. Spot is just the beginning. While Boston Dynamics is turning heads with aerial tricks, Deep Robotics is tackling the problem of adaptability in a completely different way. Their new Lynx M20 robot dog might not be leaping midair, but it's bringing something just as groundbreaking. a unique front elbow, back knee design that changes how quadripeds function. Most robot dogs follow a fairly standard leg orientation inspired by animals. Deep Robotics flipped that formula, giving the Lynx M20 a hybrid structure that combines both elbow-like and kneelike joints. The result is a robot that can crouch, climb, and stabilize itself in ways traditional designs can't. This design choice isn't just clever, it's practical. The Lynx M20 is built as an industrial-grade robot meant for environments where ruggedness and adaptability matter most. Picture it navigating construction sites, inspecting pipelines, or traversing uneven terrain in search and rescue missions. The hybrid leg design allows it to handle obstacles and adjust its posture dynamically, giving it a level of flexibility that's hard to match. In other words, while Boston Dynamics is pushing agility to the extreme, Deep Robotics is reimagining versatility, the Lynx M20 is less about showing off and more about delivering reliable performance in environments where robots have to be tough and adaptable. What's fascinating here is that these two updates represent very different visions of what robot dogs should be. Spot's new aerial maneuvers push the limits of athleticism and AIdriven motion, making it an elite performer in scenarios where mobility is everything. Link's M20, on the other hand, takes a design first approach, showing how clever engineering can make robots more useful in tough industrial contexts. Both directions are essential for the future of robotics. Spot demonstrates how reinforcement learning can unlock unexpected abilities that expand what robots are capable of. Links M20 proves that sometimes the most important breakthroughs come from rethinking hardware design for realworld use. Together, they're painting a picture of a future where robot dogs aren't just lab experiments, but mainstream tools across industries. Both Spot and Lynx M20 highlight a key truth. Robot dogs are no longer just novelties. They're becoming serious tools with limitless applications. For Spot, the aerial behaviors mean it could play a critical role in scenarios where humans can't go. Disaster response, military operations, or even planetary exploration suddenly look a lot more",
        "start": 874.639,
        "duration": 2094.7079999999987
    },
    {
        "text": "M20 highlight a key truth. Robot dogs are no longer just novelties. They're becoming serious tools with limitless applications. For Spot, the aerial behaviors mean it could play a critical role in scenarios where humans can't go. Disaster response, military operations, or even planetary exploration suddenly look a lot more can leap, twist, and land with such precision. Reinforcement learning ensures it won't just repeat the same moves. It will adapt in real time to new challenges. For Lynx M20, the industrial-grade design makes it a natural fit for industries that need tough, reliable robots. Oil and gas, mining, construction, and logistics could all benefit from a quadriped that handles terrain with clever joint mechanics. Its unique body design could also inspire new generations of robots, showing that innovation doesn't always have to look flashy to be game-changing. And let's not forget the ripple effect. As companies like Boston Dynamics and Deep Robotics push these boundaries, smaller startups and research groups around the world are inspired to experiment. That accelerates the entire field of robotics, meaning we're likely to see even more radical updates in the months and years to come. Boston Dynamics spot performing controlled aerial maneuvers is something we never thought we'd see from a quadriped robot. And it's all thanks to reinforcement learning teaching it moves beyond human imagination. At the same time, Deep Robotics Links M20 is proving that bold, practical design choices can give robots the rugged versatility needed for realworld deployment. These two announcements happening side by side show us that the race to perfect robot dogs isn't slowing down, it's accelerating. And while engineers may be shocked by what they're seeing today, it's clear that we're only scratching the surface of what these machines will be able to do in the years ahead. [Music]",
        "start": 1061.039,
        "duration": 2286.0059999999994
    }
]