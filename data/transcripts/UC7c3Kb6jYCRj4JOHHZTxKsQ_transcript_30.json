[
    {
        "text": " Google aims to put massive AI data centers in space. This is not science fiction. This is something they are actually working on. This is called project starcatcher. And the gist is they want to put data centers in space. They want to connect the data centers with satellites and they want to power the satellites with solar energy. So here are the interesting bits from this announcement. In the right solar orbit, a solar panel can be up to eight times more productive than on Earth. So, as solar panels continue to get better, to get more efficient, then you couple that with actually putting them in space where they are up to eight times more productive than being on Earth. All of a sudden, you have plentiful energy. And that's the key. Down here on Earth, it seems like the biggest bottleneck is energy production, especially in the United States. Apparently, there are data centers being built out and completed that can't even be powered properly. Satia Nadella just talked about his data center that is built out and some of the GPUs can't even be turned on yet because they simply don't have the power. And so, this futuristic vision of putting data centers in space could solve it. But, of course, when you're training models, when you're running inference on models, there needs to be high data throughput. That means all of the chips need to communicate with each other very very quickly. And if you're putting data centers in space and there's a lot of distance between them, that might get very hard, especially because you can't simply hardwire them together. So instead, they're going to use satellites closely coupled clusters of these satellites to beam signal between the different arrays. According to the announcement, this approach would have tremendous potential for scale and also minimizes impact on terrestrial resources. So, we're just getting started building out these massive data centers here on Earth. And Google and many others at this point are already thinking about how do we actually build them in space? And for those of you thinking this sounds ridiculous, Google has a long history of making huge bets on very futuristic technology. Remember, Whimo, the autonomous driving fleet that is in so many different cities and works incredibly well, was a moonshot. It was one of these projects that they bet on very early, 15 years ago, and basically built it from nothing. And now it's becoming a major business and is going to change the world. This could be another one of those. But there's so much behind this. They started already developing the math and the physics to simulate whether this is even possible. High bandwidth intersatellite links require our satellites to fly in a much more compact formation than any current system. We developed numerical and analytic physics models to analyze the orbital dynamics of such a constellation. And here's what this",
        "start": 0.08,
        "duration": 348.08000000000015
    },
    {
        "text": "math and the physics to simulate whether this is even possible. High bandwidth intersatellite links require our satellites to fly in a much more compact formation than any current system. We developed numerical and analytic physics models to analyze the orbital dynamics of such a constellation. And here's what this are all of the satellites kind of moving in unison to maximize the energy captured from the sun. And for those of you wondering, each of these satellites needs to be positioned only a few hundred meters apart. That is very close when you're talking about objects that are very large, the size of a bus, let's say, and moving at incredible speeds. Then you have to worry about the radiation. Remember, the sun puts off a tremendous amount of radiation, most of which is captured by Earth's atmosphere before it hits us. But if you're above Earth's atmosphere, you don't get that benefit. And so, of course, radiation may affect the chips. It actually probably would. But they tested that. For ML accelerators to be effective in space, they must withstand the environment of low Earth orbit. We tested Trillium, Google's V6e cloud TPU, that is their high-end custom silicon for running AI in a proton beam to test for impact from total ionizing dose and single event effects. The results were promising and so these chips were lasting a pretty sufficient amount of time even though they were being blasted by radiation. And then there's another problem. How about the incredible costs it takes to get all of this Earth material to orbit? Historically, high launch costs have been a primary barrier to large-scale space-based systems. However, our analysis of historical and projected launch pricing data suggests that with a sustained learning rate, prices may fall to less than $200 per kilogram by the mid 2030s. And of course, SpaceX is pushing a lot of that pricing downwards. When we start to get reusable rockets, which SpaceX is working on, we could see a 10 or even 100fold decrease in the price of getting materials to space. I love stuff like this. It makes me so excited about the future. I love big bets, especially with companies that have the coffers to bankroll it at the scale it really needs. So, this is awesome. Google, keep up the great work. There are so many AI tools out there, it becomes incredibly expensive to get access to them all. Trust me, I know. And there's a better way. With Chat LLM by Abacus AI, you get access to chatbt5, 03 Pro, Gemini 2.5 Pro, Claude 4.1, Gro 4, Deep Seek, and many others. And with Route LLM, it automatically picks the best model for whatever prompt you give it. And you get the latest image generation models like seedream, nano banana, ideoggram, flux and more. And you can even generate video, cling, runway, helio with just a simple prompt. And their humanized feature turns obviously AI written text",
        "start": 174.319,
        "duration": 682.6389999999997
    },
    {
        "text": "automatically picks the best model for whatever prompt you give it. And you get the latest image generation models like seedream, nano banana, ideoggram, flux and more. And you can even generate video, cling, runway, helio with just a simple prompt. And their humanized feature turns obviously AI written text it goes from boring AI generated to something unique and compelling. And last, they also have Deep Agent, which is a powerful general agent that can build websites, apps, AI agents, chat bots, presentations, and complete research reports. And all of this is just $10 per month. Visit their website by clicking the link down below. That lets them know I sent you. And thanks again to Abacus. Now, back to the video. Next, speaking of data centers, Open AI is on a massive tear, signing absolute monster deals for compute infrastructure. According to the Kobayi letter, I think I'm pronouncing that right. OpenAI has now signed a $500 billion Stargate deal, hundred billion Nvidia deal, a hundred billion AMD deal. They just signed a deal with Amazon, which I'm going to go into more detail in a moment. Stick around for that. an Intel deal, TSMC deal, Microsoft, Oracle, and so on and so forth. And they are considering a trillion dollar IPO soon. So, when rumors first started happening that Sam Alman and OpenAI were looking to raise $7 trillion, a lot of people thought it was either fake or he's just completely out of his mind and this is just unnecessary. But, it looks like he's well on his way to achieving that number in a pretty short record amount of time. And here's what I wanted to talk about with the AWS deal. The week after Microsoft lost its right of first refusal, OpenAI went straight to Amazon AWS to sign a deal. So, let me break that down for you. Just about a week ago, Open AAI finalized their restructuring of their corporate entities. Basically, we have a nonprofit that owns the now public benefit corporation, which is a for-profit. And in this finalization to a for-profit company, they also ironed out the details of their new Microsoft partnership. Now, previously, Microsoft got the right of first refusal of all compute that OpenAI would ever need. And so, if they needed to buy compute, they had to go ask Microsoft first, basically. And as part of the restructuring, Microsoft still has a huge percentage of Open AI, but they lost the right of first refusal. And literally days after that finalized, they announced a massive partnership with AWS because now they're going to go to AWS and get their compute. It's so interesting to see all of this play out. All of these frontier lab companies just begging, borrowing, and stealing to get the compute they need to run their models, to run their use cases. And ultimately, it just seems like demand far outstrips supply as of now. Next, it",
        "start": 344.16,
        "duration": 997.2799999999996
    },
    {
        "text": "so interesting to see all of this play out. All of these frontier lab companies just begging, borrowing, and stealing to get the compute they need to run their models, to run their use cases. And ultimately, it just seems like demand far outstrips supply as of now. Next, it version of Siri from Apple. According to The Verge, Apple is planning to use a custom version of Google Gemini for Apple Intelligence. The iPhone maker will reportedly pay Google around a billion dollars annually to gain access to the technology which will generate summaries and perform planning related tasks. Apple will run the custom Gemini model on its private cloud compute servers. Now, that is the really interesting bit. Apple has incredible infrastructure. They have their private cloud compute thing and it's not going to be running through Google. They are going to control the model. They're going to control the inference. They're going to be able to optimize it for their own use cases and they're going to pay a billion dollars per year for the license to do so. So, it was reported in June that Apple had weighed using models from OpenAI or Enthropic to power the new Siri before reportedly settling on a partnership with Google. And remember, Apple and Google have had a long partnership. Google actually pays Apple to be the default search engine, and they pay quite a pretty penny for that. But they also have a partnership with Open AI. When they first announced Apple Intelligence, if you needed to ask Apple Intelligence anything beyond a simple question and answer, it would throw it to a third party, that third party being Open AI's models. But it was a clunky experience. It would tell you, \"Okay, I need to go to OpenAI. Are you okay with that?\" and it was just not a good experience. So having a Frontier model natively in Apple Intelligence is going to be very welcome. Last week, Apple CEO Tim Cook told investors that the new Siri is set to arrive next spring. All right, this is a very OpenAI heavy episode, but of course they're just shipping like crazy and I got to talk about it. So next, OpenAI shipped something that is just such a quality of life improvement. You can now interrupt longunning queries and add new context without restarting or losing progress. That is phenomenal. If you've ever hit enter and just felt that sink in your stomach like, \"Oh, I forgot to do something. Now I'm going to have to stop it and rephrase it and it's going to have to start over.\" No more. You simply add the context. It will pause and then it will continue from where it left off. This is especially important for things like deep research and GPT5 Pro where stopping it could mean restarting a 10, 15, 20 minute process. So, this is very cool. Glad to see it. All right. Next,",
        "start": 503.12,
        "duration": 1276.9609999999998
    },
    {
        "text": "context. It will pause and then it will continue from where it left off. This is especially important for things like deep research and GPT5 Pro where stopping it could mean restarting a 10, 15, 20 minute process. So, this is very cool. Glad to see it. All right. Next, generation model. And no, it's not open- source or open weights, but it's still really good. Let me tell you about it. It is called MAI image one and it is a completely in-housebuilt image generation model from Microsoft. It is currently in the top 10 of text to image on LM arena. We train this model with the goal of delivering genuine value for creators. And we put a lot of care into avoiding repetitive or generically stylized outputs, which is very welcome because the thing that I notice most of all with AI, and it's not just image generation, it's also text generation and video generation is it's kind of generic. And it's especially obvious with text generation. I think at this point, it's very obvious when you're reading something that was generated with artificial intelligence. there are just signals and you just read it and you're like gosh this is so boring or generic and there's just no life to it and that means it's AI generated and it comes across in images and apparently they have greatly improved that little negative feature. We prioritize rigorous data selection and nuanced evaluation focused on tasks that closely mirror real world creative use cases taking into account feedback from professionals in the creative industries. It excels at generating photorealistic imagery like lighting, landscapes and much more. So, if you want to try it out, you can go to bing.com/create. Check it out. And by the way, let me just tell you about a giveaway that we're doing. We're giving away this awesome 4K ultrasharp webcam by Dell. All you have to do is either follow Forward Future on Twitter, ForwardFuture, or you can sign up for our newsletter, forwardf.ai. I'll drop a link to the giveaway down below. Make sure you click it. Make sure you sign up and you can have the chance to win this ultrasharp webcam. Next, there's a new video generation model, appearing on the artificial analysis leaderboards. VU Q2 launches at number eight on the artificial analysis texttovideo leaderboard, surpassing standard Sora 2 and one 2.5. It is also one of the first models to support video generation with multiple reference images, enabling more controllable results by using multiple angles of the same character, scene, or object. All variants support up to 8 seconds of 1080p videos, which is pretty good. A little bit short, but 1080p is good quality. VU Q2 Turbo is $4 per minute, and VU Q2 Pro, $610, which is competitive, but doesn't blow the others out of the water. All right, let me show you a few examples. A 2D animation of a young elf riding a",
        "start": 646.56,
        "duration": 1574.0000000000002
    },
    {
        "text": "little bit short, but 1080p is good quality. VU Q2 Turbo is $4 per minute, and VU Q2 Pro, $610, which is competitive, but doesn't blow the others out of the water. All right, let me show you a few examples. A 2D animation of a young elf riding a colors are vibrant with magical glowing lights hanging in the trees and the elf's hair flowing behind as they race through the dense foliage. Okay, so here's what this looks like. Kind of looks like the game Zelda if it were made by Disney in the 80s. But here's Vu Q2. Pretty good. Cling 2.5 Turbo. Here's Vo 3.1, which has a very unique style. And then Sora 2 Pro. Here's another one. and a dolphin leaps out of the water in slow motion before diving back under the waves. The cinematic view follows the dolphin from beneath the surface, showing the splash as it disappears into the ocean's depths. VU Q2 up in the top left, we have Cling 2.5 over here, VO3.1 bottom left, and Sora 2 Pro on the bottom right. I think this looks pretty darn good, especially the lighting. As it passes over the sun, it becomes darker. You really only see the dolphin silhouette. them when it goes under the water. All of the water physics look extremely real. I'd say much better than Cling. VO3.1 decent. And then Sora 2 Pro is very, very good. So, VU is way up there in quality. All right. Next, Amazon seems to be threatening perplexity with legal action. And the reason they don't want Comet users to be able to purchase Amazon's products with an agent. Now, Comet is Perplexity's AI first browser. And so when you use it, you can say, \"Hey, go buy me some soap on Amazon. It'll navigate through Amazon's website, purchase it, and complete the order.\" Amazon saying, \"No, no, no. We don't want that.\" This week, Perplexity received an aggressive legal threat from Amazon demanding we prohibit Comment users from using their AI assistants on Amazon. Amazon wants to block you from using your own AI assistant to shop on their platform. Here's what they're trying to prevent. You ask your common assistant to find and purchase something on Amazon. If you're logged into Amazon, the common assistant quickly finds and purchases the item for you, saving you time for more important tasks. Or you can ask it to compare options and purchase the best one for your needs. Now, here's why they're doing it, and I've been talking a lot about this on the channel, the great decoupling of humans and the traditional internet. As agents browse the web more and more on our behalf, we're kind of breaking the internet's business model, which has been all about serving advertising because if an agent is browsing on my behalf, it is not my eyeballs on the page. So, they can't serve ads to me.",
        "start": 798.079,
        "duration": 1869.678
    },
    {
        "text": "As agents browse the web more and more on our behalf, we're kind of breaking the internet's business model, which has been all about serving advertising because if an agent is browsing on my behalf, it is not my eyeballs on the page. So, they can't serve ads to me. choice. I should be able to deploy my agent and say, \"Go buy me this thing or go book me this trip or go accomplish this other task on my behalf.\" it's my agent and I should be able to do that. And of course, if you didn't know, Amazon doesn't only sell products on Amazon, they serve advertising. And advertising on Amazon.com is a massive business. So, I understand why they're doing this. I just don't agree. So, this is just the first of many legal battles that we're going to see as agents continue to increasingly browse the web on our behalf. All right. Next, for a quick update, the texttovideo generation social network from OpenAI, Sora, has had a bunch of updates. The latest being character cameos. So, not just cameos with other humans, but you can actually include characters. You can design your own characters, you can upload your pets, and you can have them cameo in your videos. So, now Sora has this upload character feature. You simply upload a picture or a design, and you can then use that character in any of your cameos. So, really cool, fun feature. Try it out. So, that's it for today. If you enjoyed this video, please consider giving a like and subscribe.",
        "start": 947.839,
        "duration": 2014.9599999999996
    }
]