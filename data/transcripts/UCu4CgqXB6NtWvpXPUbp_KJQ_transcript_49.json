[
    {
        "text": " I am delighted to welcome to the stage Ian Buck, VP of hypers scale and high performance computing at Nvidia for his keynote on advancing innovation in AI infrastructure. Welcome Ian. Well, good morning everyone and thank you to YJ. , we spent a lot of time together. Building AI infrastructure is a a challenging and brave endeavor. and building infrastructure that works not only a single rack but an entire data center with that much compute to keep all those researchers active and busy and humming along is a challenge and of course as you know now we're actually building those data centers the next generation data centers and every year we have a new a new platform that with and that's just the pace at which AI is going so here at the AI inference summit I wanted to talk through some of the u perspectives of how Nvidia thinks about this market particularly around inference which is a highlight and a focus of so many of us today. , you know, inference is a pretty complicated landscape. Often people think of training as being hard. Certainly, it's the 100,000 GPUs that we've just talked about, but inference itself has so many vectors of optimization and trade-offs people have to consider. There's the intelligence. How big of a model I want to deploy? goes directly to the value of the model. But a bigger model costs more to run. The responsiveness of the model, how much compute should I put behind a particular model to make it more interactive, faster, more tokens per second for the user or more tokens per second for my entire data center are tradeoffs that we can make on how much infrastructure, how much revenue versus how much experience I want to provide for every user. We have cost. Obviously, there's a litany of different hardware platforms configurations from H100 to B200 to GB200, but GB300 and picking the right cost in the infrastructure you want to play into it and and to build and run those models. there's a trade-off between throughput and tokens per second per user. I can serve a model with incredibly fast if I throw a a massive number of GPUs at one query, but my data center generates revenue for all my queries and that trade-off. The more I'm spending on one person's queries, the fewer total tok queries I can do for all the users. We have to think about our trade-offs in that paro. And finally, energy efficiency with every our data centers are measured not necessarily in square footage but in megawatts or gigawatts. So understanding how much energy efficiency I can I can operate for a single model matters. All of these tradeoff are trade-offs we have to make in our infrastructure and what we decide to build and also our roadmap and how we innovate and think about the future. Hardware takes a long time to build.",
        "start": 0.08,
        "duration": 339.91999999999996
    },
    {
        "text": "efficiency I can I can operate for a single model matters. All of these tradeoff are trade-offs we have to make in our infrastructure and what we decide to build and also our roadmap and how we innovate and think about the future. Hardware takes a long time to build. think about all those trade-offs and project out a year, two years ahead in AI, which is nearly impossible. One of the most challenging parts of my job to think about these trade-offs and make the right bets at NVIDIA. Another way of thinking about it is looking at this acronym we call smart. First, you have to think about the scale and complexity. What is the scale and complexity of the infrastructure you're building and how can we make it scale and scale more efficiently? We have to look at that multi-dimensional performance. Every every decision we have to look at the different axes of how much intelligence, performance, throughput, cost, energy efficiency, and how those different metrics weigh into the final end solution for an AI factory. In the end, it becomes a full stack solution. You have the chip architecture, the rack, the node architecture, the rack architecture like you saw, but then the multiple layers of software and software optimization that go on top. All of that comes together to the solution. So that that is a it it's hardware plus software that creates the total end and throughput and performance in the end that delivers ROI. In inference especially, performance is equals revenue. And I'll give you some examples how the math works there. But performance really is paramount to having the the generating tokens to getting those out those queries processed. And it it is a per performance to revenue where training tends to be about capability per cost. And finally, all h is happening not in a vacuum but as a community. All of you here who are help building the AI infra of the future. Many of you representing the companies that are building data centers are building hardware or building racks connecting them all together and serving it. But also the the open software community. YJ talked about OCP open source software like Open Triton, PyTorch and the many other software stacks. All of these innovations are coming from the community. Unlike many of the other computer revolutions of the past, AI is particularly open where researchers and and companies are actually openly sharing their ideas, publishing their results, blogging about how to advance the future. And that's because it's a rising tide. As we all do that, we accelerate AI. We increase the opportunity for more AI, more more models, more capabilities, which feeds a virtuous cycle. At Nvidia, our contribution again is to continue to look at ways that we can increase the performance per watt per dollar and as a result turn it into profit for an AI data center. The most recent innovations and focus is was",
        "start": 170.08,
        "duration": 678.3999999999997
    },
    {
        "text": "more capabilities, which feeds a virtuous cycle. At Nvidia, our contribution again is to continue to look at ways that we can increase the performance per watt per dollar and as a result turn it into profit for an AI data center. The most recent innovations and focus is was originally a server box that had eight GPUs that were MVLink connected of H100 or H200 or B200 and disagregated it. We moved MVLink out of the box and made it rack scale. an incredibly challenging endeavor. It built some amazing infrastructure that's being deployed now and being used now. We had to think and rethink about how we do Ethernet at scale. We want to scale up beyond the traditional 10 20 50,000 GPUs connected with Infiniband to the 100 to million GPUs that want to be connected with Ethernet scale infrastructure. A very different kind of Ethernet. an Ethernet where every GPU or every node wants to talk to every other node at full performance. This is not what Ethernet was designed to do. It was designed to be able to connect clients with servers and and it have you know a usage case pattern where it isn't everything talking to everything all the time. That required a new kind of Ethernet new and new kind of switch and that's the basis for our Spectrum X Ethernet platform. numerical methods. AI is a statistical problem. It's really just giving it data and producing a prediction and using techniques as statistic techniques combined with numeric can exploed up the space of numerical representation. Everything started with FP32 32-bit floating point. We all know Google 10 years ago did Bloat 16. , I e's now added FP8 which we support going all the way down now to 4bit floating point. In fact, there's multiple 4-bit floatingoint formats. , and recently we've been in Blackwell bring brought to market the NVFP4 format which actually has a micro tensor scaling to try to keep all these math the entire the entire AI operating in just four bits. to keep that statistically in range without things capping. We're we're constantly having hardware capabilities to to keep the calculation totally in scale with constant biasing and and rebiasing of the of the calculation. NVFP4 is being used for inference and actually more recently we able to figure out the numerical algorithmic techniques to do NVF4bit 4-bit floating point for training as well. This was recently announced in hot chips. , doing the math is easy. Doing the numeric incredibly complicated. , software is a big part of it. Of course, Nvidia is just one one contributor to the software ecosystem. Specifically, we released back at GTC announced NVIDIA Dynamo and I'll talk more about that. our ability to take inference and disagregate it across all those servers to be able to do context processing and generation and really scale up the number of GPUs we can assign to a single a single model",
        "start": 340.56,
        "duration": 1022.0789999999993
    },
    {
        "text": "back at GTC announced NVIDIA Dynamo and I'll talk more about that. our ability to take inference and disagregate it across all those servers to be able to do context processing and generation and really scale up the number of GPUs we can assign to a single a single model software stack for serving models. In the end, this results in ROI. We have Blackwell delivers about 10 times the return on investment. to this is comparing the amount of dollars you spend on infrastructure to how much you can generate in revenue in your AI factory. And of course, none of this happens alone. I manage teams that work on PyTorch with YJ also jacks and other software stacks like Open Triton, but also for inference. We have SG Lang, VLM, but has their own inferencing libraries called TensorFlm. All this is happening in a community, an open community. And by contributing and supporting and providing the foundational software for these these technologies or allowing researchers or developers or companies and and clouds to be able to use it, we can elevate and bring all these technologies to market kind of in real time at a pace of well accelerating pace. It's quite amazing. performance is a difficult thing to measure particularly in inference and you know it's easy to make a claim but how accurate is the model? I can take a model and I can run it in reduced precision but if my precision isn't if I'm not delivering the same accuracy in a reduced precision format and running faster, I might as well be running a cheaper model. ML Perf is a a benchmark that's been around for quite some time. In 2019, they started MLF inference. Is this is where Google, Nvidia, Meta, and many other companies in the industry come together, create a a formula. We peer review each other's results to run benchmarks and actually prove that we can actually deliver a model with a certain amount of performance with a certain token rate for a particular user and agree upon that this is the you know established standard for measuring inference. since 2019, Nvidia has been submitting and running on all the benchmarks going back to the AMP architecture through Hopper to Blackwell and you know we've been continuously every year you know delivering new benchmarks and rising the the performance of AI models and inference. We hold every single per GPU record since the MLF data center benchmark started. , and in fact this last round which just gets announced today and I think later on you'll hear from David Caner. We've recently added DeepCar 1, the new Llama 3.145B and Llama 2 benchmarks were announced just recently. The new Blackwell Ultra or GB300 platform. this was used we used the NVFP4 we used the Dynamo software we used the new tensor RT and we fully distributed the the inference across the MV link of the",
        "start": 515.839,
        "duration": 1371.3599999999994
    },
    {
        "text": "the new Llama 3.145B and Llama 2 benchmarks were announced just recently. The new Blackwell Ultra or GB300 platform. this was used we used the NVFP4 we used the Dynamo software we used the new tensor RT and we fully distributed the the inference across the MV link of the software to do all that is quite complicated and literally since we've launched Blackwell last year to today we have actually doubled the performance of Blackwell just with software alone. It's the complexity of how you distribute the commu communication, apply the new numerical techniques to improve performance, reduce cost and get better throughput on ML perf 2x in software on the same blackwell totally for free. the same story played out in Hopper. We actually quadrupled Hopper's performance over its lifetime the same Hopper four times faster just through software and and applying all the optimizations by Nvidia engineers. But more importantly by the open source community and researchers, we're figuring out new ways to serve the same models at the same grade accuracy, just faster, cheaper, and generating more revenue. This is kind of the math that is today. Actually, a $3 million GB200 MBL72 rack actually will generate about $30 million in token revenue to the point where actually a free GPU is not even cheap enough. If you kind of compare like a let's say a previous generation or alternative platform with a quarter of the performance you can see at the bottom there the gray is actually the GPU cost and also the the shell and server cost that all goes around it. If I even delete or remove the the GPU cost you can see that the the revenue we generate over the course of multiple years is is demonstrable. This is how we feel about inference. The performance of the platform is it is the revenue of an AI factory and Blackwell literally delivers a 10x on what you buy to what you make. So let's talk a little bit more about inference and how it works and where the innovations are happening right now in inference and what we're focused on. inference is actually two different workloads you know and on the left on the right here you get a query that comes in and you do traditional inference serving. So then the first the first thing the AI model does is all the quant context processing. This is literally the question that you may ask a chat GPT or chatbot but also all the other tokens that come in that are unique to you or the system prompt. So these are things that you've asked in the past or things that are natural to your query that should assist the AI in answering the question that it's pulling from databases. It does doesn't just look at the of your question but all the other input tokens that that is called the context and prefill phase and then",
        "start": 695.6,
        "duration": 1681.0379999999993
    },
    {
        "text": "past or things that are natural to your query that should assist the AI in answering the question that it's pulling from databases. It does doesn't just look at the of your question but all the other input tokens that that is called the context and prefill phase and then and all the related data all the input token then it actually starts outputting tokens that you're reading and that's the generation facial decode. Typically we do this over a cluster of GPUs depending on the model might be four GPUs, eight GPUs or even multiple multiple GPUs depending on the model size and the performance but it's generally running one model across one set of GPUs. What's interesting though is that the context processing and the generation are actually different. They're both running the same model but the context processing can be done in a massively parallel way. we can process all the input tokens in parallel and where the generation of AI tends to be auto reggressive every token gets outputed you have to run it again to calculate the next token to calculate the next token and in a auto what's called an auto reggressive way where processing 16k 32k even 100k input tokens can be processed in parallel as a result you actually have this dis this this performance delta where the content is very comput heavy because we can do a massively parallel we all do it at once where the generation decode because it's auto reggressive needs a combination of memory bandwidth MV link bandwidth and compute in order to fastly output in the vers of tokens if we stick on the same platform or one one GPU we end up having to pick the best of both worlds but sort of in the middle somewhere but not necessarily optimal for these two workloads today most modern data centers actually do disagregated inferencing so they actually take that input query and they run generate the context processing on separate GPUs creating what's called the KB cache basically outputs just the first token and then it's it hands off the KB cache to another set of GPUs which are optimized for for generation. This allows us to actually split the number of GPUs we're doing for context versus generation and actually and dramatically improve the overall performance. the Nvidia Dynamo software is designed to do this. It's all open source. You can go check it out on GitHub. all of our development is gith github first so you can see a live check-ins but by doing this optimization we can actually configure those GPUs and run the model with different AI kernels in a mass in a parallel compute context prefill way and then for the auto reggressive part configure different with different kernels different parallelization techniques for the fast auto reggressive this overall increases the total throughput the same number of GPUs and in fact that will generate a six times improvement",
        "start": 857.36,
        "duration": 1978.6389999999992
    },
    {
        "text": "a mass in a parallel compute context prefill way and then for the auto reggressive part configure different with different kernels different parallelization techniques for the fast auto reggressive this overall increases the total throughput the same number of GPUs and in fact that will generate a six times improvement about two to four between anywhere from two to 4x faster same number of GPUs just doing disagregated it's a lot harder because you have have to have two sets of basically workloads running in parallel on the system and also having this KV cache transfer between the two platforms and keeping everything busy we have but this is in production today there's a company called ben which is a inference aggregator basically model serving company. They have over 8,000 GPUs of both Hopper and Blackwells spread around multiple clouds including Google cloud. they actually when GPTOSs first launched they had the fastest inference performance of any cloud provider because they super optimized using NVIDIA Dynamo that split between the context processing and the output generation. It's an important example of how much software matters both and how it combines with the infrastructure of of a rack like NVL72 and GB200. overall disagregation gives you about six times faster first token on models like Quen. we're seeing 3x higher faster token output on models like DeepSeek and basically turning inference into sort of a data scale data center infra scale kind of problem. as a in addition to this we're seeing that that context processing becoming more and more important and higher and higher value. most of the models you see today can accept up to about 256,000 input tokens. , and there's roughly, you know, , two to three tokens per word. So, you can kind of get a sense of how much input token they can consume when you ask a question to a typical chatbot. But there's a slice of workloads that actually really love having super long input tokens. Two examples of that is advanced codings. , we all we've heard of coding chatbots basically help you write code. advanced coding chatbots take the entire program and allow and use the AI to add new functionality. Instead of helping you write like a little loop of code or fix find bugs, advanced AI code is going to take a 100,000 lines of code or a million input tokens of code and actually be able to output new functionality, entire code blocks, entire portions of of the application to turn the AI really into a software agent that can interact with a software developer in a totally different way. But you need to be able to process literally millions of tokens. But the value is incredibly high because now I have basically a software developer that is that that can amplify my entire software developer workforce by like 10x because there the AI is actually generating the initial code that the",
        "start": 1008.56,
        "duration": 2315.9989999999984
    },
    {
        "text": "need to be able to process literally millions of tokens. But the value is incredibly high because now I have basically a software developer that is that that can amplify my entire software developer workforce by like 10x because there the AI is actually generating the initial code that the of functionality scale. The other use case that's really hot right now is video processing and generation. Think of processing like an hour of HD video and producing new video content that's generated. Video is a lot of data, millions of tokens. Today the video generation market is about $4 billion for AI video and by starting the next decade it's projected to get be over $40 billion market. this is B in the entertainment space and also in the media and marketing and advertising space. , one way to think about it is, you know, we used to live in a world where when we came home and watched on our TVs, you know, it was whatever was on the TV. We moved to the digital era. Now we have on demand. We can watch whatever we want. And by the end of the decade, we're basically going to be on interactive media. It won't be whatever on demand we want, but every con all the interaction we could do with our for entertainment will could be interactive and of course will be done through video. So having these long context capabilities is really interesting and whenever we see this kind of opportunity at NVIDIA where there's a high value market with a place where we're pushing the limits like how big our input context is. It's opportunity for us to optimize further. So maybe there's a way we can actually process or work on the highv value large context instead and maybe not use the same GPUs for context and generation but focus on on bringing these large market these new capabilities to market and that's why we announced this morning and here at specifically at the at this AI conference a new kind of Ruben processor dedicated for long context processing. This is the the Reuben CPX GPU is a GPU specifically built for massive context length processing for these high value use cases of million scale token processing. It's specifically optimized for context processing and still of course CUDA capable. This is a new Reuben GPU which we haven't disclosed or talked about before. Based on the same Reuben architecture but a new instantiation, it has over 30 pedaflops of NVFP4. , all CUDA capable. We've actually tripled down on the attention processing. attention is the building block of many of the models we we have today and we actually added new attention acceleration cores to to this chip which is three times faster than what we have in the current GB300 GPU is memory optimized so a lot of that compute processing of context is comput it's less dependent on HPM bandwidth or",
        "start": 1181.44,
        "duration": 2660.318
    },
    {
        "text": "we have today and we actually added new attention acceleration cores to to this chip which is three times faster than what we have in the current GB300 GPU is memory optimized so a lot of that compute processing of context is comput it's less dependent on HPM bandwidth or having a vlink scalability so we can use the standard GDDR7 memory that we use today and most of the GPUs that are available in the market. And of course, we doubled down on video. So, we added four Nvidia video encoders and four decoders for processing and generating AI video content. And this will come online in the end of 2026 right after our initial launch and availability of Nvidia Rubin. So how do you integrate this processor, this chip, this single die Ruben into the Vera Rubin rack? So here is Vera Rubin. We announced this at GTC this year. this is it has over 3.6 exoflops of AI performance in a single rack. This is coming be available in the second half of 26. as you can see there's the compute tray. each tray has four Rubins, two Veraro CPUs and Connect X9 for the scale out interconnect. It's pretty impressive platform. It's got over three and a half time 3.3 times more compute in a rack than GB300 which is deploying today. it'll have 70 75 terabytes of fast memory and 1.4 pabytes of HBM4. and quite an impressive rack in of itself. It all sits in the same rack architecture as DB300 to hopefully to help YJ and others here to deploy it actually will fit in the same mechanical and space. and of as you can see 72 GPUs or package GPUs in a single rack. This is actually a dual die GPU. So it's 144 that's why we call it MBL 444 Rubins in a single rack. But let's talk about CPX. We can actually just add CPX to this platform. In fact, right down the bottom, we have areas where we can insert additional this the context processors and really boost this rack up for million scale token processing. This is the U Vera Ruben NVL 144 CPX. All we have done here is taken the same tray, same architecture, but we've inserted eight of our Reuben CP CPXs behind those the Vera in connection with the the ConnectX9s and those processes are available for the entire rack to do to do context processing and we just totally boosted up the the performance of the rack. As you can see now, we're up to eight exoflops or seven and a half times what GB300 can do today. we've increased our memory again to 1.7 to 3x. our fast memory has increased f further to 100 terabytes and again all this will fit nicely into the existing rack infrastructure so that for customers that want to prioritize verbin for million token input context. This is a a a seamlessly way to",
        "start": 1355.679,
        "duration": 3005.2789999999995
    },
    {
        "text": "memory again to 1.7 to 3x. our fast memory has increased f further to 100 terabytes and again all this will fit nicely into the existing rack infrastructure so that for customers that want to prioritize verbin for million token input context. This is a a a seamlessly way to centers. We also don't have to put the CPXs in the tray. We'll also be making a CPXon compute tray version. And in fact, customers can actually just put it as a sidec car to their to their Vera ribbon rack. This is a on the left there's a new tray called VR CPX. As you can see, you have two Ver CPUs and eight CPX processors connected with the same networking behind the scenes and they can add a VR CPX rack in their data center side by side either one to one, one to N, whatever their mix between their context processing or and their output generation where all the context processing is happening on the CPX rack and all the generation token generation can be happening on VR. and of course one ratio is fine. they can mix it to two to one or they can start with some and expand later. All that makes it possible. You don't have to have them next to each other. the way context generation works is as soon as you have your first token, you just need to send that KV cache to your to your token generators wherever they are in your data center. Quite an upgrade and and running really fast. we've already been working with some of the Lighthouse customers who are super interested in long context. these are different AI innovators. We all heard of cursor who is probably one of the leaders in intelligent code generation. and Nvidia uses cursor along with many others. and this will help them get to the next level of developer productivity with those million tok input token code generators. magic actually is a as a magic.dev has a 100 million token input model. Quite impressive. And we're working with them to figure out how to get that working on CPX along with runway and which which is a company which generates cinematic video and other leading inference providers like fireworks and together AI who have some of the most advanced techniques for the fastest ple model serving and how they can inject get to that next level of million million token inference. So, we've added another chip to our rim app. , you can see here we have on the not in Blackwell, we have the the Blackwell and Blackwell Ultra, the gray CPU, , our MVLink switch chip, the Spectrum 5 , switch, and of course, the CX8 Nick. All of these chips come together to make AI and AI Infra work. It's never just one chip. It's a family. And now with Reuben, we've added the CPX processor, a different Reuben GPU",
        "start": 1530.32,
        "duration": 3316.3199999999997
    },
    {
        "text": ", our MVLink switch chip, the Spectrum 5 , switch, and of course, the CX8 Nick. All of these chips come together to make AI and AI Infra work. It's never just one chip. It's a family. And now with Reuben, we've added the CPX processor, a different Reuben GPU context processing that'll be paired and matched with Reuben for the one the million scale context processing and fits nicely in into the full family. And of course that'll extend and look forward to talking more about Fman when we're when when we get a little bit closer. All of this has to come together. The AI is not served and data centers aren't built with one processor. They're connected machines. They require CPUs. They require GPUs. They require various levels of accelerating. The network and infrastructure at scale is needs to work as one in order to serve these models and bring that token value and that revenue that inference will generate all together in one. And that's what we are focused on at Nvidia is bring all that infrastructure and the baseline software stack to market as quickly as we possibly can. One challenge of course is how do we build those future data centers? Shown you a lot of racks. We show a lot of chips. the you know the next challenge of course is can we what is that future data center going to look like? Y talked about how the CPU data centers have evolved and how they're so different and you know we're Nvidia is also a huge proponent of open standards or members of OCP. We've contributed the GB 200 rack to OCP and we'll do and we'll do so for the upcoming infrastructure as well. But the problem now is becoming a data center scale one. How can we build and provide and working with the community a data center roadmap not just a rack and GP road mapap that's going to be future designed for the future that allows us to scale and grow. It's pushing the limits of power generation, mechanical, plumbing, electrical, you know, bus bar design, row length, CDUs, and how are all these pieces and components going to work together in order to run and have these data center factories work well and be futurep proofed as we scale out not just for Ver Rubin, but Ver Rubin Ultra and going on to to Fman. we've started a new initiative called the AI factory gigascale reference design and of course Nvidia can't isn't doing this by ourselves working the entire community from cadence to emerald AI to EAP GE Verona Jacobs which is the main engineering consulting firm that builds a lot of these data centers Schneider Electric Seammens and Verdiff to build the cooling plumbing and electrical systems that can scale to deliver these kinds of this kind of future data center in a way that's future that take a long time to build",
        "start": 1689.84,
        "duration": 3632.4769999999994
    },
    {
        "text": "engineering consulting firm that builds a lot of these data centers Schneider Electric Seammens and Verdiff to build the cooling plumbing and electrical systems that can scale to deliver these kinds of this kind of future data center in a way that's future that take a long time to build architecture. They all and all these components need to talk to each other that the CDUs and the power and the data center operations have to work as one along with the GPUs and compute infrastructure so they work seamlessly maintain uptime and and high efficiency. working with these partners now and we expect to have the first version of the reference design done in our next upcoming GTC conference. That's my update for today. I thank you everybody. It was really exciting to launch CPX here at AA infra and look forward to the rest of the talks. [Applause]",
        "start": 1852.64,
        "duration": 3704.996999999999
    }
]