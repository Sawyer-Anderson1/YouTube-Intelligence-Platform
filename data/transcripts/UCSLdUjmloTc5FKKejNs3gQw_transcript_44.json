[
    {
        "text": " I think the AI SDK is the best way to build AI powered apps in Typescript. And hopefully by the end of the video, you'll see why, as I want to go over my favorite features while building out an application that has text streaming, structured outputs, file uploading, image generation, web search, and even MCP servers as tools. And all of this genuinely takes about 10 minutes to do. If you don't know what the AI SDK is yet, it standardizes integrating AI models. So instead of needing the open AI SDK, the anthropic one, the Google one, the Deepseek one, you just use AI SDK and you can use pretty much any popular model, it also has some super nice features that will help you build any AI feature, whether it's a small one or for your app or if your whole app is going to be AI powered. The AI SDK will have you covered. So let's jump in and take a look. Now, I'm going to start with the simplest way that you can generate text from an LLM using the AI SDK. As you can see, we're importing a function called generate text from AI. Now, I am a little bit curious if they had to pay and how much to get that name on mpm. But it's pretty cool they did manage to. All we need to do with that then is await the generate text function and that returns a text property with the answer from our LLM. Inside of the generate text function, we pass some configuration options. There's loads that you can choose from, but the simplest ones that you need are the model that you want to use and the prompt to send to that model. So, that's about five lines of code and we've got a response from the LLM. Now, a quick side note here. I'm using OpenAI's GPT5, but I'm using it as a string name in the generate text function. That's because I'm using the Vel AI gateway. So, I have one API key and one bill and I can use pretty much all of the popular models with fallbacks from different providers. You can, however, use the provider of your choice. All you need to do is find the relevant provider package from the AI SDK. They support pretty much all of the providers. I actually use this quite a lot with open router. In this case, I've used Anthropic. Then, all we need to do with that is make sure we have our Anthropic API key set. And now we can go ahead and use the model by using the anthropic function and passing in the model name that we want to use as the argument of that function. So in this case, I've now got Claude Sonic 4 answering me and it's coming straight from anthropic. I'll go back to what we had though. And one thing to note is",
        "start": 0.0,
        "duration": 232.40000000000006
    },
    {
        "text": "function and passing in the model name that we want to use as the argument of that function. So in this case, I've now got Claude Sonic 4 answering me and it's coming straight from anthropic. I'll go back to what we had though. And one thing to note is non-interactive use cases when you want this to be userfacing. It's going to be a little bit slow. This is taking over 20 seconds to return a response in some cases. That's because we're currently not streaming the text in. This is where I'll start to build out my application. Now I'll be using Nex.js JS for this, but obviously these functions can be used anywhere in Typescript. I've created an endpoint at / API/ chat and instead of using the generate text function to get streaming text, we now use the stream text function. As you can see, it works a little bit differently. I'm not awaiting this function anymore since we want to access the stream. And I'm also just getting the full result back. The result object will still have the text property on it, similar to generate text, except that one will still wait for the full generation to complete before it returns anything. If you want to access the text stream, you can do that via result.extream. or text stream or you can even use the result of fullstream if you want to see all of the individual pieces like the reasoning tool calling and anything like that. Luckily for us though we don't have to consume this manually at all as the AI SDK has tons of helper functions. The one I'm going to use in this case is going to be called to text stream response and now this endpoint has a HTTP text stream coming back from it. If I send a request to this endpoint now we should see that the answer streams in. There we go. We now have text streaming working in pretty much 10 lines of code. Now, back to that endpoint, there are a few upgrades that we can make. First, we want to take in an array of messages in the body of the post request. This way, we can actually have a chat with the model instead of just sending a static prompt. You see, this is of type UI message. This is essentially the state for your application. This will contain all of the information that your app might need to know about how the user is chatting to the model who include things like the text stream, the reasoning, the tool calling, and also the total tokens. Then what we need to do is actually convert those messages to model messages since the model doesn't need to know that extra information like total tokens and things like that. Luckily, there's a helper function from the AI SDK. So we can say convert to model messages and",
        "start": 116.72,
        "duration": 446.23899999999963
    },
    {
        "text": "tokens. Then what we need to do is actually convert those messages to model messages since the model doesn't need to know that extra information like total tokens and things like that. Luckily, there's a helper function from the AI SDK. So we can say convert to model messages and it's essentially only sending over the chat history and the result of the tool calls so that it doesn't take up the token context window with a load of gibberish. After that, since we're now using UI messages, the final thing we need to do is instead of doing result to text stream response, we now return a UI message stream response. With those changes, this endpoint is now ready to be hooked up to a UI. This is where the AI SDK actually gets split in two. The backend functions that we just looked at like generate and stream text are part of what's called the AI SDK core. And the front-end functions that we're about to look at are called the AI SDK UI. At the moment, the UI side of things only supports React, View, Spell, and Angular. Obviously, if you're on another framework, you can simply use the backend functions and then implement the front end yourself. Now, in my case, I'm using React, so I can use the AI SDK UI. And there are pretty much only three hooks that you need to be aware of. There's the use chat hook, which is how we're going to have a chat with our app. Then, there's also the use object hook and the use completion one. Pretty self-explanatory what those ones are going to do. Since I'm currently building out the chat part of our application, I'm going to be using the use chat hook. Now, you can see here we can set up the transport. to where the API endpoint is that we should use with this hook. And at the moment, I've set that to / API/ chat. Now, that's actually the default of the use chat hook. So, I can go ahead and delete this, but I just wanted to show you in case you're using a different endpoint. This hook then returns information like the messages that we have with the chat. And see, this is of type UI message. Then, we have a send message function, which is a nice helper as it's going to go ahead and append to that messages array for us. So, we can simply just send a string through as our message and also the status of the chat. So, is it submitted, streaming, ready, or in error? With those three pieces of information, then we pretty much have everything we need to build out our own shiny UI. But bonus tip here is I recently did a video on AI elements, which are essentially the Shaden components that work with the AI SDK. So, I'll go ahead and install them",
        "start": 224.64,
        "duration": 656.4789999999998
    },
    {
        "text": "pieces of information, then we pretty much have everything we need to build out our own shiny UI. But bonus tip here is I recently did a video on AI elements, which are essentially the Shaden components that work with the AI SDK. So, I'll go ahead and install them Now, I'll leave a link to the video I did on those if you want a full breakdown. What you can see here is we're still using that same use chat hook with those three properties. I've also set up an input so we can actually type a message. Then in our handle submit, all we need to do in the send message function is send over our text here and then set the input to empty. After this, we can simply map over the messages array. So we can actually display it nicely to the user. In here, you can see what a UI message is composed of. First, we have a role. So was it a user message, a system one or an assistant. Then each message has parts. So this will be the reasoning tool calling and then the final response. If it is simply the final response, so it's text, we'll render this in a nice markdown block. If it's reasoning, we'll have a separate reasoning component. And then if it's a tool call as well. At the moment I have a generic tool component so we can show the output of those tools. We'll come back to this in a bit to actually add in specific rendering for different types of tools. But for now that is pretty much all we need to know about the UI messages. After this we also map over the sources that we might get back from the model. This will come in handy when we actually activate web search. You can see again that's just a separate react component. Then finally we just have our prompt input which is literally just a text area. Then we send it via that handle submit function that we saw earlier. When I run this then and we send a message saying hello world, we see a loader. We can see the model is currently thinking and there we go. The final response was streamed in. It is absolutely awesome how quickly you can get a nicel looking UI and set it up with AI streaming with the AI SDK and those AI elements. There are a few things that we can really quickly improve though. First, we're not actually seeing the reasoning that we're getting back from GPT5. And also, I want to be able to send images and PDFs to my chat. I'll start out with the file uploading as that one is super simple. If you're using a model that already supports file uploading, like most of OpenAI's, Google's, and Anthropics, all we need to do is have a file list somewhere here in React. And I've simply",
        "start": 331.84,
        "duration": 864.2379999999999
    },
    {
        "text": "to my chat. I'll start out with the file uploading as that one is super simple. If you're using a model that already supports file uploading, like most of OpenAI's, Google's, and Anthropics, all we need to do is have a file list somewhere here in React. And I've simply here, which is of type file that accepts images and PDFs. Then all we need to do on our send message function is also send over the files, which is that file list that we just created. Now, back in my app, we should be able to attach an image. Now, in this case, I'll use this one. And to test it out, I'll ask the model to describe the image. And there we go. It says it's a large glossy red rounded button with the word subscribe. Something you should definitely do. This means it should also be working with PDF. So, we'll give that a test. And there we go. We now have file uploading working and it can process our images and PDFs. The next issue we have to tackle then was streaming in the reasoning. Now, this is where things start to get a little provider and model specific because of course not every single model has reasoning and also not every single provider streams that back by default. In my case with GP5, we need to tell OpenAI to stream us back the reasoning summaries. We're able to do that with provider options. You can see I passed these through on stream text here. This is where you can set up your individual providers and their unique configuration options. In the case of GT5, we need a reasoning summary. This can either be concise, detailed, or auto. And also just sets it to detailed for now. Then I've also gone ahead and set the reasoning effort here to medium. Now, if I send a message to our application, we'll see the reasoning is being streamed in. There we go. All is working. Now, GP5 actually uses multiple reasoning steps, which why we have a few of these different components. You see it thought for 3 seconds, 2 seconds, 2 seconds. This might think for a little bit longer since I set that reasoning effort to medium. That's the provider options. Then I highly recommend you check out the documentation if you're working with something that's provider or model specific. It will have all of the options that you're able to send over. Now, the next improvement I want to make to my app is an artificial one. You might notice since I'm using GT5 Nano, it's a pretty fast model, so the text comes in in large chunks and it's just not a pleasant user experience. It's not as readable as some of the other sites like Chat GBT and Anthropic. is because what some of those actually do is they have artificial smoothing. We can also achieve this really easily by",
        "start": 437.599,
        "duration": 1083.2799999999993
    },
    {
        "text": "text comes in in large chunks and it's just not a pleasant user experience. It's not as readable as some of the other sites like Chat GBT and Anthropic. is because what some of those actually do is they have artificial smoothing. We can also achieve this really easily by the AI SDK. All we need to do is pass this through as an experimental transform on our stream text here. Then you can also add in some options like the delay of the buffer. In this case, it's 10 milliseconds. Then also the chunking strategy. So do you want a custom reax word line or a custom chunk detector? Now these are the default options. I'll show you a sideby-side comparison now of a stream without any smooth streaming. then one with the default options and also one with the line strategy. I personally think the default option looks the best with that. Then we have a pretty good chat, but let's take it to the next level by adding in some tool calling. There's actually three types of tools that you can use. The first one I'm going to go through is essentially a manual tool. This is where you control the code of that tool call. In this case, I've set up an image generation tool. We're using the tool function from the AI SDK to make sure that we've got the right options. We pass it a description so the model knows what this tool does. Then we have an input schema here in Zod. What's really nice is we can use the describe here to make sure the model understands what this value is supposed to be. Then finally, we have our execute. This is just the function that's going to run when the model calls this tool. In my case, I want to add in image generation into my chat. So I'm using the experimental generate image, which also comes from the AI SDK. Then I'm using the GBT image one model. So now we should have the image generation in our application. All we need to do now with this tool though is pass it through to the stream text function. All that means is we need to have a tools object on here and then we pass through my generate image tool. I'll also add in some UI code that's going to render a custom component when it runs the generate image tool. You can see similar to how we set up the text and the reasoning before all we need to do is say if it's tool dash and then the key that we used in that tools configuration in my case generate image then in here we can have our custom component. In my case I'm just simply going to return an image element if we have the image back. Otherwise it will just say generating image. So now if I ask it to generate me",
        "start": 548.88,
        "duration": 1294.6389999999985
    },
    {
        "text": "tools configuration in my case generate image then in here we can have our custom component. In my case I'm just simply going to return an image element if we have the image back. Otherwise it will just say generating image. So now if I ask it to generate me that it goes through its reasoning steps first and then runs the generate image tool. And after a bit of time, we now have a cute image back and our chat app now has image generation. The benefit of setting up tools like this means that it's going to work the same across all of the models and have a similar behavior since we're using a set model in that tool call. Another type of tool that we're able to use though is really cool and it is provider specific. These are things like computer use, code execution, web search, all which OpenAI, anthropic Google seem to offer. My favorite one though is the Google search and URL context tools. Now, for this you will need to switch to using the specific provider that has those tools. So, in my case, I've switched to using Google's Gemini 2.5 flash. Then, in our tools configuration, we can say our web search tool and our URL context tool, our Google.tools, and then whichever tool we want. You can see there's actually three of them. There's also code execution. I'll leave this on Google search and URL context. And the final thing I'm going to do is in our 2UI message string response is we can add in a configuration option that says send over the sources that it used in our Google search. So we can also render them in in the front end. So that is literally all of the code that you need to give your chat app access to the internet. I'll test this out by asking it for the latest news stories in F1. And it's now responding with relevant up-to-ate information and we're able to see the sources that are used. Now there are some downsides of using provider specific tools. not only that you're locked to that provider and its models, but also the fact that if we use Google search in this case, it actually locks it down so you can't use any other tool calls. That's why I personally prefer to use an external service for web search in URL context. In this case, I've used Tavilli. You can use other services like fire crawl and also Brave's search API. So, I've set up my own manual web search tool here which uses Tavilli search function and also the URL context tool which uses its extract function. Then down in my tools object, I can pass through those tools. We can activate image generation again. And now, no matter what model or provider I'm using, it will have access to all of these tools. I gave it a test by asking what",
        "start": 656.079,
        "duration": 1521.5189999999989
    },
    {
        "text": "which uses its extract function. Then down in my tools object, I can pass through those tools. We can activate image generation again. And now, no matter what model or provider I'm using, it will have access to all of these tools. I gave it a test by asking what 1.com. And remember, this is GPD5. Then it used our URL context tool. It scraped the website. As you can see, it replied with the latest headlines that it found on that homepage. Now, our app is actually getting quite powerful, but the final type of tool is really cool. We can actually give our LLM access to certain MCP servers. Now, this is currently experimental, but in my case, I want to hook it up to the GP MCP server, so we're able to search GitHub. You can see I've created a new HTTP client transport, which actually comes from the MCP SDK. In here, I've passed in the URL to the MCP server itself. You can also use the standard IO and SSC methods if you want. And then down here, we've used the experimental create MCP client from the AI SDK and passed it that transport. Then, all we need to do to get the tools is simply await the tools function. And then in our tools configuration down here, we can dstructure all of those tools. So now our LLM has access to all of the tools that that MCP server does. Now when I ask my chat app to search GitHub for that experimental create MCP client function, you can see it went off and ran a dynamic tool, which in this case is the MCP servers tool, it searched GitHub, it got some results back, and it presented me with some code snippets straight from GitHub. Now, the final thing you need to be aware of when using tools in the AI SDK is the stop when condition. This essentially says when should we stop generation? I believe it's actually set to a step count of one, but if you're using multiple tools, you might want to set it to a step count of something like 20. Essentially, what this means is it's able to go in, use the web search tool, it'll find a bunch of URLs, then it can use another tool call to look into those URLs, but will never do more than 20 tool calls. It just helps stop it creating an infinite loop. You can also pass in custom conditions if you want. Just the most popular one tends to be the step count is, which is why there's also a helper function for this from the AI SDK. You can see there's just so many helpful things in the AI SDK that will help you build out any AI feature. Now, for all of these, we pretty much mean dealing with a chat app, but there is also structured outputs. In this case, I want",
        "start": 772.0,
        "duration": 1749.999999999998
    },
    {
        "text": "the AI SDK. You can see there's just so many helpful things in the AI SDK that will help you build out any AI feature. Now, for all of these, we pretty much mean dealing with a chat app, but there is also structured outputs. In this case, I want as a prompt and break it down into subtasks. And for that, I'm going to use a structured output. To do that, we can simply use the stream object or generate object function. Obviously, pretty clear what those ones are going to do. We pass in the same information as we do to stream text. So, we have our model, prompt, provider options. But the one thing that is unique to the object ones is passing in a schema. We can pass in a Zod schema here for how the LLM should reply. In this case, I'm using a breakdown schema which has the original task. Then it has an array of subtasks which are a description and a time estimate. Then an overall time estimate for the task. Again, we're using this describe function here to pass additional context to the LM of what these values should be. It's just way nicer than having to describe this in the system prompt as text then forgetting to update it when you change one of these values. With that, we can return this to a text stream response. Then on the UI side of things, all we need to do is use our use object hook. We pass in the API that we just set up and also the schema again so that it can be validated. This returns the object. In this case, since I'm using stream object, it will actually be a partial object since not all pieces will be streamed in yet. Then we also have the submit function so we can send over our prompt. With that, we can simply go ahead and map this out. So we have a nice UI. I'll test this out with a task of clean the kitchen. We should see that the object is streamed in. There we go. You saw each individual piece was being streamed in, which is a really nice feature. It's just crazy to me how easy these things are becoming because when the OpenAI SDK first came out, some of these things were a little bit painful to implement. Now, that is all I wanted to explore for this video. But honestly, I still am only scratching the surface of the capabilities of the AI SDK. There's even more functions like generate speech, generate transcription, and handling embeddings. Hopefully though, that has been a good overview of the SDK and what it's able to do for you. Let me know down in the comments if there is something that you want to see a deeper dive into. While you're there, subscribe and as always, see you in the next one. Quick bonus fact if you made",
        "start": 888.24,
        "duration": 1974.0789999999977
    },
    {
        "text": "of the SDK and what it's able to do for you. Let me know down in the comments if there is something that you want to see a deeper dive into. While you're there, subscribe and as always, see you in the next one. Quick bonus fact if you made the AI package on npm. It was published 12 years ago and it originally stood for abandoned issues before took it over and turned it into the AI SDK.",
        "start": 1002.079,
        "duration": 1993.6769999999979
    }
]