[
    {
        "text": " So, OpenAI and Anthropic both just dropped major new coding models on the exact same day. This is one of those moments where you can actually feel the competition heating up in real time. Both companies are pushing hard into full-on agent style software development where AI doesn't just write a function and stop. It plans, uses tools, runs commands, checks results, and keeps working through longer tasks like an actual digital teammate. Let's start with OpenAI because they came out with something called GPT 5.3 codecs and this model is clearly built for people who live inside code editors, terminals, and dev environments all day. OpenAI says this version runs about 25% faster for codeex users. That speed boost matters a lot in agent workflows because the model isn't just generating text, it's thinking through steps, running tools, and iterating. Faster loops mean less waiting around while it executes multi-step tasks. GPT 5.3 Codeex is already available across paid chat GPT plans anywhere Codeex exists inside the Codeex app, through the CLI, in IDE extensions, and on the web. API access is coming later once they are done with additional safety controls. That tells you right away they see this as a higher capability model that needs a bit more careful roll out. The model is designed to handle longer multi-step tasks where it can use tools, interact with systems, and keep working through a problem instead of stopping after a single response. It can operate a computer, use external tools, and complete tasks end to end. Inside the Codeex app, it gives frequent progress updates while it works. So instead of waiting for one big final output, you can jump in mid-process, ask questions, redirect the approach, and basically steer the solution while it's still unfolding. There's even a setting in the app for this called follow-up behavior where you enable more active steering. That's OpenAI leaning into the idea that this is a collaborative agent, not a singleshot generator. Now, let's get into the numbers because they came with a full benchmark table on S.WE, which measures real multi- language software engineering problem solving. GPT 5.3 Codeex scored 56.8%. GPT 5.2 Codeex scored 56.4. 4% and GPT 5.2 scored 55.6%. The gap looks small. Yet in these benchmarks, even a one-point move can translate into noticeably better realworld bug fixes and patches. Where the jump gets more obvious is terminal bench 2.0, which focuses on the terminal skills a coding agent needs. Things like navigating directories, chaining commands, handling outputs, and using CLI tools. GPT 5.3 Codeex scored 77.3%. GPT 5.2 codecs came in at 64.0% and GPT 5.2 at 62.2%. That's a serious leap and it lines up with OpenAI's whole pitch that this model is trained to actually live in terminal style workflows and keep things moving. Then there's OSWorld Verified which measures computer use performance in a desktop environment using vision. GPT 5.3 codeex scored 64.7% GPT 5.2 codeex was at 38.2%",
        "start": 2.639,
        "duration": 395.11899999999997
    },
    {
        "text": "and it lines up with OpenAI's whole pitch that this model is trained to actually live in terminal style workflows and keep things moving. Then there's OSWorld Verified which measures computer use performance in a desktop environment using vision. GPT 5.3 codeex scored 64.7% GPT 5.2 codeex was at 38.2% Open AAI included a human reference too. Humans average around 72% on OS world verified. So GPT 5.3 codeex is being framed as a model that's getting fairly close to human level performance on basic desktop task completion. For economically valuable knowledge work, OpenAI sites GDP val wins or ties at 70.9% for the new model. On cyber security capture the flag, it scored 77.6% compared with 67.4% for GPT 5.2 codecs and 67.7% for GPT 5.2. OpenAI said all these evaluations ran with X high reasoning effort. So these scores reflect the model in its more advanced reasoning mode. Because of that strong cyber performance, OpenAI classified GPT 5.3 codeex as its first model labeled high capability for cyber security tasks under its preparedness framework. That classification triggers extra safeguards and staged access controls. They also announced a trusted access for cyber pilot program alongside the release which is basically a gated channel for security professionals to use these higher capability models responsibly. All right, quick break from the coding stuff because this is actually huge for anyone into AI video. Higsfield just rolled out Clling 3.0 access directly inside their platform, bringing one of the most advanced AI video engines straight into a full production style workflow. And they are sponsoring today's video as well. So Higsfield in general is built as a creator first AI production hub. When you land on the site, everything is structured like a real workflow, not just a prompt box. You can go from concept to finished video inside one pipeline. Now, inside that pipeline, you get many models. Basically, the best and most recent ones. And the newest edition is Cling 3.0, which is one of the most advanced AI video engines out right now. Fully integrated, so you're not jumping between tools. Cling 3.0 runs on a native multimodal system, meaning scripts, images, references, and audio all work together in one model. It supports multi-shot generation in a single run, so scenes flow naturally with automatic camera transitions. You also get native dialogue with lip sync, strong character and object consistency, clean text rendering for signs and subtitles, and up to 15 seconds of continuous video per generation. If you're serious about AI film making, Higsfield is definitely worth trying. Link is in the description. All right, now back to OpenAI. One of the most interesting parts of OpenAI's announcement is how the model was used internally. They said early versions of the new model helped debug its own training run, supported deployment, diagnosed evaluation results, and assisted with operational tasks like adapting harnesses and scaling GPU clusters as traffic changes. So, the",
        "start": 199.599,
        "duration": 729.4400000000002
    },
    {
        "text": "the most interesting parts of OpenAI's announcement is how the model was used internally. They said early versions of the new model helped debug its own training run, supported deployment, diagnosed evaluation results, and assisted with operational tasks like adapting harnesses and scaling GPU clusters as traffic changes. So, the It was part of the engineering tool chain that helped build and ship itself. On the hardware side, OpenAI said GPT 5.3 codecs was co-designed for, trained with, and served on NVIDIA GB 200 NVL72 systems from Nvidia. That kind of detail shows how closely model design and cutting edge GPU infrastructure are tied together. Now, product-wise, this model launch also connects to OpenAI's new Codeex desktop app, which they rolled out just days earlier. That app is built for managing multiple AI agents over longer periods of time, not just quick chats. It can use code to gather and analyze information. And OpenAI said more than 1 million developers use codecs in the last month. All of this paints a clear picture. Open AAI wants codecs and the new model to be central tools in everyday development workflows, especially for longer agent style tasks. Sam Alman even had a very onbrand line about these systems. He said, \"The models just don't run out of dopamine. They keep trying. They don't run out of motivation.\" The point there is that AI agents can grind through attempts and retries without fatigue, which fits perfectly with debugging, refactoring, and long multi-step coding work. That's the open AI side of this drop. Faster agent loops, stronger terminal performance, much higher desktop computer use scores, internal use for training and deployment, and a cyber security capability label that triggered extra controls. Now over at the other camp, Anthropic launched their answer almost immediately and the focus shifts from raw terminal execution toward long context reasoning and coordinated AI agents. Anthropic introduced Claude Opus 4.6 and they rolled it out across major platforms starting with GitHub copilot through GitHub. It's available to Copilot Pro, Pro Plus, business, and enterprise users and people can select it in Visual Studio Code across chat, ask, edit, and agent modes. It also appears in Visual Studio, on GitHub.com, in GitHub mobile, through GitHub CLI, and inside the C-pilot coding agent. The rollout is gradual and enterprise admins have to enable a new policy before users get access. The headline technical feature is a 1 million token context window. That means clawed opus 4.6 can process huge code bases, long design documents, and deep project histories in a single context without dropping key information. Anthropic has been pushing long context reasoning as a core strength and this is them stretching that to an extreme scale. They also address the context rot problem where models struggle to retrieve information buried deep in long conversations or documents. On MRCR version 2, a benchmark for retrieving details from very large text. Claude Opus 4.6 scored 76% compared to 18.5% for the earlier",
        "start": 371.12,
        "duration": 1060.2400000000005
    },
    {
        "text": "stretching that to an extreme scale. They also address the context rot problem where models struggle to retrieve information buried deep in long conversations or documents. On MRCR version 2, a benchmark for retrieving details from very large text. Claude Opus 4.6 scored 76% compared to 18.5% for the earlier big improvement in memory retrieval over long spans of information. Claude Opus 4.6 can output up to 128,000 tokens in one response, which is enough to generate large code sections or long documents in a single go. The API includes adaptive thinking where the model decides when deeper reasoning is needed and four effort levels so developers can balance speed, intelligence, and cost. There's also a context compaction tool in beta that summarizes older parts of the conversation to keep longrunning tasks efficient. Anthropic also introduced agent teams inside clawed code as a research preview. This lets multiple AI agents work in parallel on different parts of a project. One agent might handle front-end code, another the API, another migrations, and they coordinate with each other. It's like managing a small team of digital developers that each own a piece of the stack. On benchmarks, Anthropic says Claude Opus 4.6 6 hits the top score on terminal bench 2.0 and leads frontier models on humanity's last exam, a broad reasoning test. On GDP vala, focused on economically valuable tasks like finance and law. They say opus 4.6 outperforms GPT 5.2 by about 144 ELO points, translating to a higher score roughly 70% of the time in those scenarios. Enterprise traction is another big part of anthropic story. They said Claude code reached a 1 billion revenue run rate within 6 months of general availability. Major companies using Claude include Uber, Salesforce, Accenture, Spotify, Racketin, Snowflake, Novo, Nordisk, and Ramp. On funding, Anthropic signed a term sheet for a 10 billion round at a 350 billion valuation, and they're preparing a tender offer so employees can sell shares at that valuation. Safety and alignment stay central for Anthropic. They report low rates of deceptive or harmful behaviors along with the lowest rate of over refusals among recent clawed models. They developed six new cyber security probes and are using Opus 4.6 internally to help patch vulnerabilities in open source software. The rivalry spilled into marketing too. Anthropic is planning Super Bowl ads that joke about open AI testing ads in chat GPT using the line ads are coming to AI but not to Claude. Sam Alman responded by calling the ads funny and misleading, arguing that Anthropic's approach focuses on expensive enterprise products. Financial markets reacted sharply. After Anthropic announced new automation tools, software and services stocks saw a combined 285 billion sell-off as investors started worrying about AI disrupting traditional enterprise software. Jensen Hang from Nvidia said fears of AI replacing software are illogical. And JP Morgan's Mark Murphy said assuming a new AI plug-in replaces missionritical systems feels like a stretch. Anthropic also pushed into Microsoft's ecosystem with a",
        "start": 537.839,
        "duration": 1404.8790000000006
    },
    {
        "text": "billion sell-off as investors started worrying about AI disrupting traditional enterprise software. Jensen Hang from Nvidia said fears of AI replacing software are illogical. And JP Morgan's Mark Murphy said assuming a new AI plug-in replaces missionritical systems feels like a stretch. Anthropic also pushed into Microsoft's ecosystem with a research preview, extending Claude into everyday office tools. Enterprise adoption numbers show how fast this space is moving. Andre Horowit's data shows Anthropic's share of enterprise production deployments rising from near zero in early 2024 to 44% by January 2026. OpenAI still leads with 77% of enterprises using its products in production. Average enterprise spending on LLMs reached $7 million in 2025, up 180% from 2024 with projections hitting 11.6 million in 2026. Pricing for Claude Opus 4.6 6 stays at $5 per million input tokens and $25 per million output tokens with premium pricing for prompts over 200,000 tokens. Anthropic also noted that if the model seems to overthink simple tasks, users can lower the effort level. So, here's the big question. When AI agents start handling full coding workflows inside and terminals, how fast do companies start shrinking engineering teams? Drop your take in the comments. If you enjoyed this breakdown, hit like and subscribe. Thanks for watching and catch you in the next one.",
        "start": 712.959,
        "duration": 1544.8790000000008
    }
]