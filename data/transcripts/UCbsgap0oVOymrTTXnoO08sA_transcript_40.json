[
    {
        "text": " Anthropic just dropped Bloom, a system that stress tests how models behave over long interactions Google released T5 Gemma 2 focused on AI that actually reads before answering NVIDIA dropped Neatron 3 built for longunning multivalent systems and Mistl just released OCR3 finally making messy documents usable for AI at scale A lot just dropped in AI, so let's talk about it All right so Anthropic released Bloom as an open source argentic framework for behavioral evaluations Bloom is essentially how Anthropic studies behavior in large AI models Not how smart they are not how fast they answer but how they act once you put them into longer more realistic situations And that distinction matters a lot more than it used to As models get more capable they also get better at presenting themselves well They respond politely follow instructions in clean demos and behave exactly the way you expect during short interactions The real complexity shows up when the task stretches out when instructions get vague or when the model has to make judgment calls over time That's where subtle patterns start appearing excessive agreement quiet self protection small deviations from intent that only become visible after many steps Until now studying that kind of behavior was a very manual process Researchers had to invent scenarios themselves write prompts by hands run countless conversations then read through long transcripts and argue about how to score them As models evolved those tests aged quickly and lost relevance which made comparisons harder and harder The workload kept growing while the signal stayed fuzzy Bloom changes that dynamic by turning behavior testing into an automated process Instead of relying on a fixed benchmark it starts with a single behavior definition That definition becomes the anchor for everything that follows From there the system generates entire evaluation suites on its own Each run produces new scenarios that still target the same underlying behavior So, the model gets placed into situations it hasn't encountered before while researchers still get consistent measurements they can compare over time Under the hood this works through a sequence of AI agents that handle different parts of the process One agent studies the behavior definition and example conversations to understand what the behavior actually looks like in practice Another agent comes up with realistic situations where that behavior might appears Another runs those situations against the target model Then judge agents analyze what happened and assign scores What Anthropic tracks most closely is how often the behavior appears strongly enough to matter across many different scenarios That single number gives teams a way to compare models changes and training approaches without relying on gut feeling And now this is where Bloom starts to matter beyond research labs Because the behaviors being studied are the same ones people notice when AI systems feel off Whether an assistant agrees too easily whether it slowly shifts priorities during a long task whether it starts optimizing for itself instead",
        "start": 2.639,
        "duration": 342.88200000000006
    },
    {
        "text": "And now this is where Bloom starts to matter beyond research labs Because the behaviors being studied are the same ones people notice when AI systems feel off Whether an assistant agrees too easily whether it slowly shifts priorities during a long task whether it starts optimizing for itself instead in a single answer They show up across time Anthropic tested Bloom across 16 Frontier models running 100 different scenarios per behavior and repeating the process multiple times They also tested it on intentionally misaligned models created specifically to behave strangely Bloom was able to separate those models from normal production ones in almost every case which tells you how sensitive this kind of testing has become They also checked how well automated judges line up with human judgment Claude Opus 4.1 showed a strong correlation with human labels especially at the extremes where decisions actually matter That gives it practical credibility not just theoretical value Anthropic describes Bloom as working alongside another system called Petri. Petri looks broadly across many behaviors while Bloom focuses deeply on one behavior at a time Together, they form a structure where models can be probed widely and examined closely before being deployed This actually shows where the real pressure is now models already reached a level where behavior drifts across long interactions and catching that manually stopped being realistic Automating this wasn't optional anymore Bloom exists because that threshold has already been crossed All right quick moment here to bring something useful into the mix for anyone who wants to level up their teach skills Try Hackme stepped in to support today's video and they're running their Advent of Cyber 2025 promotion right now daily festive challenges and 30% off annual subscriptions Try Hackme is one of the most accessible ways to get into caber security because everything is hands-on, ramified and built around real attack and defense scenarios You open your browser launch a virtual machine in a split screen and start hacking through guided tasks labs and full learning paths that cover everything from absolute beginner content to red teaming SOC workflows, web ape security cloud and pen testing They have more than 900 labs structured paths dreamworld challenges and a format that feels fast and engaging instead of slow and academic Points, streaks certificates practical exercises the whole thing is optimized for people who want real skills without getting lost in textbooks If you want the advent of caber discount grab the link below and upgrade while the offer is still live Now, let's get back into the story Now, let's move to Google because they just released T5 Gemma i which is a new family of open encoder decoder transformer models built by adapting Gemma 3. This one is not about chasing chariot vibes or viral moments This is Google doing what Google usually does best building foundations that other systems quietly rely on later Most modern models are optimized to respond fast They're great at producing",
        "start": 174.16,
        "duration": 663.0410000000003
    },
    {
        "text": "encoder decoder transformer models built by adapting Gemma 3. This one is not about chasing chariot vibes or viral moments This is Google doing what Google usually does best building foundations that other systems quietly rely on later Most modern models are optimized to respond fast They're great at producing explanations The cracks start showing when you give them a lot of material and expect them to actually digest it Long documents mixed inputs reports with charts text plus images things where missing one detail changes the entire outcome That's the gap this model is trying to close T5 Gemma 2 is built around a simple idea Let the model fully process everything first then generate output based on that understanding That's why Google went with an encoder decoder setup here The encoder's entire job is to take in the input and form a solid internal picture of what's going on The decoder then works off that picture instead of juggling raw input and generation at the same time This matters in very practical situations internal search tools document analysis research assistance systems that deal with long reports legal files product specs or messy real world data Places where an AI skipping a paragraph or misreading a diagram isn't a small mistake It breaks the whole task T5 Gemma 2 carries over what already worked well in Gemma 3. It handles text and images together It works across more than 140 languages It fits comfortably into modern production expectations None of that is positioned as a breakthrough here It's treated as baseline The focus is on structure and reliability Google released three sizes a 270 million version a 1 billion version and a 4 billion version with the encoder and decoder matched in size When you look at the full setup excluding the vision component those land at roughly 370 million 1.7 billion and 7 billion parameters The vision encoder adds another 417 million parameters and stays frozen which keeps the system stable when handling images What's interesting is how Google built this without starting from scratch Both sides of the model are initialized from a restrained Gemma checkpoint then refined further using the UL2 training objective Images are converted into compact representations using SIGLIP and fed directly into the encoder alongside text Everything gets blended at the understanding stage where full attention makes sense By the time the decoder starts producing text it's working with something structured not raw input That separation reduces confusion and keeps generation focused It also makes the whole system easier to adapt later for specific tasks which is clearly the point here Google also made a few efficiency choices that quietly matter Word embeddings are shared across the encoder and decoder instead of being duplicated The attention mechanism inside the decoder is streamlined so it doesn't bounce between separate modest These changes don't make headlines but they make the model easier to train easier to adapt and cheaper to run at",
        "start": 335.919,
        "duration": 970.3190000000005
    },
    {
        "text": "matter Word embeddings are shared across the encoder and decoder instead of being duplicated The attention mechanism inside the decoder is streamlined so it doesn't bounce between separate modest These changes don't make headlines but they make the model easier to train easier to adapt and cheaper to run at approach Google introduced with Gemma i a mix of local and global attention keeps things stable when inputs get large This isn't framed as something extreme or experimental It's just treated as normal infrastructure the kind of thing models are expected to handle Now, training details reinforce that mindset Around two trillion tokens large batch sizes conservative optimization settings checkpoint averaging nothing flashy everything aimed at producing a base model that behaves predictably when people build on top of it The important part is what this enables downstream T5 Gemma 2 is the kind of model you use when the cost of misunderstanding is higher than the cost of waiting an extra moment for an answer When accuracy over long inputs matters more than conversational speed When the system needs to feel like it actually processed what you gave it This is Google investing in AI that understands before it responds Not because that sounds impressive but because too many systems right now fail quietly by skimming instead of reading All right Now, Nvidia just released Neotron i and this one is built for longunning multivalent setups The kind where AI systems work together share memory and handle huge amounts of information without turning inference into a disaster Neotron 3 comes in three versions Nano sits at around 31.6 billion total parameters super around 100 billion and Ultra pushes toward 500 billion Those numbers sound massive but what matters is how many parameters are active at any moment In kano only about 3.2 billion parameters are used per token Super goes up to around 10 billion Ultra to about 50 billion The rest stay idle That's the whole trick here Instead of firing the entire model every time Neotron 3 selectively activates only the parts it needs You get the capacity of a huge model without paying the full compute cost on every step That's what makes this usable for real systems instead of just demos Architecturally, it mixes Mamba two blocks attention and sparse mixture of experts layers Mamba handles long range sequence modeling efficiently Attention steps in where structure and reasoning matter The expert layers give the model specialization without turning everything on at once For Nano, that can mean routing a token through six experts out of 128. In practice this lets a 30 billionclass model behave much closer to a three billion class model at inference time while still having a lot more knowledge available when it needs it Nvidia reports that Nematron 3 Nano delivers around four times higher token throughput than Nematron 2 Nano while also reducing the number of reasoning tokens needed to finish tasks That",
        "start": 492.479,
        "duration": 1273.679
    },
    {
        "text": "a three billion class model at inference time while still having a lot more knowledge available when it needs it Nvidia reports that Nematron 3 Nano delivers around four times higher token throughput than Nematron 2 Nano while also reducing the number of reasoning tokens needed to finish tasks That more complex The models are designed to operate over very large shared memories up to 1 million tokens The point here isn't the number itself It's the idea that these systems are meant to keep track of long workflows, revisit earlier steps and coordinate over time instead of constantly resetting context For the larger super and ultra versions Nvidia adds latent MOE, where expert computation happens in a compressed latent space before being projected back This allows more experts to exist without blowing up communication costs They also add multi-token prediction where the model predicts several future tokens per forward pass which speeds things up during inference Training scale matches the ambition around 25 trillion tokens total with more than 3 trillion new tokens compared to the previous generation Super and Ultra rely heavily on NVFP4, Nvidia's bit floatingoint format to keep throughput high while maintaining accuracy What Nematron 3 really represents is AI that can stay efficient while thinking long term systems that don't fall apart once tasks stretch out memory grows or multiple agents start working together That's the direction Nvidia is pushing here Finally, let's talk about Mistral AI because this update fixes a very real very common problem They just released Mistral OCR3 and the simple idea here is turning messy documents into something AI can actually used PDFs, scans forms invoices handwritten notes Most important data still lives there and bad OCR breaks everything that comes after OCR 3 is built for those exact cases Forms with boxes loquacity scans handwritten text mixed with print and especially tables that usually fall apart On Mistrol's internal tests with real business documents it beat the previous version about 74% of the time which basically means fewer silent mistakes in real workflows. What makes it useful is the output It keeps layout intact Tables stay tables when needed It returns proper structured formats instead of a wall of text That makes a big difference for search analytic and AI agents that rely on clean structure It's also easy to used You can try it in Mistral's document AI playground by uploading a file And the same pipeline is available through an API for production used PDFs, Word files PowerPoint files images all handled the same way Pricing is a bit aggressive i per 10,000 pages normally i per 10,000 pages with batch processing that suddenly makes large-scale document processing affordable not just possible So basically OCR3 removes one of the biggest friction points between real world data and AI systems and that's exactly why it matters AI is clearly moving toward longer tasks deeper workflows and more autonomy across the board Drop your thoughts in the",
        "start": 646.079,
        "duration": 1605.5180000000007
    },
    {
        "text": "large-scale document processing affordable not just possible So basically OCR3 removes one of the biggest friction points between real world data and AI systems and that's exactly why it matters AI is clearly moving toward longer tasks deeper workflows and more autonomy across the board Drop your thoughts in the this evolving If this breakdown was useful hit like subscribe for more AI updates like this and thanks for watching Catch you in the next one",
        "start": 814.48,
        "duration": 1624.7990000000007
    }
]