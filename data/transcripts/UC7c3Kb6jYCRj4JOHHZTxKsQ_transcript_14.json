[
    {
        "text": " We now have a fully open-source open weights texttovideo model. This is LTX2 and now it is completely open. You can download and run it on your local machine. Here's the announcement. The release makes a full LTX2 development stack publicly available. You get the model weights, both the main model and the distilled version, enabling local inference, fine-tuning, and production use across a range of hardware, which means you can download it and you can customize it for your own needs. They are also offering Laura adapters, so you get to see how they've customized the model for specific styles and tasks, and that might give you inspiration for how you might do it. They're also providing something called trainer, a modular training framework that includes multimodal data pipelines, reference configurations, multi-GPU support and evaluation tools. They are basically giving you the full stack to do anything you want with this texttovideo model. Really the first time we're ever getting this much support from a company. LTX2 is optimized for Nvidia machines and specifically it can run well on an RTX5090, but those chips might be getting pretty expensive very soon. More on that later. A few more things about this model. You can get up to 20 second generations, but you have control over the first and last frame. So, you can tie together these 20 second clips into however long you want. The special thing about LTX in particular is that it gives you kind of this storyboarding control as if you were sitting inside of a movie studio. With these 22nd generations, they are 4K and up to 50 frames per second. So very high quality. It also has very accurate lip sync. So obviously it comes with audio and you get incredibly fine grain control over everything including multi keyframe conditioning, Laura based customization, 3D camera logic and multimodal inputs including text, video, audio and more. So download the model, download the code, download the pipeline all from GitHub, from HuggingFace, run it locally. Of course, if you don't want to run it locally, you can use a cloud provider. And all of this is listed in this announcement. I'll link that down below. And LTX2 has already taken the number one spot on the artificial analysis open weights leaderboard. This is the number one video model that is also open source open weights. Go check it out. It's super cool. I'm thinking about making more videos, maybe a tutorial about how to plug it into Comfy UI. Let me know if you want to see that in the comments. And they even released a research paper detailing all of the techniques that they use to create this model. Here's a few demos of what you can actually get out of LTX. Look at the quality. Look at the detail. Look at the realism of the physics, the different styles that you can use. All of this is",
        "start": 0.08,
        "duration": 342.561
    },
    {
        "text": "all of the techniques that they use to create this model. Here's a few demos of what you can actually get out of LTX. Look at the quality. Look at the detail. Look at the realism of the physics, the different styles that you can use. All of this is I just got back from CES. I attended Jensen's keynote where he announced multiple new products including this absolute beast of a supercomput called Reuben. This is their next generation architecture. This is not consumer grade hardware. This is what the hyperscalers and model providers like OpenAI and Enthropic and AWS. This is what they're going to be buying. Rubin delivers up to a 10x cost reduction in inference and a 4x reduction in the number of GPUs required to train a mixture of experts model. This is all compared to the Blackwell platform, their current Frontier supercomputer. It's also much more power efficient and something that kind of blew my mind, it doesn't even require water coolers. It's basically cooling the system with hot water. And if that sounds weird, that is literally what Jensen said. Look at this. Very importantly, the water that goes into it is the same temperature, 45\u00b0 C. With 45\u00b0 C, no water chillers are necessary for data centers. We're basically cooling this supercomput with hot water. Is so incredibly efficient. Each of these has 1,152 GPUs, which is just insane, and 16 racks to power it all. Each rack has 72 Reubins. Elon Musk commented on this expost. It will take another 9 months or so before the hardware is operational at scale, and the software works well. That might be true, but it's already in production. And they've greatly simplified this machine. No cables, no hoses, no fans. It is as close to being cooled ambiently as you can be. And as I mentioned, it's already in full production and should be available later this year to Nvidia's partners. And of course, one of the biggest use cases for all of this AI power is coding, which is why I'm excited to tell you about today's sponsor, Grapile. AI has gotten pretty good at writing code, but what you may not have realized is that with the right tools, it can be really good at reviewing code, too. That's where Grapile comes in. Gretile is AI that reviews all of your code automatically. And the GPile team has spent a lot of time on this reviewing code problem. And they showed me some incredible bugs and errors and issues that Greile was able to find that humans overlooked. In fact, they have an entire page dedicated to showing off some of those crazy findings. And Gretile is completely free for open source and has some massive repos using it today, including Nvidia, Postthog, Storybook, and more. So check out Graptile. They're offering a 14-day free trial right now. I'll drop a link down below. Check them out to review all",
        "start": 171.04,
        "duration": 673.6800000000002
    },
    {
        "text": "of those crazy findings. And Gretile is completely free for open source and has some massive repos using it today, including Nvidia, Postthog, Storybook, and more. So check out Graptile. They're offering a 14-day free trial right now. I'll drop a link down below. Check them out to review all another layer of confidence to your deployment cycle. Now, back to the video. And next, OpenAI is launching ChatGpt Health, a major feature that, at least for me, is very welcome. ChachiPT health is a dedicated experience that brings your health information into CHACPT so that it could proactively give you health recommendations. So you can plug in things like your Whoop device if you have that Oro ring Apple health and all of this. You plug it into Chad GPT and it will have all of that information for whenever you ask it health related questions but also if you want it to proactively give you recommendations about your health. You can connect your medical records, wellness apps, anything. I already use ChachiPT to help me understand test results from my doctor, scans, different things about my health. I'm asking ChachiPT first. And of course, I'm going to a human doctor, but I want to arrive at the doctor's office better informed. And so now giving it access to my health data seems like a natural next step. Now, I know a lot of you are probably not so willing or excited to share all of your health data with OpenAI. And I kind of understand that. And this is actually where I think Apple might be just once again getting in their own way. For folks who use the Apple ecosystem, they already have all of your health data built into their Apple Health app, which people seem to be very comfortable sharing with Apple. And they are very big on local artificial intelligence. So AI running on your phone, on your laptop. And so you can imagine a world in which you don't actually have to share your health data with a third party like Chad GPT and Apple could use its AI to give you those proactive recommendations to answer your questions about your health data without it ever leaving your phone. But of course they're not doing that at least not yet. And so that's where chatbt comes in. So there is a wait list for this feature. It is available to essentially all the plans. the free, the go, the plus, the pro plans, all of them have it as soon as it goes public with one big exception and that's Europe. Of course, Europe has much higher regulation and thus they're not going to have this available, at least not on day one. So, this is going to operate as a completely separate area in Chat GPT. And the thinking is to increase privacy and security so that if you happen to share a chat or you're",
        "start": 339.84,
        "duration": 964.2419999999997
    },
    {
        "text": "regulation and thus they're not going to have this available, at least not on day one. So, this is going to operate as a completely separate area in Chat GPT. And the thinking is to increase privacy and security so that if you happen to share a chat or you're the time, you won't accidentally share your health information. And of course, your health information is not used to train the chat GBT models. So go sign up for the weight list and hopefully we get it soon. I haven't actually played around with it. I just pinged my contact at OpenAI. Hopefully I get access and I'll show it off to you when I can. All right. I mentioned that GPU prices might be going up and yeah, they actually might be. And it's all because of the lack of inventory of memory, one of the core components of GPUs. We might be paying up to $5,000 for the latest RTX5090. According to Newswire, Nvidia and AMD to significantly increase GPU prices starting next month. RTX 5090 increasing from 2,000 to 5,000. However, there is a community note on this post. Reports suggest GPU prices might rise in early 2026 due to memory shortages and some speculate high-end cards could fetch $5,000, but neither Nvidia nor AMD has officially confirmed these price hikes. But the rumors are happening and somebody just asked Jensen about it during the live Q&A at CES. Tom's Hardware in response to these potential price increases of the latest generation GPUs asked Jensen specifically, do you think that maybe spinning up production on some of the older generation GPUs on older process nodes where there might be more available production capacity would help with that? Or maybe also increasing the supply of GPUs with lower amounts of DRAM. Are there steps that could be taken or any specific color you can give on that? And Jensen said, \"Yeah, possibly. And we could possibly, depending on which generation, we could also bring the latest generation AI technology to the previous generation GPUs, and that will require a fair amount of engineering, but it's also within the realm of possibility. I'll go back and take a look at this. It's a good idea.\" So, price increases seemingly are coming. There's a few different solutions that Nvidia is exploring, but the entire industry is feeling the pinch of memory shortages. So, if you don't already have a 5090 and you want one, now is probably a good time to get it. And next, in what I thought was probably the coolest announcement of all the announcements that Nvidia made, Alpa Mayo, their model and really full stack architecture for autonomous vehicles. Now, why is this so special? Well, first of all, it's just really cool. Autonomous vehicles is so futuristic. It is so cool. And Tesla and Whimo work incredibly well. In fact, my friend drove me in a Tesla from my house",
        "start": 487.44,
        "duration": 1271.4429999999995
    },
    {
        "text": "and really full stack architecture for autonomous vehicles. Now, why is this so special? Well, first of all, it's just really cool. Autonomous vehicles is so futuristic. It is so cool. And Tesla and Whimo work incredibly well. In fact, my friend drove me in a Tesla from my house us the entire way. No disengagements. It was truly impressive. Now, back to Alpameo. And so, how did Tesla and how did Whimo get so good? Well, they had millions and millions of miles of training data to train their models on. But that's where Alpameo really shines. Nvidia is not a car company. They don't have millions of miles of data to train their models on. So what did they do? They used synthetic data. They generated synthetic video clips of a car driving around different cities and used that synthetic data to train the models. So they didn't need millions and millions of real world miles. They simply use synthetic data. Now what also makes this special? It is completely open source. The models you can download. You can play around with them. the entire stack you can download. And basically, if you own a car company, you can implement it yourself. And it's also a reasoning model. So, not only is it a vision model, but it is a vision reasoning model, which seems to be different from Whimo and Tesla, but nothing confirmed because those are completely closed systems. According to the announcement, Nvidia is the first to release an open reasoning VLA, I think vision, language action model designed to tackle longtail autonomous driving challenges. This Nvidia Alpameo family also includes simulation tools and data sets for AV development. It includes Alpameo 1, Alpa SIM and physical AI open data sets enable the development of vehicles that perceive, reason, and act with human-like judgment. And they actually showed off their partnership with Mercedes. And a Mercedes vehicle with cameras only driving around San Francisco quite successfully. And so what Jensen said is Alpameo is an end toend AI architecture. Video in, actuation out, meaning it looks at the world and then controls the steering wheel and controls the acceleration and the braking. It is so impressive and it was trained using only 1,700 hours of driving data, which is really not a lot, but apparently it's really good. So, any car company now can catch up to Tesla's drastic lead in autonomous driving, but I don't actually think they're going to. Some of these legacy auto manufacturers just cannot get out of their own way. And so, even though all of these tools are essentially free, obviously Nvidia wants you to buy their GPUs to power it all in the vehicle, I just don't think they're going to be able to implement it successfully. And it's also interesting when you look at Tesla versus Whimo. Whimo uses LAR, radar, all of these different sensors which are quite expensive. And Tesla took a completely",
        "start": 642.24,
        "duration": 1600.2419999999997
    },
    {
        "text": "GPUs to power it all in the vehicle, I just don't think they're going to be able to implement it successfully. And it's also interesting when you look at Tesla versus Whimo. Whimo uses LAR, radar, all of these different sensors which are quite expensive. And Tesla took a completely only. It is cameras on the road making decisions. And it seems like the Alpameo system is more akin to that approach, although I've been hearing it also can use LAR. And next, breaking. At the time of recording this video, Anthropic has raised a new massive funding round. According to the Wall Street Journal, Anthropic is raising $10 billion at a $350 billion valuation. Now, for context, Open AAI is at I think about a $750 billion valuation. So, Anthropic is coming in at just about half, but Anthropic is nearly profitable. They are generating a great amount of revenue. And so, this is a continuation of Anthropic success. This is in addition to the 15 billion that Nvidia and Microsoft are investing in Anthropic. And if that sounds weird because Microsoft is also invested in OpenAI, well, so is Nvidia. They are just on a bet on everything strategy. So that's it for today. If you enjoyed this video, please consider giving a like and subscribe.",
        "start": 809.68,
        "duration": 1724.1629999999998
    }
]