[
    {
        "text": " So, Chat GBT just lost the fight to keep its chat GBT logs private in the copyright case. This is absolutely insane and we have to talk about it. So, this is insane for privacy. OpenI has been ordered by a United States judge to hand over around 20 million anonymized Chad GPT user logs to the New York Times and other news outlets as part of their copyright lawsuit after losing its attempt to keep those logs confidential. The court held that those logs are relevant evidence and that privacy concerns can be addressed through deidentification and protective orders, but they still must be produced in discovery. This is insane for privacy because most of you guys use chat GPT on a daily basis. Most of [music] you guys use AI and LM on a daily basis, but now we're seeing one of the first cases where our private conversations might not be so private. Okay, so this is crazy because the judge literally rejected opening eyes request. So if you aren't familiar with what caused this entire debacle, I actually spoke about this last year and essentially the problem is that we all know this, okay, let's be honest here. OpenAI and all of these chat models, they essentially trained on millions of pieces of text that were on the internet. Now the real question that the New York Times is arguing is that they trained on our articles [music] and we didn't get paid. That's what the New York Times are saying. And I spoke about this in like 2024. I think it was 2024, early 2025. I don't remember when the, you know, the exact case was, but this has been going on for a long, long time now. And the New York Times are basically saying, \"Look, we want a piece of the pie because you guys are making billions of dollars.\" And well, you didn't really ask us. And there have been many different copyright suits. But the problem is is that right now there is a problem because in order to see whether or not they actually trained on conversations, they're basically saying, \"Look, we need to see the users conversations to see if the New York Times content was actually reproduced.\" It's pretty crazy. Okay, so you can see that the judge rejected OpenAI's request. The judge rejected OpenAI's privacy related objections to an earlier order requiring the AI startup to submit the records as evidence. There are multiple layers of protection in this case precisely because of the highly sensitive and private nature much of discovery. An OpenAI spokesperson on Wednesday cited an earlier blog from the company's chief information officer which said that the Times demand for the chat logs disregards long-standing privacy protection and breaks common sense security practice. And openly, you know, they've appealed this, but the judge has basically request rejected this. So, this is pretty crazy because I don't think people understand just how",
        "start": 0.08,
        "duration": 301.0390000000001
    },
    {
        "text": "information officer which said that the Times demand for the chat logs disregards long-standing privacy protection and breaks common sense security practice. And openly, you know, they've appealed this, but the judge has basically request rejected this. So, this is pretty crazy because I don't think people understand just how know, later on in the video, I'm going to speak to you guys about why this is a fundamental shift for AI. And it's probably one of the most important videos should you should watch because if you use AI on a daily to day-to-day basis, I think this should probably change how you use AI on a fundamental level because most people don't realize the level at which their data is exposed. And I'm going to show you guys a lot of big companies are harvesting your data, including Meta, OpenAI, you know, all of these companies. And there are some certain settings that you guys need to just enable. and I'll show you how to to turn those on later on in the video. So, this is of course, you know, happening now because, like I said at the start, they basically want to see those logs between users to see whether or not chat GBT actually reproduced the New York Times copyrighted content. So, you know, the crazy thing about this is that, you know, OpenAI essentially looked at the New York Times. So when the New York Times presented this crazy lawsuit, they basically said, you know, opening eye in response, they were like, well, when we look at what you guys are saying here, it actually looks like you hacked quote unquote the chatbot to manufacture evidence. Now, I guess what you would call this is prompt engineering. If you ever look at the, you know, first actual lawsuit from New York Times, a lot of the responses that were verbatim New York Times articles, I don't know how you would get those without prompt engineering the chatbot to do so. So that's kind of the point they're saying. They're saying, \"Look, if we did prompt engineer it, why can't we see the chat logs?\" It's just a complete fourth back back and forth. And it's gotten to the point now where, of course, OpenAI counted that turning over the logs would disclose confidential user information that 99.9% of the transcripts have nothing to do with the infringement allegations. And Wang said in her initial order to produce the chat that OpenAI's users privacy would be protected by exhaustive deidentification and other safeguards. and Wang reiterated on Wednesday that the company measured would be reasonably mitigate associated privacy concerns and Wang ordered OpenAI to produce the logs within 7 days of removing the users's identification. So basically what they're going to do is they're going to, you know, remove users identifying information. So if you put, you know, your name, your number, and all that stuff, they're going to remove it. But",
        "start": 151.12,
        "duration": 548.399
    },
    {
        "text": "OpenAI to produce the logs within 7 days of removing the users's identification. So basically what they're going to do is they're going to, you know, remove users identifying information. So if you put, you know, your name, your number, and all that stuff, they're going to remove it. But watch this video until the end because after I saw this article on Twitter, I was tweeted out and I was reminded by it, I actually went and changed a bunch of different settings. I went and downloaded a few different open source AI models and I just have to protect myself because you never know how crazy the future is going to get and data privacy is more important than ever. So, you know, why is this happening now? So, I want to show you guys quickly an article from the New York Times and showing you how OpenAI actually responded to this and why this entire thing, the fact that the judge actually rejected them is pretty crazy. So, if you come over to this blog here, you can see June 5, 2025. This was actually a little bit of time ago. You can see that you know after months of litigation we are no longer under legal order to retain consumer and API content indefinitely and our obligations under the early order ended on September the 26th. So you have to understand that what they're talking about here is the data privacy and OpenAI in all defense they are trying to be transparent as possible because they know that individuals and data privacy and the whole Facebook scandal you really don't want to have another situation like that because regaining user trust after losing it is very hard to do and OpenAI is in a rocky road at the moment they are on you know fighting so many different battles they're fighting Google they're fighting copyright they're just fighting you know an uphill battle at this point and so what they're saying about your conversations with chatbt is that the New York Time continues to demand that OpenAI to keep a specific set of user data from April to September 2025. So while we're no longer required to indefinitely retain new user data going forward or any conversations originating from the EU, Switzerland, the United Kingdom, we will securely store limited historical data from April, you know, 25th to September 2025. It remains locked down accessible to only small audited legal and security team and it can't be used for anything other than meeting obligations and the data will not be turned over to New York Times court or anyone else at this time and we will continue to fight these norms. Of course, now we know that they will have to actually hand that over. But it was pretty crazy that the judge said that, you know, you have to indefinitely retain new user data going forward. Like that's insane. Like that that's an",
        "start": 276.32,
        "duration": 796.8790000000004
    },
    {
        "text": "we will continue to fight these norms. Of course, now we know that they will have to actually hand that over. But it was pretty crazy that the judge said that, you know, you have to indefinitely retain new user data going forward. Like that's insane. Like that that's an But, you know, of course, now now they're no longer required to do that and they're now going to be deleting chats. Think about how many chats you've had with chat GBT that you probably thought were private even in the temporary chats that you thought were deleted. Sometimes, you know, with this new law and with these kind of courtrooms, it's kind of scary that sometimes it's not there. So, of course, OpenAI has given some questions. They said, why are the New York Times and plaintiffs asking for this? The New York Times is suing OpenAI as part of their baseless lawsuit. They've recently asked the court to force us to retain all user content indefinitely going forward based on the speculation that they might find something that supports their case. That's insane. That's basically saying anyone that has a conversation with Chat GBT, apologies for speaking so fast. Anyone that has a conversation with Chat GBT, they wanted them to retain that conversation. That's that's that's just like an overreach. Okay. Of course, thank god Openi are fighting this and they said, \"Is my data impacted?\" Yes, if you have chatbt free plus and pro and a team subscription or if you use an openi API without a zero data retention agreement and this does not impact chat enterprise or chat edu customers. So you have to understand that this is pretty pretty bad for those of you who you know use chatbt without the knowledge that your data was going to be retained to some extent and I think this is like a wakeup call in the sense that guys if you're using an AI on the cloud you are sending that you know data off to the cloud okay and you know the crazy thing about this as well is that even if you deleted your data from chat GBT will it still be retained under this order the New York Times is demanding that we retain even deleted chat and API content that would typically be removed from our systems within 30 days. And that's why I said that is the crazy part because this is the first lawsuit from chat activity. What about if other companies have to comply with this or there are other you know data restrictions or you know copyright cases where your data is just in some file or in some kind of copyright suit. I mean it's pretty crazy right? So of course they say you know this data is not automatically shared with New York step and we're going to push and if they push for this access in any way we're going to fight to protect",
        "start": 402.08,
        "duration": 1037.0390000000007
    },
    {
        "text": "in some kind of copyright suit. I mean it's pretty crazy right? So of course they say you know this data is not automatically shared with New York step and we're going to push and if they push for this access in any way we're going to fight to protect pretty crazy that at one point the court was, you know, the court order was forcing them to retain chativity and API content going forward. That was pretty crazy in June. Like I I can't believe I didn't see this at the time. So it's pretty crazy. Now, I think what this should show you from this article, and I'm not sure if this is going to happen, if this would happen, because, you know, the frontier is pretty quick, but I think for a large, you know, in fact, maybe a private subset of people, you're probably going to want to shift to local AI. Now, I made a tutorial on how to install, you know, the OpenAI model for free, a pretty decent, you know, open source model, but I don't think the average person needs a PhD level AI on a constant regular basis. There are many, you know, open source AI tools that you can run on device that perform remarkably well. And of course, I do think that there are things like images and, you know, reasoning and, you know, generic capabilities that you just can't get on an open source and not never mind, you know, the GPU constraints because if you don't have a powerful enough computer, sometimes running that can be a little bit difficult. So, of course, it's not just easy for everyone to just shift off onto local AI. But the point I'm trying to make here guys is that you need to be a bit careful with how you put your informations into chat GPT because often times those conversations aren't as private as you would think. Now I got to be honest, this isn't just an opening eye thing. Google as well are unfortunately a culprit of this. By default, your conversations with Google Gemini are actually being used to train the next set of models. So those conversations aren't private unless you enable certain settings. And I know that sounds crazy, but I will literally, you know, give you guys a link in the description. I made a two tutorials yesterday. One on how to disable it on Google Gemini, one on how to disable it on Chat GBT. But even then, if some kind of core restriction comes up, those conversations might not be used to train the next set of models, but they might still be retained. So, it really does depend on what kind of sensitive data you're using. The point I'm trying to say here guys, if you have any sensitive data, you want to ideally shift that to a local AI system, a private, you know, language model that's",
        "start": 523.599,
        "duration": 1279.8380000000009
    },
    {
        "text": "they might still be retained. So, it really does depend on what kind of sensitive data you're using. The point I'm trying to say here guys, if you have any sensitive data, you want to ideally shift that to a local AI system, a private, you know, language model that's and maybe even buy one of those, you know, small computers that is just built completely for running at home on device AI that is not connected [music] to the cloud. That way, you can completely stay safe if your data is, I guess you could say, really important to you and it's anonymized. That would be my best advice. I'm going to leave three links. One to how to install the, you know, OpenAI GPT OSS, one to how disable them from retraining on your conversations for OpenAI, and one for Google Gemini. If you guys enjoyed the video, hopefully it made you think differently about how we use AI going forward. It's going to be really interesting to see how this impacts the landscape. I don't think it will too much, but for those users who are private and you really don't want to have OpenAI messing around in there, don't forget to check",
        "start": 646.64,
        "duration": 1361.999000000001
    }
]