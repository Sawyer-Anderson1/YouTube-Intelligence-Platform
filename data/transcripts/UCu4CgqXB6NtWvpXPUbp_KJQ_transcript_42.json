[
    {
        "text": " AI inference is being used by every industry and creating billions of tokens every day across healthcare, financial services, and the public sector. All this requires a platform that can deliver the performance and efficiency to deploy inference at scale. Today, we're announcing breakthrough performance on leading open source models like Deepsee R1, GPTOSS, and Llama based on the inference max benchmark. It's a new benchmark that's designed not just to understand performance, but also the cost and efficiency across a wide range of configurations. And it gives you an understanding of what it requires to deploy inference at scale. And the results are clear. Blackwell delivered leadership performance and efficiency across the board. In fact, a single GB200 MBL72 system can generate enough tokens to create $75 million of revenue. That's a 15x return on investment on GPT OSS. Leveraging the latest TRT LLM software improvements, we're able to generate 60,000 tokens per second per GPU. And for dense open models like Llama, we're able to generate 10,000 tokens per second per GPU. That's actually 4x compared to our previous generation Hopper platform. And it's not just about performance. Efficiency is key. For power limited data centers, Blackwell can deliver 10x more performance per megawatt compared to our previous gen Hopper platform. And that translates into not just more tokens, but more revenue for AI factories everywhere. Expect to see new results from Blackwell Ultra, as well as more software improvements and enhancements that will drive more performance and efficiency for AI factories.",
        "start": 0.4,
        "duration": 211.35899999999995
    }
]