[
    {
        "text": " So, Claude Opus 4.5 rolled out and suddenly you've got an AI beating every human on anthropic's hardest engineering test, running long form agents and pushing past benchmarks that were never designed for something this smart. At the same time, Anthropic signed a massive Azure compute deal. Open AAI dropped new upgrades in chat GPT, and Google quietly added more tools into Gemini. But first, let's talk about Opus 4.5. All right, so the moment Anthropic mentioned that Opus 4.5 scored higher than any human candidate ever on their 2-hour take-home engineering exam, people fixated on that. And honestly, it makes sense. The test is famous inside the company because it's meant to push candidates into the pressure zone where they have to design, build, debug, and adjust a working system with several constraints. Anthropic said the test measures technical ability and judgment under time pressure. So not communication or teamwork, just raw technical thinking. Humans get two hours. Claude got the same limit but with multiple runs per problem so that the best solution could be selected. And even with that advantage, the result still makes a statement because the test is used as an internal benchmark for the strongest applicants. Now there isn't a public breakdown of what that exam looks like. just a short glass door review from 2024 saying the test has four levels and makes candidates implement a system and add functionalities. Anthropic didn't confirm whether Claude saw the exact same version and they didn't share details of the tasks, but the internal takeaway was that the model solved it within the rules and outperformed every human they've evaluated so far. And this lines up with something Daario Amod said earlier at Dreamforce where he mentioned that Claude already writes about 90% of the company's code. He also said engineers aren't being replaced, they're supervising the model, correcting the toughest logic and guiding the direction of whole projects. Opus 4.5 fits into that pattern even more tightly. Now what makes the new model stand out isn't just benchmark scores, though those are strong as well. Opus 4.5 leads on SWEBench multilingual AC. across seven of eight programming languages and hits 80% on swbench verified, which is the highest score recorded so far. And beyond accuracy, it's better at handling ambiguous bugs. Testers said the model doesn't panic or freeze when a bug spans multiple systems. It breaks the issue down calmly and figures out a chain of fixes without a lot of back and forth prompting. Anthropic also pointed out that Opus 4.5 has improved vision, reasoning, and math skills compared to the previous generation and that its state-of-the-art across multiple domains they evaluate. One example that stood out came from TA 2 bench. That's a realworld agent benchmark where the model acts like an airline service rep. A distressed customer wants to modify a basic economy ticket. The benchmark is designed so that the correct response is to decline the request. basic economy",
        "start": 2.48,
        "duration": 364.0789999999998
    },
    {
        "text": "One example that stood out came from TA 2 bench. That's a realworld agent benchmark where the model acts like an airline service rep. A distressed customer wants to modify a basic economy ticket. The benchmark is designed so that the correct response is to decline the request. basic economy read the airline policy all the way through, noticed a loophole that the test creators didn't expect, and solved the problem legally. It upgraded the cabin first because cabin upgrades are allowed, and then modified the flight, which becomes allowed once the ticket isn't basic economy anymore. The benchmark marked it as a failure because the creators didn't anticipate anyone thinking that creatively, but testers described that behavior as exactly what makes the model different. It's not limited to the most obvious path. It looks at rules like a human who's been doing the job for years. Now, creative workarounds can be dangerous when models start bending instructions. So, Anthropic talked a lot about safety as well. They've been running Opus 4.5 through an upgraded version of Petri, their automated evaluation tool. and through strong prompt injection tests developed by Grey Swan. Opus 4.5 ended up being the hardest model in their lineup to manipulate with adversarial prompts. These are the extreme attacks where someone embeds a harmful instruction inside harmless text to trick the model into doing something outside its safety boundaries. Anthropic said Opus 4.5 is their most robustly aligned release and they believe it's the most aligned Frontier model available right now. They've been trying to build street smarts into the system so it recognizes when someone is trying to lure it into a harmful situation. And this isn't just theoretical. Many of their enterprise customers use Claude for sensitive workflows. So avoiding prompt injection isn't optional. Anyway, the safety improvements sit next to efficiency upgrades. Anthropic introduced a new effort parameter in the API that allows developers to control how deeply the model reasons. At medium effort, Opus 4.5 matches Sonnet 4.5's best SWE verified score while using 76% fewer output tokens. At maximum effort, it beats Sonnet 4.5 by 4.3 percentage points while still using 48% fewer tokens. And these aren't tiny savings. When companies run tens of thousands of queries a day, this kind of efficiency directly cuts costs. The context system got a major boost, too. Now, the model can shrink older parts of the conversation on its own, which keeps these huge hourslong chats from falling apart. So, instead of hitting that classic limit where the AI suddenly forgets what happened earlier, Opus 4.5 keeps everything stable and consistent. The memory tools also help with long running agent sessions, which is a big deal for companies that run tasks for hours at a time. And when anthropic combined all of this, the context compaction, the way sub aents coordinate, and the new effort controls, opus 4.5 jumped from 70.48% to 85.3% on their deep research test. That's a massive improvement that comes",
        "start": 184.159,
        "duration": 698.6399999999996
    },
    {
        "text": "big deal for companies that run tasks for hours at a time. And when anthropic combined all of this, the context compaction, the way sub aents coordinate, and the new effort controls, opus 4.5 jumped from 70.48% to 85.3% on their deep research test. That's a massive improvement that comes bigger model. And on the enterprise side, the model can now literally use computer and browsers, clicking, typing, and jumping between tabs to handle repetitive tasks. The Excel update adds sidebar chat, pivot tables, charts, and file uploads, while the Chrome tools let it move across tabs without losing track. It's built for the everyday office workflows people deal with non-stop. You see, Rocketin's business AI team said their agents hit peak capability within four iterations using Opus 4.5. Other models needed more than 10 and still couldn't reach the same level. That kind of speed is a big deal when you're tuning agents that maintain S sur operations or internal infrastructure and the agents didn't forget their earlier work. They kept insights from previous runs and reused them on new tasks, which is exactly how you turn AI from a static assistant into something that grows more useful the longer you use it. Claude Code also received upgrades. Plan mode is sharper now and more methodical. Instead of jumping into code immediately, the system asks clarifying questions, builds a plan MD file with a step-by-step outline, and then executes. The desktop app now lets users run multiple cloud code sessions simultaneously. So, one agent might debug a bug, another research documentation, and a third refactor a codebase. This kind of multitasking uses the strengths of the model and reduces the amount of micromanagement developers usually need to do. All right, on top of all of this, Anthropic reduced pricing for Opus level capabilities to $5 for input tokens and $25 for output tokens per million. That price cut opens the door for more startups and teams to use high-end models without burning their budget. Anthropic also removed Opus specific usage caps for users with access to the new model and increased overall usage limits for Macs and team premium users. The company also expanded integration support including new developer platform capabilities, a desktop version of cloud code in research preview, and deeper Chrome and Excel support. Now, behind the scenes, Anthropic also committed to purchasing $30 billion worth of compute from Microsoft Azure. That's a staggering number and signals that the company is planning far beyond current demand. Microsoft and Nvidia are both strategic partners now, and the compute deal makes it possible for Anthropic to train larger models, expand the developer platform, and scale agent workloads without running into capacity issues. But real quick, if you've been following all this AI news and thinking, \"Okay, this is cool, but what can I actually do with it?\" You're definitely not alone. That's why we created the AI income blueprint. It shows you seven",
        "start": 354.4,
        "duration": 1028.3999999999999
    },
    {
        "text": "and scale agent workloads without running into capacity issues. But real quick, if you've been following all this AI news and thinking, \"Okay, this is cool, but what can I actually do with it?\" You're definitely not alone. That's why we created the AI income blueprint. It shows you seven build extra income streams on the side. No tech skills needed and you can automate everything pretty easily. The guide contains simple proven methods using tools I often talk about on this channel. Download it free by clicking the link in the description. Okay. Now, while Anthropic was rolling out all of this, OpenAI came in with a pretty interesting update of its own. A full shopping research mode inside Chat GPT. It's built to handle the messy realworld process of trying to figure out what to buy, especially during holiday season when people are comparing a dozen things at once. So, the way it works is pretty straightforward. You tell ChatGpt what you need, who it's for, what your budget looks like, and what the main priorities are. And then instead of giving you a giant text dump like a traditional search engine, it pulls real-time prices, specs, and reviews from trusted retail sites and turns that into a clean buyer guide. You get product cards that actually look organized, and you can mark each one as more like this or not interested, which helps chat GPT refine the results over a few rounds. The whole thing feels natural, and I personally like this new feature. Behind the scenes, this mode runs on a GPT 5 mini variant that OpenAI trained specifically for shopping accuracy. It doesn't hallucinate prices because it reads the data live. It also uses memory to keep track of your style, your past picks, and any details you mentioned earlier, so the suggestions feel consistent instead of random. And this whole system plugs into the commerce tools they've been rolling out over the past few months. Instant Checkout, for example, already lets US users on free plus and pro accounts buy items from Etsy and certain Shopify merchants right inside the chat. No redirects, no complicated forms. It's all powered by the Agentic Commerce Protocol, which basically standardizes how products, prices, and checkout flows move into ChatGpt. Merchants just feed their inventory through ACP, and ChatGpt handles the rest. So now you've got product discovery, product comparison, and actual checkout living in one place. And then there's chat GPT Pulse, which quietly ties it all together. Pulse can surface shopping research cards based on older conversations you had, things like previous browsing interests, gear you talked about, or items you asked about weeks ago. So the system doesn't only respond to what you say in the moment. It remembers patterns, surfaces useful recommendations automatically, and makes the whole thing feel more like an ongoing assistant experience instead of a one-off search window. So, yeah, Open AI basically turned Chat GPT into a full",
        "start": 520.8,
        "duration": 1344.1609999999998
    },
    {
        "text": "ago. So the system doesn't only respond to what you say in the moment. It remembers patterns, surfaces useful recommendations automatically, and makes the whole thing feel more like an ongoing assistant experience instead of a one-off search window. So, yeah, Open AI basically turned Chat GPT into a full that reads real data in real time. Now, Google pushed its own update around the same time. And even though it didn't make as much noise as the anthropic news, it's still a pretty big step for people who rely on Google's ecosystem for research and long-term projects. They're working on a direct notebook LM import feature for Gemini, which basically means you'll be able to pull an entire notebook straight into Gemini without doing the usual exporting, copying, or dragging files around. It's all happening through a new option inside the attachments menu. Right now, Gemini supports photo uploads, file uploads, and code imports. But once this notebooks option shows up, everything you've built inside Notebook LM, your summaries, references, structured notes, sources, even those long research threads will load directly into Gemini's reasoning engine. And honestly, that's a pretty natural move. Notebook LM has become one of Google's strongest tools for people who need deep context that stays stable for months at a time. Writers, academics, analysts, and students have been using it to build long interconnected documents that the model can navigate with almost no confusion. So, if you think about it, letting Gemini read those notebooks natively opens up a different level of workflow. Instead of juggling between apps, the model can now take the notes you already curated, merge them with its own live reasoning, and help you write, analyze, or expand everything in one place. And since Gemini is getting more heavily tied into Google Docs, Sheets, and Search, the Notebook LM import feature is basically tightening that whole loop where your sources, your notes, and your AI assistant all sit in the same ecosystem. The feature is still in development, and Google hasn't given a timeline, but the code traces show it's actively being integrated. And to be honest, it fits Google's current push perfectly, making Gemini feel like the center of their productivity stack. For people who move between Notebook LM and Gemini constantly, this is going to cut out a ton of friction and make the whole workflow feel more like one continuous workspace instead of two separate tools. So, here's the real question. If models are already outperforming top engineers, where do you think this all goes next? Drop your thoughts in the comments. Make sure to subscribe and leave a like if you enjoyed the video. Thanks for watching and I'll catch you in the next one.",
        "start": 681.76,
        "duration": 1622.4809999999998
    }
]