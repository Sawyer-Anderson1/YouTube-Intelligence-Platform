[
    {
        "text": " All right So today I want to talk about something coming out of Metapare led by Yan Lun and his music team that shows a very different kind of AI taking shape It's not built around generating text or chasing better word prediction at all Instead, it drops the idea that intelligence has to revolve around producing words and focuses on predicting meaning directly And once you really see how this works it honestly feels like what comes after the LLM era not just another upgrade on top of it What they've built is called VLJA, short for vision language joint embedding predictive architecture The name sounds heavy but the idea behind it is surprisingly straightforward And once it clicks a lot of design choices we've all accepted in modern AI suddenly start to feel inefficient music or at least unnecessary To understand why this matters you have to look at how vision language models usually work today Right now most VLMs follow the same basic music pattern You show them an image or a video You give them a prompt or a question and they respond by generating text one token at a time That's how image captioning works That's how visual question answering works That's how most large multivocal models operate under the hood They're trained to predict the next word then the next word then the next word again And again That approach clearly works but it comes with some hidden problems The first issue is that these models are forced to learn a lot of things that don't actually matter for correctness Take a simple example If you ask \"What happens if I flip this light switch down There are many answers that are all perfectly fine The light turns off The room gets darker The lamp goes dark Humans immediately understand that these all describe the same outcome But for a token-based model those answers are totally different They're different sequences of symbols with almost no overlap During training the model has to learn exact phrasing word choice and sentence structure Even though none of that changes the meaning a huge amount of training effort goes into modeling surface level language variation instead of the underlying idea The second issue shows up when you try to use these systems in real time Token by token generation is slow and awkward If you're dealing with live video wearable robotics or anything music that needs to understand what's happening continuously You can't really know what the model means until it finishes generating music text The semantics only appear at the end of the decoding process That adds latency burns computer music and makes it hard to update information selectively This is exactly where VLJA takes a completely different route Instead of predicting words VLJA predicts embeddings These are continuous vectors that represent meaning directly not the surface form of language During training the model never tries to generate text at all It",
        "start": 2.8,
        "duration": 325.11899999999997
    },
    {
        "text": "makes it hard to update information selectively This is exactly where VLJA takes a completely different route Instead of predicting words VLJA predicts embeddings These are continuous vectors that represent meaning directly not the surface form of language During training the model never tries to generate text at all It query straight into a semantic representation of the answer The system is built from four main components and each one has a clear role First, there's the visual encoder This takes an image or a sequence of video frames and compresses it into a set of visual embeddings music You can think of these as visual tokens except they're continuous vectors rather than discrete symbols In this setup they use VJEPA i a self-supervised vision transformer with around 304 million parameters and it stays frozen during training Next comes the predictor which is the core of the whole system This module takes the visual embeddings and the text query music like a question or prompt and predicts what the answer embedding should look like It's built using transformer layers initialized from llama 3.21b but without causal masking That means everything can attend to everything else Vision and text interact freely Then there's the Y encoder This encodes the target text during training the correct music answer into an embedding That embedding becomes the learning target Importantly, this representation is meant to capture the meaning of the answer not the exact wording Finally, there's the Y decoder and this part is barely involved It doesn't participate in training at all At inference time it only gets used when you actually need readable text Most of the time the model stays entirely in embedding space Training works in a simple loop You give the model a visual input a query and a target answer The Yen encoder turns the answer into an embedding The predictor tries to produce music that same embedding from the visual input and query The loss is computed directly in embedding space not in token space What matters here is how the model learns without everything collapsing into noise Vla is trained so that its predicted meaning is pulled toward the correct meaning while different answers are kept clearly separated In practice this forces the system to build a structured semantic space Similar answers cluster together naturally Different answers stay far apart Instead of memorizing phrasing the model organizes meaning itself which keeps the whole representation stable and useful This leads to the key insight behind the whole approach In token space multiple valid answers can be extremely far apart In embedding space those same answers can sit close together That turns a messy multivocal learning problem into a clean single mode one The model no longer has to guess which wording you want It just has to understand what the answer means Because of this VLJA doesn't need a heavy language decoder during training It's not learning how to write sentences It's learning how to predict",
        "start": 165.2,
        "duration": 628.08
    },
    {
        "text": "into a clean single mode one The model no longer has to guess which wording you want It just has to understand what the answer means Because of this VLJA doesn't need a heavy language decoder during training It's not learning how to write sentences It's learning how to predict huge amount of unnecessary work out of the system And you can see it clearly in the results To test whether this idea actually holds up the researchers ran a rare kind of comparison where almost nothing was allowed to change Same vision encoder same resolution same frame rate same data mixture same batch size same number of training steps The only difference was what the models were trained to predict One model followed the standard route predicting tokens with a 1 billion parameter language model The VLJ version predicted embeddings using a roughly 500 million parameter predictor So right away the embedding based system had about half the trainable parameters Early in training the two systems look similar After around 500,000 samples performance is roughly comparable But as training continues a clear pattern emerges VLJA starts improving faster and it keeps improving After 5 million samples it reaches around 14.7 CR on video captioning while the token-based model is still around 7.1. Classification accuracy jumps to about 35% top five for VLJA versus roughly 27% for the baseline And the gap doesn't close later At 15 million samples the difference remains VLJa continues to learn more efficiently even with fewer parameters That's not a tuning trick That's a structural advantage The story doesn't stop at training efficiency either Inference is where this approach really starts to shine especially for video Because VLJA produces a continuous stream of semantic embeddings it supports something called selective decoding Instead of generating text at fixed intervals you monitor how the embeddings change over time If the meaning stays stable you don't decode anything If there's a significant semantic shift then you decode They test this on long procedural videos from Ego XO4D. These videos average about 6 minutes each and contain roughly 143 action annotations per video Decoding text is the expensive part So, the goal is to recover the annotation sequence while minimizing how often decoding happens They compare two strategies Uniform decoding where text is generated at fixed time intervals and embedding guided decoding where the embedding stream is clustered into semantically coherent segments and decoded once per segment The result is clean To match the performance of uniform decoding at one decode per second VLJA only needs to decode about once every 2.85 seconds That's roughly a 2.85 times reduction in decoding operations with similar side ER scores No fancy memory tricks no KV cache gymnastics It's just a consequence of working in semantic space This is especially important for mealtime systems like smart glasses robotics navigation or live planning where latency and compute cost actually matter Another major advantage is versatility VLJa can handle generation classification retrieval and",
        "start": 318.88,
        "duration": 970.0799999999996
    },
    {
        "text": "fancy memory tricks no KV cache gymnastics It's just a consequence of working in semantic space This is especially important for mealtime systems like smart glasses robotics navigation or live planning where latency and compute cost actually matter Another major advantage is versatility VLJa can handle generation classification retrieval and using the same architecture There are no task specific heads and no separate models For open vocabulary classification candidate labels are encoded into embeddings and compared to the predicted embedding The closest match wins For texttovide retrieval the text query is encoded and videos are ranked by similarity For discriminative VQA, all candidate answers are embedded and the nearest one is selected They evaluate this across a wide set of benchmarks on eight video classification data sets and eight texttovideo retrieval data sets The base VLJA model with 1.6 billion parameters and only about 2 billion training samples outperforms clip SIG lip 2 and perception encoder on average Some of those baselines have seen up to 86 billion samples After supervised fine-tuning, the VLJA SFT model improves even further It's no longer strictly zeroth but as a single generalist model it approaches specialist systems that are tuned individually for each data set On visual question answering the results are especially telling They evaluate on GQA for compositional reasoning tally QA for complex counting and POPE and POPE va for hallucination detection Vla SFT with 1.6 6 billion parameters lands in the same range as models like Instruct Blip and QuenVL, many of which rely on much larger backbones and multistage instruction tuning It doesn't dominate every benchmark but the fact that it's competitive at all is important because it's not a classic generative VLM. It answers questions by comparing meaning not by generating free form text Then there's the world modeling experiment Here the model is shown an initial image and a final image and has to choose which action caused the transition from four candidate video clips This is closer to understanding physical causality than language generation Vla SFT reaches 65.7% accuracy setting a new state-of-the-art. It outperforms larger vision language models and even beats frontier language models like GPT40, Claude 3.5, and Gemini i which rely on captioning and textbased reasoning That result matters It suggests that directly predicting latent semantics can be more effective than narrating the world in words and reasoning over those words afterwards They also analyze the quality of the text embeddings themselves using hard negative benchmarks like Sugar Crate++ and Vizla. They test whether the Y encoder can detect subtle semantic changes like swapped attributes or altered relationships The base VLJA Yen encoder outperforms clip SIGLIP 2 and perception encoder indicating a sharper and more structured semantic space Finally, they stress test the system through ablations When the large captionbased restraining stage is removed performance drops sharply especially for classification and retrieval Breezing the Y encoder hurts alignment Overly simple training objectives weaken learning Larger predictors help particularly for VQA.",
        "start": 492.24,
        "duration": 1321.5179999999996
    },
    {
        "text": "indicating a sharper and more structured semantic space Finally, they stress test the system through ablations When the large captionbased restraining stage is removed performance drops sharply especially for classification and retrieval Breezing the Y encoder hurts alignment Overly simple training objectives weaken learning Larger predictors help particularly for VQA. consistently boost retrieval and classification The pattern is consistent When components that support semantic learning are strengthened the model improves When they're removed it degrades That kind of behavior is exactly what you want to see from a system that's meant to scale VLJA isn't trying to replace language models everywhere Tasks like deep reasoning tool used and agent style planning still favor token-based systems But for perceptionheavy problems especially those involving video mealtime input and continuous understanding of the world this approach fits naturally It shifts the center of gravity from language to meaning Words become an output option not the core mechanism of intelligence And that shift is what makes this work feel like more than just another model iteration Thanks for watching and I will catch you in the next one",
        "start": 670.079,
        "duration": 1419.4389999999996
    }
]