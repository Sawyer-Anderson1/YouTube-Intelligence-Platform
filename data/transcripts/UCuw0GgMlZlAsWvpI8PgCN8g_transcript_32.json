[
    {
        "text": " Welcome to Zcast. I'm Zaroval from ZK Research and I'm here at Nvidia's GTC event in DC. We're going to be talking about networking and I'm delighted to be joined by Will Eden, SVP of network engineering for Cisco and SVP of networking for Nvidia. So, this is traditionally been a GPU conference, but we're going to talk about networking. So, that's really exciting, right? So, , before we get going though, , Golad, I think most people know who you are, but just a quick intro on yourself. Yeah. , I, , I joined Melanox around 25 years ago. A long time. As a, yeah, very long time. , as a design engineer and I, I took part of the design of the first Infiniban devices. later on I moved and and managed several project within Merlox and after 20 years more or less we joined Nvidia to build the infrastructure of the AI data center. and and today you know we're proud to have great AI infrastructure is being built to deliver the best efficiency and essentially be able to build a data center as a single unit of computing. Okay. And will how about you? so I actually first joined Cisco about 25 years and at that time we were doing actually multi- chassis routers for high-scale telos. So, interesting enough, you get these racks. You're connecting them with fiber cables, fiber and and and copper and doing ASIC design and have been in and out of Cisco and for the last six years have been very excited both on the hypers scale side as well as all the fun that's happening with AI infrared data. Yeah. Well, there's a lot going on with AI. And I think if you were to ask the maybe tech generalist about AI, they tell you about the importance of the GPU, the computer, and things like that, but the network actually plays a really important role in scaling AI. And so obviously Cisco's got more domain knowledge in networking than anybody. And and talk about the role the network actually plays in in scaling AI. Sure. So obviously the the cost of the infra across the GPUs and the racks has been quite high and the role of the network is really the getting the utilization of those GPUs at at a a maximal optimization and so everything from you know early on in training being able to distribute the workloads but even in inference we see that customers are you know the scaleout network is very key for getting higher efficiency by allowing workload workload orchestration across those GPUs and then you get into things like you know the storage network is more important than ever high performance being able to do loss you know high high 400 moving to 800 gig in that storage network so it's very key for for the the entire sort of representing a supercomput across the data",
        "start": 5.44,
        "duration": 355.3600000000002
    },
    {
        "text": "into things like you know the storage network is more important than ever high performance being able to do loss you know high high 400 moving to 800 gig in that storage network so it's very key for for the the entire sort of representing a supercomput across the data visionary of Nvidia to acquire Melanox when it did obviously it the world's leader in GPUs but then it wanted to bring Melanox in to make networking part of its stack and so talk about how networking fits into that broader AI stack that Nvidia has. I think it's it's basically the development of the computing unit over time, right? Nvidia started and and build GPUs and and back then GPUs were the unit of computing. Now the unit of computing is the data center and building a data center or building a supercomputer as a unit of computing requires to bring an infrastructure that will connect all those computing silicons computing devices and form one large unit of computing. and and definitely that's not an easy task. Yeah. because it's it's not just you know take offtheshelf standard network and just put it together or use that in order to build that data center. Building an AI supercomputer requires to bring multiple computing infrastructures together. For example, we start with scale up and the the scaleup network or the mission of the scaleup network is to take those or connect those GPU A6 together to form one large GPU, a Rex scale GPU. Right? That's kind of that's step number one. Step number two is to scale out the Rex. This is where we bring the scale out infrastructure and this is where Nvidia is working with Cisco and actually we announced and great infrastructure together to to bring u that scale out elements that are needed to actually build that data center. Now it it doesn't stop there. So now we have unit of computing. We also need to add a north a north a north south network, an access network, a storage connectivity so forth. And now you build a supercomputer. Now from that point you actually also want to connect multiple of those together and essentially you're going to see a new new developments around the infrastructure itself. but that was the important of bringing the infrastructure in order to build the unit of computing of today which is the entire data center. Okay. And yeah, it's interesting because Unity can be think of as a server, but it's actually becoming logistically anything you want it to be now, right? you know, AC across distance too. And will the NVIDIA Cisco partnership has been going on now about almost a decade, I think it's it's it started. And so talk about the partnership, how and how important it's been to Cisco customers. Sure. so yeah, [clears throat] we've had a a partnership for a very long time. we've been selling Nvidia",
        "start": 183.36,
        "duration": 704.4000000000001
    },
    {
        "text": "partnership has been going on now about almost a decade, I think it's it's it started. And so talk about the partnership, how and how important it's been to Cisco customers. Sure. so yeah, [clears throat] we've had a a partnership for a very long time. we've been selling Nvidia would say it really turned up in the last 10 months. So Glad and I started working together you know probably a year a little over a year ago from an engineering and a product standpoint and what we announced in February was that we were going to take it much closer and you know I think folks who have known Cisco for a long time it might be a little surprise you know we we know networking we've been working so long why is this so key and it's it's from a customer standpoint they you know folks we've worked with for many years they want both they want the the experience and the operational model that they've had with Cisco, they want Cisco support, , optics, the solution level delivery. , but as they're deploying clusters, you know, we've used the word supercomputer a lot. A lot of my customers don't want to manage a supercomputer. They want to, you know, start 64 120 GPUs potentially scale into hundreds or or thousands eventually , in that high-end enterprise, but they they they don't want that complexity and so they're looking for the best of both worlds. And that's where it really, you know, I I didn't know if it would happen, but as we started working together on how this, you know, the pieces would connect got, you know, all the the CEOs for both companies and we came to that agreement announced in February and since then we've been working on getting the products out. Yeah. And Glad, you know, just your thoughts on the partnership, but I'm you know, it seems from an Nvidia standpoint, this is really just about customer choice. I think I think it's it's it's more than that. Okay. I think it's more than that. An AI data center is a supercomputer. Now there are customers or entities that knows how to manage supercomputers. There are customers that don't really know how to manage a supercomputer. They need to have that supercomputer but they want to manage that as they manage their enterprises data centers before. And this is the strength of the collaboration between Nvidia and Cisco because what we do is that we build the infrastructure for AI. That's our focus and we need to do it very quickly because we are essentially on annual cadence. If you see the growth in AI and you see the the the pace of technology today, every year there is a new generation of systems coming out. So you need to have a fast pace. You need to develop the technology and this is what Nvidia is doing. Now working with Cisco",
        "start": 359.68,
        "duration": 994.6399999999996
    },
    {
        "text": "the growth in AI and you see the the the pace of technology today, every year there is a new generation of systems coming out. So you need to have a fast pace. You need to develop the technology and this is what Nvidia is doing. Now working with Cisco enterprise sector and that enterprise sector will continue to manage that supercomputer but as they manage their enterprises so far right Cisco has built a great management infrastructure great software infrastructure that covers all the aspects of the enterprise data center taking the AI technology of that infrastructure into that Cisco enterprise class capab capabilities, bring the supercomputing, the AI, the most efficient AI into the customers and they will continue to manage the same way that they've managed their previous data centers, the new AI supercomputers that they actually acquire. All right. And what I love about these shows is you always get more product announcements, right? Everybody loves the shiny new products and both of you had announcements at the show. Well, Cisco announced the N9100 today, the the new switch. talk about that switch and how it helps customers scale their AI deployments. Sure. so yeah, one of we announced three new things and the first one was the Nexus 9100 and so the key key components for that is that it first of all it does support the NCP with NVIDIA and so we have customers especially in the Neocloud but potentially broader that really want that that NCP certification aspect. and that switch allows us along with our UCS to to support the the backend the compute and then the rest of the Cisco you know Ethernet solutions for front-end storage you know DCI data center interconnect can surround that and so that's a full solution that we can support and that's the first third party NCP compliant switch I believe too isn't it? Yes. Yeah. Yeah. So it does inside it has a spectrum silicon and then we're running both Nexus OS which is our longtime data center as well as Sonic for customers that are interested in Sonic. We're going to support both of those and then coming to the management layer it's both things like Nexus dashboard which helps customers manage fabric and life cycle as well as new hyper fabric which is a new cluster. So those you know those are key aspects. It's, you know, you know, key Ethernet based switch and we're happy to add it to our lineup alongside our Cisco silicon one switches. Yeah. And Glad I know Nvidia historically has lived on both sides of the networking protocols, Ethernet and Infiniband. do you see where where do you see Ethernet playing a role? Do you see [music] this being in in many ways as it goes enterprise really time for Ethernet to shine in AI? No, I don't think I don't think that you know it's it's for enterprise and infinib is for non-enterprise in that",
        "start": 506.24,
        "duration": 1316.4809999999993
    },
    {
        "text": "do you see Ethernet playing a role? Do you see [music] this being in in many ways as it goes enterprise really time for Ethernet to shine in AI? No, I don't think I don't think that you know it's it's for enterprise and infinib is for non-enterprise in that and the reason that we start with infinib is because infinib was built for distributed computing from the beginning and that's why infinib is being used by the majority of the the HPC systems in the world and it continues to be there. you know in in the recent list I think the pest the pe the highest number of of of infinibate systems was just in in the last list so it's a great solution for that area now as AI extend to more places and it's not just enterprise it's clouds it's it could be large CSPs and so forth we saw the need to actually also bring internet as an option as a choice because there is entities or companies or customers of us or mutual customers that that that wants to use it because they know how to manage Ethernet is because they're using iterant based software and they want to continue and and and keep their investment and so forth. there is a a good need for internet not just enterp enter enter enterprise and that's the reason that we actually created spectrum x internet so when we approached and understood the need to bring ethernet into the AI and again not just enterprise there there was not an internet element or version that was designed for AI meaning there there is no one internet out there there is many internets there is many off-the-shelf internet out there there were some that was developed for heavy virtualize some that was developed for single server workload and so forth. There was nothing really that was developed for distribute computing. So that's why we created Spectrox internet. That's why we build an infrastructure. It's not a device. It's an infrastructure. and that Spectrumx internet infrastructure can be used by enterprises by clouds neocloud CSPs and so forth. And partnering with Cisco gives the flexibility, the options. Customers can use Nexus for example, they can use Sonic, they can use Cisco [clears throat] infrastructure management around that and take it to more places together. So the work within with between us bring AI or enable AI reach to more and more entities more and more customers that wants to go to AI and that's actually reduce the burden because they can continue what they've done so far you so far I mean I think just one one thing on that we're certainly seeing that networking on the east west is different than our traditional data center networking we have large flows the expectations on reliability and lossless is much different and So the key we've seen is benchmarking reference architectures and so we've seen things",
        "start": 670.0,
        "duration": 1646.720999999999
    },
    {
        "text": "one one thing on that we're certainly seeing that networking on the east west is different than our traditional data center networking we have large flows the expectations on reliability and lossless is much different and So the key we've seen is benchmarking reference architectures and so we've seen things a lot of work on intelligent packet flow for things like load balancing you know for storage and being able to get high performance there on the east west having the spectrum x umbrella technology between the nicks connecting with the fabric that's either spectrum siliconbased or cisco siliconwood based means that customers can get that same tuned perform performance for that and that's been key in this you know the whole benchmarking thing and and the reference architectures is key. Yeah. Now glad you talked about the [clears throat] infrastructure that you have for networking part of that is the DPU right you announced at the show a new blue field 4 right and so can you talk about the difference between three and four and the performance benefits you get and why the DPU is such an important part of the overall network infrastructure. Yeah. So, so the DPU is the core device for the north south network for the access network. It does several things, but I I'd like to emphasize two. One is to secure the infrastructure, secure the data center. The DPU actually runs that infrastructure operating system. So instead of managing the data center managing the server or the access in the server itself in the same compute engine that also serve the user you want to separate the infrastructure domain from the application domain to bring security in and this is what we're doing with the DPU. So the DPU run the data center operating system. That's one thing. The second thing is storage. DPU is a great storage processor. It brings or it it it gives or enables a lot of storage offloads. It can run the storage stack on it and it being used not just on the server side which called also the storage client but also on the storage server. So it runs the storage stack on the s on the storage server. It run the storage the storage stack also on the compute server and it's a great storage processor. So it does the two those two main functions. there is some other function that it does but that's the reason or that's the purpose of the DPU security management of the data centers and storage. Now as we move from generation to generation and build a new infrastructure we needed to bring more bandwidth we needed to bring more compute power we needed to bring more memory and those are the key advantages of blue feed 4. So we're doubling the bandwidth. We're increasing compute capacity of that debut by 6x. We're increased memory by by 3x. So it's it's a great addition. The performance is now",
        "start": 838.32,
        "duration": 1962.799999999999
    },
    {
        "text": "bring more compute power we needed to bring more memory and those are the key advantages of blue feed 4. So we're doubling the bandwidth. We're increasing compute capacity of that debut by 6x. We're increased memory by by 3x. So it's it's a great addition. The performance is now rubin infrastructure to support the storage workloads, the access inferencing, training and so forth. And that's a great part of of ver rubin. Yeah. look at the performance stats for Bluefield 4, I think it would have been computer considered a supercomputer just a few years ago, right? I mean, it's off the charts now. Well, when I think of Cisco historically, the what I think Cisco's always done well is architectures, right? It's, you know, all the way back to voice video architectures and and at the show you announced the cloud reference architecture, right? And obviously for a lot of organizations scaling AI there's a lot of challenges and so talk about what the cloud reference architecture addresses and how it's going to help customers scale their so [clears throat and cough] so for us the the cloud reference architecture is a way for us to help our customers build secure AI factories that are full solutions and so it's it's key key things is it's you know fully performance validated and a step by step on how to put it the the cluster together and for us a key component of that underlying is that it's using Cisco silicon one or or spectrum but on the Cisco silicon one we work with using the adaptive routing between the Nvidia and so that allows us to get the performance levels we're looking for and then we are that goes from hundreds to thousands of GPUs and then we're also the overall architecture will continue to extend with security so Today we're doing things with AI defense for LLM. we're adding in aspects of firewall and going back to blue field as an example of a cluster level solution. We're looking to bring further like isovalent services from a K8's level where we accelerate that using blue field and that's something we'll be extending over time in our cluster. Yeah. The thing I really like about the reference architectures and validated designs is it takes a lot of the tweaking and tuning time out of the deployments and I've talked to customers where it takes something that maybe took a a year to optimize down to just a couple of months. So there's a lot of value in customers for that. and the secure AI factory is obviously a big part of the go to market for Nvidia. just a little more double click on on how those work and and when you think about the secure AI factories why the reference architecture is so important to it. I think it's not just for the security or secure that it's I think it's important for every anyone or everyone",
        "start": 998.16,
        "duration": 2273.2799999999975
    },
    {
        "text": "a little more double click on on how those work and and when you think about the secure AI factories why the reference architecture is so important to it. I think it's not just for the security or secure that it's I think it's important for every anyone or everyone So that's why we have those reference architecture the the NCPs and and and and those covers everything from the building blocks the the full software frameworks that runs on top of that and all the way to running the full application on top that of that system. the great benefit is that to our customers is that we build that data center inside before we actually release the technology out. We make sure that everything runs on top of that. Making sure that all the services are there. The compute services, all the frameworks, the security services, the storage services, we performance tune it. And by doing that, we guarantee that's the best performance that we're seeing internally. It's the same thing that our customers going to see, our partners going to see when they're following that reference architecture. That's why reference architecture is is key. You know in the past people would buy components and try to put them together and that's why I say you could take long time months and even years. Yeah. If if you are if you're in an annual cadence of technology you don't have the luxury of that's true and actually want [clears throat] to take it down to weeks because every day is expensive. So by building that reference architectures you can enable we enable our customers to actually go and copy that one to one and when they copy that they can actually bring it very quickly. We can help them and and obviously they can go and and start running very quickly. Now customers can al also take pieces of what we build and take elements of what we build and and that's the full flexibility of that reference architecture. they can choose a GPU and they can choose someone else's connectivity. They can use our connectivity and use their own accelerator. That's a strength of an Fusion for example where people can take the infrastructure that we built but they can connect to their own CPU or their own accelerator. So we build a reference it's fully optimized fully set you can actually take that reference or you can take pieces of that and you can enjoy those pieces with another components of your own. All right. Now, so that's what I thought were the highlights from the networking payload. is there anything else you either you want to add? And then so we announced one other thing which is hyper fabric AI is is orderable and that you know going back to this topic of solutions. This is an example of a solution that pulls all the components together from the the compute",
        "start": 1155.44,
        "duration": 2584.639999999998
    },
    {
        "text": "you either you want to add? And then so we announced one other thing which is hyper fabric AI is is orderable and that you know going back to this topic of solutions. This is an example of a solution that pulls all the components together from the the compute from the starting of planning. You know how many GPUs, what storage do you want? our initial partner is vast and we'll expand on that and then being able to have that equipment come everything from helping wire that up and then be able to get the the visibility and performance at a cluster level. So we see that you you know again we're early on there's a lot of folks following these reference architectures and maybe there some things are fragile the ability to manage these and and to scale them in place is a key point. So that was a third third thing. All right. And certainly the technolog is all here, right? It comes down to your point, management and scale, right? And being able to do that in a way that's consumable for customers, right? Because, , I don't think there's any question that the technology is not ready to make AI go. Yeah. So, , last word for you. So, first they're going to be great announcements regardless, you know, not just on the infrastructure itself in GTC. So, you know, watching the keynote is going to be , you know, amazing. We'll all be there. Yeah. But it's always fun to talk networking at a at an AI conference, right? So now maybe two more things. So one is the collaboration between Nvidia and Cisco. It's not just for a single device or one box. You know, we're talking about long-term collaboration here and we're already working on the next generation and generation after that. So you know, it's a great collaboration between the two companies. All right. So more to come. More to come. Yeah. and then one last thing is that we we we call it networking. Networking. Networking. I think it's it's you know networking is the world of the past in a sense we're not building networking we're building an infrastructure you know scale out it's actually computing infrastructure and you have a storage infrastructure and so forth I think the networking is deserve a little bit you know more than just calling that one it is a good point right I think historically networks have been thought of as plumbing and things that connect a couple things together but it's really playing a much more important role you have comput of the network today you know it's not just moving data it's actually do some interesting things on top of that all right Well, it's great to spend some time with both of you talk about the partnership and new innovations. So, on behalf of Glad China from Nvidia, well Etherton from Cisco, I'm Zas Caraval",
        "start": 1312.799,
        "duration": 2837.6
    },
    {
        "text": "it's not just moving data it's actually do some interesting things on top of that all right Well, it's great to spend some time with both of you talk about the partnership and new innovations. So, on behalf of Glad China from Nvidia, well Etherton from Cisco, I'm Zas Caraval watching. why don't you give us a like and also hit that subscribe button and I'll see you next time on my next episode of ZCast. [music]",
        "start": 1442.72,
        "duration": 2853.5399999999995
    }
]