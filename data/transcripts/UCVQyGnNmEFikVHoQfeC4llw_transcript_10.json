[
    {
        "text": " If you've ever tried to use Siri or Alexa in a crowded Indian market, you've seen it happen, right? They have a total nervous breakdown. Global AI is built for a very quiet living room in California. It is historically deaf to India's everyday audio chaos because we don't speak in textbooks. We speak in code mixed, noisy, multiacented reality. But today, the silence from big tech gets a very serious homegrown answer. Servi just launched Servam audio and it's a 3 billion parameter reality check for the rest of the world. Ladies and gentlemen, welcome to front page. We all know that in India speaking is easier than typing. But for AI, listening to us is a nightmare. Most speech recognition systems are trained on clean data. One person, one language, zero noise. They fail the second they hit a crowded call center, a government office or a rural health clinic. Server Audio is an audio first model built on their SERM serv 3B architecture trained on a massive 4 trillion tokens. It's designed specifically for the strangely complicated middle code makes speech where you switch from Hindi to English mid-sentence regional accents that global benchmarks ignore and those very lowquality 8 kilohertz telephony recordings that make up most of India's customer service. So what Servang does is apart from hearing the words very clearly, it also understands the environment. Serv claims that on the Indic voices benchmark which of course is the gold standard for Indian language AI, Servam audio delivers lower word error rates than GPD 4.0 transcribe and Gemini 3 flash. Accuracy is a big win, but the true victory lies in the fact that it's built locally, which brings us to Servam's tokenizer, which according to me is a masterpiece of efficiency. It uses up to 75% fewer tokens per word for Indian languages compared to global models. For developers, that means it's accurate. It's significantly faster and very cheap to run at a price point of about just 30 rupees per hour for transcription. It's the difference between a cool demonstration and being infrastructure one can actually afford to scale to millions of users. I guess we have all felt it that one of the biggest headaches in AI is diorization. Knowing who said what in a room full of people. Server Audio solves this with a specialized mode that handles up to eight distinct speakers at once, even when they're talking over each other. It can process recordings up to an hour long with high precision speaker labeling, which is marvelous as potential for a doctor's clinic, a legal deposition, or for that matter, a chaotic Monday morning startup standup, sorry, that you have faced in your offices. It turns a chaotic cocktail of noise into a structured searchable database of human intent and that too for just an extra 15 rupees per hour. With it, one gets diarized speech recognition that finally knows who is actually in charge of the conversation.",
        "start": 0.08,
        "duration": 443.59999999999997
    },
    {
        "text": "you have faced in your offices. It turns a chaotic cocktail of noise into a structured searchable database of human intent and that too for just an extra 15 rupees per hour. With it, one gets diarized speech recognition that finally knows who is actually in charge of the conversation. Voice to text, then text to a model, then finally an action. Serv is eliminating the middleman by processing the audio modality directly. They have actually enabled speech to action. So if a user says for example pay my electricity bill, the system doesn't just write it down. It identifies the biller, extracts the account number and triggers the payment instantly. No typing, no reading, just voice first automation. Now of course can you see the magnitude of this? because it can be the bridge to the next billion users who might never touch a keyboard in their lives. And of course, here it is the front page take server audio is proof that language is infrastructure and voice is the new operating layer. This launch follows serv dub which by the way was recently used to live dub the 2026 union budget. So as they prepare to unveil their massive foundational model under the India AI mission later this month, the message to Silicon Valley is if you want to talk to India, you have to learn to listen like an Indian. This lad and gentlemen is front page by the Aim network. So of course if you're tired of your phone not understanding your accent or if you think voice first is the future, let's talk in the comments. Till then of course thinky I think he I am. [music]",
        "start": 221.84,
        "duration": 631.3969999999999
    }
]