[
    {
        "text": " So, Google's Nano Banana Pro is here and here are eight things you missed. Number one is native grounding via search. One of the things that I really like about Nano Banana Pro is the fact that you have the ability to ground your images via search. This means that the image is based on reasoning, not just the training data. All this means is that the grounded models check reality using search before generating. So right here you can see I asked it to generate me an image of the price of Bitcoin right now. And what you can do is you can then use Nano Banana to basically give you that information. You can see that the price of Bitcoin and it literally shows you the last update of $84,000 326. The reason I actually chose the price of Bitcoin is because it's a numerical value. So I could ideally see how recent that information is versus a piece of just random news on the internet. This means that your images are based in reality and there is a lot less hallucination which is going to lead to a lot more accurate searches and also just saving a lot of time. Often times we have to search this and search that before finding out exactly what we're going to do. But now we can literally just ask the model to get something recently updated in order for us to create them images. Now what you have to understand is that this native grounding via search does have some caveats. It does work only with text. For example, there was of course the recently released new Porsche. So on the right hand side you can see the real image of the Porsche that was released. This was the new electric Porsche Cayenne for 2026. And I asked Nano Banana to be able to create a new image featuring this 2026 Cayenne. And it actually couldn't go ahead in search and get me the image. So, you have to understand that if you're trying to create an image of something that has recently been made, it's best in that example just to use reference images. Native grounding is good for things like text, weather updates, current factual information like who's the president of the United States. But for visual reasoning, it doesn't work. Now, another thing that you can also do with Nanto Banana Pro is you can also view the image reasoning. So, one of the things I really do love about this is that when it was reasoning for the price of Bitcoin, I was able to look at the visual reasoning chain just by hitting the drop-own menu and I was able to see why the model got to the conclusion that it got to. So, this is super interesting because what we have is a situation where we can see how the image was created. You can see that it was able to",
        "start": 0.08,
        "duration": 302.08100000000013
    },
    {
        "text": "hitting the drop-own menu and I was able to see why the model got to the conclusion that it got to. So, this is super interesting because what we have is a situation where we can see how the image was created. You can see that it was able to clear price is $84,000 326, which was actually based on yesterday's update. So if you wanted to know why it chose that price, you could always see why it actually chose that price. You could also see how it defined the image parameters. You can see that it says, okay, I'll showcase this Bitcoin price here. Then I'm going to define the presentation elements. And the reason that this is actually useful is because often times when we create an image, we have no idea how the model came to the conclusion or how it interpreted our prompt, leading us to sometimes struggle with prompt engineering. So, what I would do is if I was trying to be more accurate in my search in terms of an actual good AI image, I would actually quickly go ahead hit the drop-own menu and view the image reasoning. That way, I can see how it generated the image. And no other image actually offers you this capability, which leads Nano Banana to have a lot more transparency. Now, Nano Banana, another thing that's really cool is that it also has multi-image composition. So you can add up to 14 characters for the same image and it will maintain character consistency which is pretty wild. That is incredible. You can have all 14 characters. Multi-reference control is effectively just built in. So you could literally have an entire mood board and have all characters there. Now I'm not sure why it's so many characters. Maybe they just wanted to flex because 14 is pretty crazy. Like genuinely a pretty absurd amount. And I genuinely did want to test this because this was the image that was floating around on the internet and honestly it it works remarkably well. Like I used eight random characters from our AI generated image and I said make all of these characters have Christmas dinner together and it just done it completely well. So this one was super super surprising honestly because having used Nano Banana before where it struggled with more than three characters, eight characters all in one scene is genuinely incredible. That means you can create scenes with multiple characters, multiple angles, and it just allows you to do a lot more. Now, this video is sponsored by NeoAI, the team behind something that I personally believe is absolutely game-changing. The world's first autonomous machine learning engineer. Now, here's why this is different. Today, building machine learning models is painfully manual. You need teams of engineers and months of work just to get something into production. And Neo changes all of that. Neo is the world's first fully autonomous machine learning",
        "start": 153.76,
        "duration": 591.842
    },
    {
        "text": "world's first autonomous machine learning engineer. Now, here's why this is different. Today, building machine learning models is painfully manual. You need teams of engineers and months of work just to get something into production. And Neo changes all of that. Neo is the world's first fully autonomous machine learning assistant. It's a multi- aent system that can do everything a real engineer would. It cleans and preps data, engineers features, trains and tests deep learning models, compares results, and even deploys them into production all by itself. And think about what that means. Entire machine learning pipelines designed and shipped in hours instead of months. Neo doesn't just throw code at you. It actually reasons, orchestrates multiple agents and delivers productionready results with minimal hallucination and it learns from you. You can drop in your own knowledge base, your own workflow, your guidelines, and Neo instantly adapts. That makes it not just powerful, but personalized. It becomes your AI teammate, one that scales endlessly, never gets tired, and works alongside humans in the loop when needed. So, from startups to enterprises, Neo is designed to make machine learning engineers superhuman. And if you want to see the future of software engineering, check out NeoAI by clicking the first link in the description. Now, another thing that most people don't realize as well, this is another thing that I was tinkering with and I was wondering how do you actually do this is 4K images. Now 4K images, I realized that you can't generate them natively in Nano Banana Pro. But what you can do is you can actually generate those in Google Studio. So, I tried to generate an image of an SR71 in 4K. And you know, at first glance, if you're not the wiser, you might actually think it's 4K. But what you actually have to do is you have to go over to Google AI Studio. And then when you're there, you'll actually see this resolution tab that you can hit the drop-own menu. And then if you put that tab to 4K, only then can you get a 4K resolution image. And then that's how you get super super high quality images with Nano Banana. And I think 4K resolution is incredible when you [music] have an AI image tool that can do so many different things. Like 4K images is just completely changing the game. Most image generation tools do not offer 4K image natively. And most people don't know that this exists, [music] you know, natively for Nano Banana. So if you're using it on the Google Gemini app, just head on over to AI Google Studio. And yeah, the 4K images, you can zoom in a ton. It's pretty crazy the quality of these images with all of the reasoning that they've done. And the images they do take a little bit longer to create, maybe around I think 20 seconds or so because the image is much",
        "start": 301.04,
        "duration": 859.6009999999999
    },
    {
        "text": "the 4K images, you can zoom in a ton. It's pretty crazy the quality of these images with all of the reasoning that they've done. And the images they do take a little bit longer to create, maybe around I think 20 seconds or so because the image is much use case where you need to zoom in, that would be the case where I would say try and do it in a 4K image. Now, there's also multilingual text reasoning. And I like this because this kind of text rendering is a big deal because one thing that most people don't realize is that generating text is incredibly hard and the model needs to understand the language, spell it correctly, format it visually, maintain a perspective, lighting style, all those crazy crazy things. And then the craziest thing about all of this is that it can now use Chinese characters, Arabic characters, Greek symbols. It's incredible because when the model truly understands writing systems and how to, you know, do everything, this unlocks global creativity. So most image models historically were English only. But with multilingual capabilities, someone in Japan, someone in Brazil, Saudi Arabia, France, Turkey can generate culturally accurate posters. They can do ads, they can do memes, comics. I mean, it turns it from just US to English centric into a global creative engine. So this means that AI once again those use cases just expanded across the globe. And think about it you know designers, marketers and companies care about one thing which is can it produce production ready images and that means you can generate packaging for so many different countries localized ads. Honestly there is a ton of implications for this that I think most people aren't you know thinking about. This is another one that I found that was really cool and we do know that Nano Banana is incredible. Of course, you can create pretty much anything, but the level of precision to which you can edit images is something I've never seen before. I don't know how it gets better than this. Genuinely, like genuinely, I I think we've hit like the wall in terms of just how good these models have got. And this is impressive. So precision level editing here is cinematographer level editing because in order to change the pose from the original to 5600K to 2200K without destroying the skin tones, the shadows or object edges is very hard. This means that the model has a really deep understanding of color temperature, how light bounces in a room, how skin absorbs light, and how shadows shift based on warmth and coolness. So it means it's not just applying a filter, it's reasoning about the light like an actual cinematographer. So this is pretty incredible that it's actually able to do such a precise level of editing because most people wouldn't be able to tell the subtle differences in the white balance between the left and",
        "start": 436.639,
        "duration": 1159.0399999999993
    },
    {
        "text": "not just applying a filter, it's reasoning about the light like an actual cinematographer. So this is pretty incredible that it's actually able to do such a precise level of editing because most people wouldn't be able to tell the subtle differences in the white balance between the left and going to be able to tell you. Even sometimes I would struggle to say what's the difference between the left one and the right. But these subtle changes are things that the model can say okay I know that this is the original. this is 5,600K and then the bottom one is 2,200K. And I literally asked the model just to change it to this, you know, photographer preset. So, it's pretty pretty interesting that the model can get down to that granular level and edit things, which is truly surprising, at least in my examples, because usually you see the model make a bunch of mistakes. It's just really off. But this is precise, very, very precise level editing here. Now, another thing is that the images no longer look AI. And I found a super prompt that I put into the model and it just it just really changed my perception. Like I was staring at a bunch of images and this isn't the only image. Like this image genuinely looks like a real person but it's not. It's actually AI generated. I know I shouldn't have to say that considering the video is a nano banana video. But when images look this real, I don't know how someone who's not in the space is going to, you know, even realize this. But we'll get into that in a moment. So take a look at this. So this image is AI generated. I found a crazy prompt which I'll leave in the description. But just look at it for comparison with other image generation models. You can see here that you know there there are clearly clearly differences in terms of how realistic the model [music] looks. I mean GPT1 image we can see is looking very AI generated. We can see the Cream once again looks kind of realistic but maybe there's a a hint of AI generatedness. Banano Banana Pro. There isn't a single thing that would let me know and that's what's concerning about this kind of image creation, which is a little bit weird, honestly. But at the same time, I do know that this has already been an issue. But now with Nano Banana Pro, I think it's just crossed that line a little bit more because not only can you generate images with such high quality, you could put yourself into those images, you can blend them with reality. So this means that once again it's going to be really harder to distinguish what's real from what's fake. The previous images Cream and GBT image 1, they really do struggle with multi-image composition. But Nano Banana, I can literally, you know, get",
        "start": 588.959,
        "duration": 1422.1589999999992
    },
    {
        "text": "images, you can blend them with reality. So this means that once again it's going to be really harder to distinguish what's real from what's fake. The previous images Cream and GBT image 1, they really do struggle with multi-image composition. But Nano Banana, I can literally, you know, get and I could literally put it in, you know, the background of her picture and it's going to be more believable that it's real. Now, of course, in order to combat this, one thing that I'm glad Google have actually done is they've done synth ID embedded into the model. So, if you don't know what this is, this is the technology developed by Google DeepMind, which basically watermarks every image. Basically, think of this like a digital fingerprint or a super invisible ink that is baked directly into every image. So, this is just an invisible digital watermark. And this is I wouldn't say hardcoded in, but it's very very hard to remove. And all you need to do is just literally put the image into Google Gemini and it will tell you if those images are AI generated. So you can see here in this example it says a technical analysis of the image detected the synth ID watermark which indicates that all part of the content was created using Google's AI tools. So every time you create an image with Nano Banana, Google Gemini, all of those image models, you have to understand that this is being baked in. So, if you don't trust anything online, this is what you need to be doing. You need to be double-checking. And this is important that you know about this because this helps slow the spread of misinformation. And if social media platforms can automatically detect AI generated fake news using these watermarks, they can, you know, quickly limit it or take it down before it causes further harm. So I would say if you're, you know, a little bit unsure online, Nano Banana, now that it's out there, now that all of this stuff is going crazy, a quick screenshot and send it into Google Gemini. Nano Banana will quickly analyze it and it will give you analysis on whether or not this is AI generated or not. And the last one is of course infographics. Infographics are now pretty much solved and usable for a variety of different use cases. So this is another thing that I think I've started to see people use quite a lot. And these infographics are really decent. They allow you to understand ideas pretty quickly. I think this one infographics is really, really cool. So, let me know if these features you missed or you knew about them. And if not, I'll see you guys in the next",
        "start": 721.519,
        "duration": 1671.3599999999985
    }
]