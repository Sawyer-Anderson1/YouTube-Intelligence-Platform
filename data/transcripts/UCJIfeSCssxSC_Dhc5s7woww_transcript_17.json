[
    {
        "text": " All right So, we just talked about a bunch of the closed weight models Let's talk about the open ones Uh, so tell me about the landscape of open LM models Which are interesting ones which stand out to you and why We already mentioned Deep Seek. i Do you want to see how many we can name off the top of our head i Yeah. Yeah. Without looking at notes i Deepseek, Kimmy, Miniax, Z.AI, Ant Lang. We're just going Chinese. Um, let's throw in Mistral AI Gemma. Um, i yeah GPT OSS, the open source model by Chet GPT. Actually, Nvidia Neimotron had a or Nvidia had a really cool one a Neotron 3. Um, there's a lot of stuff especially at the end of the Quinn one maybe the one i Oh, yeah Quinn was the obvious name that was I was trying to get through the You can get at least 10 Chinese and at least 10 Western. I think that i I mean OpenAI released their first open model i since GPT2. That was when I when I meant talked when I was writing about opening eyes open model release They were all like don't forget about GPT2 which I thought was really funny because it's just such a different time But GPTOSS is actually a very strong model and does some things that the other models don't do very well And I think that selfishly I'll promote a bunch of like western companies So both in the US and Europe have these like fully open models So I work at Allen Institute for AI where we've been building which releases data and code and all of this And now we have actual competition for people that are trying to release everything so that other people can train these models So there's the institute for foundation models or LLM 360 which is like had their K2 models of various types Apparis is a Swiss research consortium Hugging face um has small LM which is very popular and Nvidia's neutron has started releasing data as well And then Stanford's Marin community project which is kind of making it so there's a pipeline for people to open a GitHub issue and implement a new idea and then have it run in a stable language modeling stack So this space that list was way smaller in 2024. So I think it was like just AI2. So that's a great thing for more people to get involved and to understand language models which doesn't really have a like a Chinese company that is has an analogy While I'm talking I'll say that the Chinese open language models tend to be much bigger and that gives some of this higher peak performance as where a lot of these things that we like a lot whether it was Gemma um and Nematron have tended to be smaller models from the US which is which is starting to",
        "start": 3.04,
        "duration": 298.71999999999997
    },
    {
        "text": "language models tend to be much bigger and that gives some of this higher peak performance as where a lot of these things that we like a lot whether it was Gemma um and Nematron have tended to be smaller models from the US which is which is starting to three came out which was a giant model very similar to deepseek architecture in December and then a startup RCAI and both Neatron have Neatron as Nvidia have teased models of this way bigger than 100 billion parameters like this 400 billion parameter range coming in this like Q1 2026 timeline So, I think this kind of balance is set to change this year in terms of what people are using the Chinese versus US open models for which will be which I'm personally gonna be very excited to watch i First of all huge props for being able to name so many of these Did you actually name Llama? i Um, not i I feel like laughter this was not on purpose i RIP Llama. i Mhm. i All right Can you mention what are some interesting models that stand out So you mentioned Quen 3 is is is obviously a standout i So I would say the year is almost bookend by both DeepSeek version 3 and R1 and then on the other hand in December Deepseek version 3.2 because what I like about those is they always have an interesting architecture tweak that others don't have But otherwise if you want to go with um you know like the familiar but really good performance queen 3 and like um Nathan said also GPD OSS and I think GPD OSS what's interesting about it is kind of like the first public or like open weight model that was really trained with tool use in mind which I do think is kind of a little bit of a paradigm shift where the ecosystem was not quite ready for it So with tool use I mean that the LLM is able to do a web search to call a Python interpreter and I do think this it's a standout because I think it's a huge unlock because um one of the most u common complaints about LMS are for example hallucinations right and so in my opinion one of the best ways to solve hallucinations is to not try to always remember information or make things up for math why not use a calculator ape or Python. Mhm. i If I asked the LM who won the I don't know soccer world cup in 1998 instead of just trying to memorize it could go do a search I think mostly it's usually still a Google search So JPD GPOSS they would do a tool call to Google maybe find the FIFA website find okay it was France it would get you that information reliably instead of just trying to memorize it I think it's a huge unlock which I think",
        "start": 152.0,
        "duration": 576.8790000000002
    },
    {
        "text": "it's usually still a Google search So JPD GPOSS they would do a tool call to Google maybe find the FIFA website find okay it was France it would get you that information reliably instead of just trying to memorize it I think it's a huge unlock which I think the open-source overweight ecosystem A lot of people don't use tool call modes because I think it's first is a trust thing You don't want to run this on your computer where it has access to tools could wipe your hard drive or whatever So you want to maybe containerize that Um but I do think you know that that is like a really important step um for the upcoming years to have this uh ability Yeah. i So uh a few quick things First of all thank you for defining what you mean by tool used I think that's a great thing to do in general for the concepts we're talking about Even things as sort of wellestablished as i uh you have to say that means mixture of experts and you kind of have to build up an intuition for people what that means how it's actually utilized what are the different flavors So what does it mean that there's just such explosion of open models What's your intuition If you're releasing an open model you want people to use it as the first and foremost thing And then after that comes things like transparency and trust I think when you look at China, the biggest reason is that they want people around the world to use these models And I think a lot of people will not if you look outside of the US, a lot of people will not pay for software but they might have computing resources where you can put a model on it and run it I think there can also be data that you don't want to send to the cloud So this the the number one thing is getting people to use models use AI or use your AI that might not be able to do it without having access to the model i I guess we should state explicitly So we've been talking about these Chinese models and open weight models often times the way they're run is locally So it's not like you're sending your data to China or to whoever developed uh to Silicon Valley whoever developed the model A lot of American startups make money by hosting these models from China and selling them selling token It's called like selling tokens which it means somebody will call the model to do some some piece of work I think the other reason is for US companies like Chad OpenAI is so GPU deprived like they're so they're at the limits of the GPUs whenever they make a release they're always talking about like our GPUs are hurting and I think there's",
        "start": 293.759,
        "duration": 827.4400000000004
    },
    {
        "text": "do some some piece of work I think the other reason is for US companies like Chad OpenAI is so GPU deprived like they're so they're at the limits of the GPUs whenever they make a release they're always talking about like our GPUs are hurting and I think there's release sessions Sam Alman said like oh we're releasing this because we can use your GPUs we don't have to use we don't have to use our GPUs and OpenAI can still get distribution out of this which is another very real thing doesn't cost them anything and for the user I think also I mean there are users who just use the model locally how they would use uh CHPD but also for companies I think it's a huge unlock to have these models because you can customize them you can train them you can uh add post training add more data like specialize them into let's say law medical models whatever you have and the appeal you mentioned lama the appeal of the open weight models from China is that the open weight models are also the licenses are even friendlier I think they are just unrestricted open source licenses where if you use something like Llama or Gemma, there are some strings attached I think it's like an upper limit in terms of how many users you have and then if you exceed I don't know so so many million users you have to report your finance situation to let's say meta or something like that and I think well it is a free model but there are strings attached and people do like things where strings are not attached so I think that's also one of the reasons besides performance why the open weight models from China are so popular because you you can just use them there's no there's no catch in that sense yeah i the ecosystem has gotten on that front but mostly downstream of these new providers providing such open licenses It was funny when you pulled up perplexity It said Kimmy K2 thinking hosted in the US, which is just like an exact I've never seen this but it's an exact example of what we're talking about where people are sensitive to this like Kimmy K2 thinking and Kimmy K2 is a model that is very popular People say that has very good like creative writing and also in doing some software things There's just these little quirks that people pick up on with different models that they like uh what are some interesting ideas that some of these models have explored that you can speak to like that particular interesting to you i Maybe we can go chronologically I mean there was of course Deepseek um Deepseek R1 that came out in January if we just focus on 2025. However, this was based on Deepseek version 3 which came out the year um before in December 2024. There",
        "start": 421.44,
        "duration": 1078.6390000000001
    },
    {
        "text": "particular interesting to you i Maybe we can go chronologically I mean there was of course Deepseek um Deepseek R1 that came out in January if we just focus on 2025. However, this was based on Deepseek version 3 which came out the year um before in December 2024. There side What is fascinating is you can still I mean that's what I do in my from scratch coding projects you can still start with GPD2 and you get can add things to that model to make it into this other model So it's all still kind of like the same lineage the same it is a very close relationship between those but top of my head deepseeek what was uh unique there is the mixture of ex not I mean they were not inventing mixture of experts we can maybe talk a bit more what mixture of experts means um but just to list these things first before we dive into detail mixture of experts but then they also had a mufti head latent attention which is a tweak to the attention mechanism where this was I would say 2025 Five, the main distinguishing factor between these open weight models different tweaks to make inference or KV cache size We can also define KV cache in a few moments But to kind of make it more economical to have long context to shrink the KV cache size So what are tweaks um that we can do and most of them focused on the attention mechanism There is multiped latent attention in in deepseek. There is group query attention which is still very popular It's not invented by any of those models It goes back a few years but that that would be the other option Sliding window attention I think almost reuses it um if I remember correctly So there these different tweaks that make the models different Otherwise um I put them all together in an article once where um I just compared them They are very surprisingly similar It's just different numbers in terms of how many repetitions of the transformer block you have in the center and like just little little knobs that people tune But but what's so nice about it is it's it it works no matter what You can tweak things You can move the normalization layers around You get some performance gains and is always very good in ablation studies showing what actually what it does to the model If you move something around ablation studies does it make it better or worse But there are so many let's say ways you can implement a transformer and make it still work Big ideas um that are still prevalent is mixture of experts mufti latent attention um sliding window attention group query attention and then at the end of the year we saw a focus on making the attention mechanism scale linearly with inference token prediction So there",
        "start": 549.76,
        "duration": 1351.039
    },
    {
        "text": "make it still work Big ideas um that are still prevalent is mixture of experts mufti latent attention um sliding window attention group query attention and then at the end of the year we saw a focus on making the attention mechanism scale linearly with inference token prediction So there a gated delta It's it's like um kind of like inspired by um state space models where you have a fixed state that you keep updating but it makes essentially this attention cheaper or it replaces attention with a cheaper operation i and it maybe is it useful to step back and talk about transform architecture in general i Yeah. So maybe we should start with the GPT2 architecture The transformer that was derived from the attention is all you need paper Mhm. i So the attention uh is all you need paper had a transformer architecture that had two parts an encoder and a decoder and GPT went just focusing in on the decoder part It is essentially still a neurons network um and it has this attention mechanism inside and you predict one token at a time You pass it through an embedding layer There's the transformer block The transformer block has attention modules and a fully connected layer and there are some normalization layers in between but it's essentially neurons network layers with this attention mechanism So coming from GPT2 uh when we move on to GPTOSS there is for example the mixture of experts um layer it's not invented by GPOSS it's a few years old um but it is essentially a tweak to make the model larger without consuming more compute in each forward pass So there is this fully connected layer and if listeners are familiar with um multilayer perceptions you can think of a mini multilayer perception a fully connected neurons network layer inside the transformer and it's very expensive because it's fully connected if you have thousand inputs thousand outputs it's like a 1 million connections and it's a very expensive part in this transformer and the idea is to kind of expand that into multiple feed forward networks So instead of having one let's say you have 256, but it would make it way more expensive because now you have 256, but you don't use all of them at the same time So you now have a router that says okay based on this input token it would be useful to use this um fully connected network And in that context it's called an expert So a mixture of experts means you have multiple experts And depending on what your input is let's say it's more math heavy it would use different experts compared to let's say translating input text from English to Spanish. It would maybe consult different experts It's not quite clear I mean not as clearcut to say okay this is only an expert for math and for Spanish is a bit more fuzzy but the",
        "start": 686.64,
        "duration": 1641.3590000000004
    },
    {
        "text": "would use different experts compared to let's say translating input text from English to Spanish. It would maybe consult different experts It's not quite clear I mean not as clearcut to say okay this is only an expert for math and for Spanish is a bit more fuzzy but the knowledge into the network but not all the knowledge is used all the time That would be very wasteful So you're kind of like during the token generation you're more selective There's a router that selects which tokens should go to which expert Adds more complexity It's harder to train There's a lot of you know that can go wrong like collapse and everything So I think that's why almost 3 still uses uh dense I mean you have I think models with mixture of experts but dense models where dense means so also it's jargon There's a distinction between dense and sparse So mixture of experts is considered sparse because we have a lot of experts but only few of them are active So that's called sparse and then dense would be the opposite where you only have like one fully connected module and it's always you know utilized So may maybe this is a good place to also talk about KV cache but actually before that even zooming out like fundamentally how many new ideas have been implemented from from GPT2 to today i like how different really are these architectures picture like the mixture of experts um the attention mechanism in GPToss that would be the group query attention mechanism so it's a slight tweak from mufti head attention to group query attention so there we have two I think they replaced layer norm by RMS norm but it's just like a different normalization layer not a big change it's just like a tweak um the nonlinear activation function people familiar in with deep new networks I mean it's the same as changing sigmoid with rely it's it's not changing the network fundamentally it's just like a tweak you a little little tweak um and that's about it I would say it's not really fundamentally that different it's still the same same architecture so you can convert one from one uh you can go from one into the other by just adding these these changes basically i It's fundamentally is still the same architecture i Yep. So for example you mentioned my book earlier that's a GPD2 model in the book because it's simple and it's very small Um so 124 120 million parameters approximately but in the bonus materials I do have almost three from scratch Gemma 3 from scratch and other types of from scratch models and I always started with my GPD2 model and just you know tweaked a well added different components and you get from one to the other It's like it's kind of like a lineage in a sense Yeah. i Can you build up an intuition for people",
        "start": 834.959,
        "duration": 1913.277000000001
    },
    {
        "text": "from scratch models and I always started with my GPD2 model and just you know tweaked a well added different components and you get from one to the other It's like it's kind of like a lineage in a sense Yeah. i Can you build up an intuition for people look at it there's so much rapid advancement in the AI world and at the same time fundamentally the architectures have not changed i So where is all the turbulence the turmoil of the advancement happening Where where's the gains to be had i So there are the different stages where you develop the network um or train the network You have the pre-training. Now um back in the day it was just restraining with GPD2. Now you have pre-training, mid-training and post-training. Um so I I think right now we are in the post-training focus stage I mean restraining still gives you um advantages if you scale it up to better higher quality data but then we have capability unlocks that were not there with GPD2. For example uh chat GBT it is basically a GPT3 model and GPT3 is the same as GPD2 in terms of architecture What was new was adding the um supervised fine-tuning and the reinforcement learning with human feedback So it's more on the algorithmic side rather than the architecture i I would say that the systems also change a lot I think if you listen to Nvidia's announcements they talk about these things like you now do FP8, you can now do FP4. And what is happening is these labs are figuring out how to utilize more compute to put it into one model which lets them train faster and that lets them put more data in And then you can find better configurations faster by doing this So you can look at like the essentially the tokens per second per GPU is a metric that you look at when you're doing large scale training and you could get you can go from like ask to ask by turning on FP8 training which means you're using less memory per parameter in the model and by saving less information you do less communication you can train faster So all of these like system things underpin way faster experimentation on data and algorithms that is kind of like it's this it's this kind of loop that keeps going where it's kind of hard to describe when you look at the architecture and they're exactly the same but the code base used to train these models is going to be vastly different and i you could probably like I don't the GPUs are different but you probably train GPTOSS 20B way faster in wall clock time than GPT2 was trained at the time Yeah, like you said they had for example in the mixture of experts this NV FP4 optimization for example where you get most throughput but I I do think this is",
        "start": 973.519,
        "duration": 2196.559000000001
    },
    {
        "text": "but you probably train GPTOSS 20B way faster in wall clock time than GPT2 was trained at the time Yeah, like you said they had for example in the mixture of experts this NV FP4 optimization for example where you get most throughput but I I do think this is doesn't give the model new capabilities in a sense it's just how much can we make make the computation coarser without suffering in terms of model performance degradation Um but I do think I mean there are alternatives popping up to the transformer There's text diffusion models u completely different paradigm Um and there's also I mean though text diffusion models might use transformer architectures but it's not an auto auto regressive um transformer and also mamba models uh it's a state space model but they do have tradeoffs and uh what's right is there's nothing that has replaced the auto regressive transformer as state-of-the-art model So like for state-of-the-art you would still do that go with that thing but there are now alternatives for the cheaper end like alternatives that are kind of um making compromises but it's not just one architecture anymore there are little ones coming up but if we talk about the state-of-the-art it's pretty much still the the transformer architecture auto regressive derived from GPT2 essentially",
        "start": 1117.2,
        "duration": 2318.239000000002
    }
]