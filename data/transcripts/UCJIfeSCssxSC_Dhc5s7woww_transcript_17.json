[
    {
        "text": "All right So, we just talked about a",
        "start": 3.04,
        "duration": 6.4
    },
    {
        "text": "bunch of the closed weight models Let's",
        "start": 4.96,
        "duration": 6.639
    },
    {
        "text": "talk about the open ones Uh, so tell me",
        "start": 9.44,
        "duration": 4.48
    },
    {
        "text": "about the landscape of open LM models",
        "start": 11.599,
        "duration": 3.761
    },
    {
        "text": "Which are interesting ones which stand",
        "start": 13.92,
        "duration": 3.76
    },
    {
        "text": "out to you and why We already mentioned",
        "start": 15.36,
        "duration": 3.2
    },
    {
        "text": "Deep Seek.",
        "start": 17.68,
        "duration": 2.24
    },
    {
        "text": "i Do you want to see how many we can name",
        "start": 18.56,
        "duration": 2.24
    },
    {
        "text": "off the top of our head",
        "start": 19.92,
        "duration": 2.56
    },
    {
        "text": "i Yeah. Yeah. Without looking at notes",
        "start": 20.8,
        "duration": 6.479
    },
    {
        "text": "i Deepseek, Kimmy, Miniax, Z.AI,",
        "start": 22.48,
        "duration": 9.119
    },
    {
        "text": "Ant Lang. We're just going Chinese. Um,",
        "start": 27.279,
        "duration": 7.361
    },
    {
        "text": "let's throw in Mistral AI Gemma. Um,",
        "start": 31.599,
        "duration": 5.921
    },
    {
        "text": "i yeah GPT OSS, the open source model by",
        "start": 34.64,
        "duration": 5.759
    },
    {
        "text": "Chet GPT. Actually, Nvidia Neimotron had",
        "start": 37.52,
        "duration": 4.48
    },
    {
        "text": "a or Nvidia had a really cool one a",
        "start": 40.399,
        "duration": 3.68
    },
    {
        "text": "Neotron 3. Um, there's a lot of stuff",
        "start": 42.0,
        "duration": 3.68
    },
    {
        "text": "especially at the end of the Quinn one",
        "start": 44.079,
        "duration": 2.241
    },
    {
        "text": "maybe the one",
        "start": 45.68,
        "duration": 2.399
    },
    {
        "text": "i Oh, yeah Quinn was the obvious name",
        "start": 46.32,
        "duration": 2.96
    },
    {
        "text": "that was I was trying to get through the",
        "start": 48.079,
        "duration": 2.721
    },
    {
        "text": "You can get at least 10 Chinese and at",
        "start": 49.28,
        "duration": 4.0
    },
    {
        "text": "least 10 Western. I think that",
        "start": 50.8,
        "duration": 4.399
    },
    {
        "text": "i I mean OpenAI released their first open",
        "start": 53.28,
        "duration": 2.72
    },
    {
        "text": "model",
        "start": 55.199,
        "duration": 3.36
    },
    {
        "text": "i since GPT2. That was when I when I meant",
        "start": 56.0,
        "duration": 4.239
    },
    {
        "text": "talked when I was writing about opening",
        "start": 58.559,
        "duration": 3.041
    },
    {
        "text": "eyes open model release They were all",
        "start": 60.239,
        "duration": 3.201
    },
    {
        "text": "like don't forget about GPT2 which I",
        "start": 61.6,
        "duration": 3.519
    },
    {
        "text": "thought was really funny because it's",
        "start": 63.44,
        "duration": 3.999
    },
    {
        "text": "just such a different time But GPTOSS",
        "start": 65.119,
        "duration": 4.241
    },
    {
        "text": "is actually a very strong model and does",
        "start": 67.439,
        "duration": 4.72
    },
    {
        "text": "some things that the other models don't",
        "start": 69.36,
        "duration": 5.68
    },
    {
        "text": "do very well And I think that selfishly",
        "start": 72.159,
        "duration": 4.561
    },
    {
        "text": "I'll promote a bunch of like western",
        "start": 75.04,
        "duration": 3.92
    },
    {
        "text": "companies So both in the US and Europe",
        "start": 76.72,
        "duration": 4.16
    },
    {
        "text": "have these like fully open models So I",
        "start": 78.96,
        "duration": 3.519
    },
    {
        "text": "work at Allen Institute for AI where",
        "start": 80.88,
        "duration": 3.279
    },
    {
        "text": "we've been building which releases data",
        "start": 82.479,
        "duration": 4.0
    },
    {
        "text": "and code and all of this And now we",
        "start": 84.159,
        "duration": 4.0
    },
    {
        "text": "have actual competition for people that",
        "start": 86.479,
        "duration": 3.441
    },
    {
        "text": "are trying to release everything so that",
        "start": 88.159,
        "duration": 3.28
    },
    {
        "text": "other people can train these models So",
        "start": 89.92,
        "duration": 3.44
    },
    {
        "text": "there's the institute for foundation",
        "start": 91.439,
        "duration": 4.961
    },
    {
        "text": "models or LLM 360 which is like had",
        "start": 93.36,
        "duration": 5.68
    },
    {
        "text": "their K2 models of various types",
        "start": 96.4,
        "duration": 5.92
    },
    {
        "text": "Apparis is a Swiss research consortium",
        "start": 99.04,
        "duration": 6.16
    },
    {
        "text": "Hugging face um has small LM which is",
        "start": 102.32,
        "duration": 5.28
    },
    {
        "text": "very popular and Nvidia's neutron has",
        "start": 105.2,
        "duration": 4.8
    },
    {
        "text": "started releasing data as well And then",
        "start": 107.6,
        "duration": 4.799
    },
    {
        "text": "Stanford's Marin community project which",
        "start": 110.0,
        "duration": 3.68
    },
    {
        "text": "is kind of making it so there's a",
        "start": 112.399,
        "duration": 2.881
    },
    {
        "text": "pipeline for people to open a GitHub",
        "start": 113.68,
        "duration": 3.52
    },
    {
        "text": "issue and implement a new idea and then",
        "start": 115.28,
        "duration": 4.159
    },
    {
        "text": "have it run in a stable language",
        "start": 117.2,
        "duration": 5.519
    },
    {
        "text": "modeling stack So this space",
        "start": 119.439,
        "duration": 5.68
    },
    {
        "text": "that list was way smaller in 2024. So I",
        "start": 122.719,
        "duration": 4.08
    },
    {
        "text": "think it was like just AI2. So that's a",
        "start": 125.119,
        "duration": 3.521
    },
    {
        "text": "great thing for more people to get",
        "start": 126.799,
        "duration": 3.201
    },
    {
        "text": "involved and to understand language",
        "start": 128.64,
        "duration": 3.599
    },
    {
        "text": "models which doesn't really have a like",
        "start": 130.0,
        "duration": 5.52
    },
    {
        "text": "a Chinese company that is has an analogy",
        "start": 132.239,
        "duration": 5.281
    },
    {
        "text": "While I'm talking I'll say that the",
        "start": 135.52,
        "duration": 4.719
    },
    {
        "text": "Chinese open language models tend to be",
        "start": 137.52,
        "duration": 4.56
    },
    {
        "text": "much bigger and that gives some of this",
        "start": 140.239,
        "duration": 3.841
    },
    {
        "text": "higher peak performance as where a lot",
        "start": 142.08,
        "duration": 3.68
    },
    {
        "text": "of these things that we like a lot",
        "start": 144.08,
        "duration": 4.4
    },
    {
        "text": "whether it was Gemma um and Nematron",
        "start": 145.76,
        "duration": 4.479
    },
    {
        "text": "have tended to be smaller models from",
        "start": 148.48,
        "duration": 3.52
    },
    {
        "text": "the US which is which is starting to",
        "start": 150.239,
        "duration": 4.481
    },
    {
        "text": "change from US and Europe. U Mr. large",
        "start": 152.0,
        "duration": 5.12
    },
    {
        "text": "three came out which was a giant model",
        "start": 154.72,
        "duration": 4.32
    },
    {
        "text": "very similar to deepseek architecture in",
        "start": 157.12,
        "duration": 5.28
    },
    {
        "text": "December and then a startup RCAI and",
        "start": 159.04,
        "duration": 6.72
    },
    {
        "text": "both Neatron have Neatron as Nvidia have",
        "start": 162.4,
        "duration": 6.24
    },
    {
        "text": "teased models of this way bigger than",
        "start": 165.76,
        "duration": 4.72
    },
    {
        "text": "100 billion parameters like this 400",
        "start": 168.64,
        "duration": 4.239
    },
    {
        "text": "billion parameter range coming in this",
        "start": 170.48,
        "duration": 5.039
    },
    {
        "text": "like Q1 2026 timeline So, I think this",
        "start": 172.879,
        "duration": 4.881
    },
    {
        "text": "kind of balance is set to change this",
        "start": 175.519,
        "duration": 3.761
    },
    {
        "text": "year in terms of what people are using",
        "start": 177.76,
        "duration": 4.16
    },
    {
        "text": "the Chinese versus US open models for",
        "start": 179.28,
        "duration": 5.2
    },
    {
        "text": "which will be which I'm personally gonna",
        "start": 181.92,
        "duration": 4.239
    },
    {
        "text": "be very excited to watch",
        "start": 184.48,
        "duration": 4.32
    },
    {
        "text": "i First of all huge props for being able",
        "start": 186.159,
        "duration": 4.881
    },
    {
        "text": "to name so many of these Did you",
        "start": 188.8,
        "duration": 3.76
    },
    {
        "text": "actually name Llama?",
        "start": 191.04,
        "duration": 2.32
    },
    {
        "text": "i Um, not",
        "start": 192.56,
        "duration": 2.56
    },
    {
        "text": "i I feel like laughter",
        "start": 193.36,
        "duration": 3.68
    },
    {
        "text": "this was not on purpose",
        "start": 195.12,
        "duration": 3.199
    },
    {
        "text": "i RIP Llama.",
        "start": 197.04,
        "duration": 2.0
    },
    {
        "text": "i Mhm.",
        "start": 198.319,
        "duration": 2.241
    },
    {
        "text": "i All right Can you mention what are some",
        "start": 199.04,
        "duration": 3.199
    },
    {
        "text": "interesting models that stand out So",
        "start": 200.56,
        "duration": 4.08
    },
    {
        "text": "you mentioned Quen 3 is is is obviously",
        "start": 202.239,
        "duration": 3.121
    },
    {
        "text": "a standout",
        "start": 204.64,
        "duration": 2.319
    },
    {
        "text": "i So I would say the year is almost",
        "start": 205.36,
        "duration": 4.879
    },
    {
        "text": "bookend by both DeepSeek version 3 and",
        "start": 206.959,
        "duration": 4.56
    },
    {
        "text": "R1 and then on the other hand in",
        "start": 210.239,
        "duration": 3.601
    },
    {
        "text": "December Deepseek version 3.2 because",
        "start": 211.519,
        "duration": 4.241
    },
    {
        "text": "what I like about those is they always",
        "start": 213.84,
        "duration": 3.759
    },
    {
        "text": "have an interesting architecture tweak",
        "start": 215.76,
        "duration": 3.759
    },
    {
        "text": "that others don't have But otherwise if",
        "start": 217.599,
        "duration": 4.081
    },
    {
        "text": "you want to go with um you know like the",
        "start": 219.519,
        "duration": 4.0
    },
    {
        "text": "familiar but really good performance",
        "start": 221.68,
        "duration": 4.88
    },
    {
        "text": "queen 3 and like um Nathan said also GPD",
        "start": 223.519,
        "duration": 5.521
    },
    {
        "text": "OSS and I think GPD OSS what's",
        "start": 226.56,
        "duration": 4.239
    },
    {
        "text": "interesting about it is kind of like the",
        "start": 229.04,
        "duration": 4.64
    },
    {
        "text": "first public or like open weight model",
        "start": 230.799,
        "duration": 5.201
    },
    {
        "text": "that was really trained with tool use in",
        "start": 233.68,
        "duration": 4.32
    },
    {
        "text": "mind which I do think is kind of a",
        "start": 236.0,
        "duration": 3.599
    },
    {
        "text": "little bit of a paradigm shift where the",
        "start": 238.0,
        "duration": 4.0
    },
    {
        "text": "ecosystem was not quite ready for it So",
        "start": 239.599,
        "duration": 4.56
    },
    {
        "text": "with tool use I mean that the LLM is",
        "start": 242.0,
        "duration": 4.239
    },
    {
        "text": "able to do a web search to call a Python",
        "start": 244.159,
        "duration": 4.561
    },
    {
        "text": "interpreter and I do think this it's a",
        "start": 246.239,
        "duration": 4.08
    },
    {
        "text": "standout because I think it's a huge",
        "start": 248.72,
        "duration": 4.799
    },
    {
        "text": "unlock because um one of the most u",
        "start": 250.319,
        "duration": 5.2
    },
    {
        "text": "common complaints about LMS are for",
        "start": 253.519,
        "duration": 4.4
    },
    {
        "text": "example hallucinations right and so in",
        "start": 255.519,
        "duration": 4.161
    },
    {
        "text": "my opinion one of the best ways to solve",
        "start": 257.919,
        "duration": 4.401
    },
    {
        "text": "hallucinations is to not try to always",
        "start": 259.68,
        "duration": 5.12
    },
    {
        "text": "remember information or make things up",
        "start": 262.32,
        "duration": 4.879
    },
    {
        "text": "for math why not use a calculator ape or",
        "start": 264.8,
        "duration": 3.44
    },
    {
        "text": "Python. Mhm.",
        "start": 267.199,
        "duration": 3.361
    },
    {
        "text": "i If I asked the LM who won the I don't",
        "start": 268.24,
        "duration": 5.2
    },
    {
        "text": "know soccer world cup in 1998 instead of",
        "start": 270.56,
        "duration": 5.84
    },
    {
        "text": "just trying to memorize it could go do a",
        "start": 273.44,
        "duration": 5.6
    },
    {
        "text": "search I think mostly it's usually still",
        "start": 276.4,
        "duration": 5.359
    },
    {
        "text": "a Google search So JPD GPOSS they would",
        "start": 279.04,
        "duration": 5.36
    },
    {
        "text": "do a tool call to Google maybe find the",
        "start": 281.759,
        "duration": 5.681
    },
    {
        "text": "FIFA website find okay it was France it",
        "start": 284.4,
        "duration": 5.2
    },
    {
        "text": "would get you that information reliably",
        "start": 287.44,
        "duration": 3.759
    },
    {
        "text": "instead of just trying to memorize it I",
        "start": 289.6,
        "duration": 4.159
    },
    {
        "text": "think it's a huge unlock which I think",
        "start": 291.199,
        "duration": 5.041
    },
    {
        "text": "right now is not fully utilized yet by",
        "start": 293.759,
        "duration": 4.88
    },
    {
        "text": "the open-source overweight ecosystem A",
        "start": 296.24,
        "duration": 5.44
    },
    {
        "text": "lot of people don't use tool call modes",
        "start": 298.639,
        "duration": 5.28
    },
    {
        "text": "because I think it's first is a trust",
        "start": 301.68,
        "duration": 3.36
    },
    {
        "text": "thing You don't want to run this on",
        "start": 303.919,
        "duration": 2.481
    },
    {
        "text": "your computer where it has access to",
        "start": 305.04,
        "duration": 2.8
    },
    {
        "text": "tools could wipe your hard drive or",
        "start": 306.4,
        "duration": 3.359
    },
    {
        "text": "whatever So you want to maybe",
        "start": 307.84,
        "duration": 4.48
    },
    {
        "text": "containerize that Um but I do think you",
        "start": 309.759,
        "duration": 3.921
    },
    {
        "text": "know that that is like a really",
        "start": 312.32,
        "duration": 3.2
    },
    {
        "text": "important step um for the upcoming years",
        "start": 313.68,
        "duration": 4.32
    },
    {
        "text": "to have this uh ability Yeah.",
        "start": 315.52,
        "duration": 5.28
    },
    {
        "text": "i So uh a few quick things First of all",
        "start": 318.0,
        "duration": 4.96
    },
    {
        "text": "thank you for defining what you mean by",
        "start": 320.8,
        "duration": 3.679
    },
    {
        "text": "tool used I think that's a great thing",
        "start": 322.96,
        "duration": 3.04
    },
    {
        "text": "to do in general for the concepts we're",
        "start": 324.479,
        "duration": 4.16
    },
    {
        "text": "talking about Even things as sort of",
        "start": 326.0,
        "duration": 5.36
    },
    {
        "text": "wellestablished as",
        "start": 328.639,
        "duration": 5.12
    },
    {
        "text": "i uh you have to say that means mixture of",
        "start": 331.36,
        "duration": 4.96
    },
    {
        "text": "experts and you kind of have to build up",
        "start": 333.759,
        "duration": 4.481
    },
    {
        "text": "an intuition for people what that means",
        "start": 336.32,
        "duration": 3.36
    },
    {
        "text": "how it's actually utilized what are the",
        "start": 338.24,
        "duration": 4.08
    },
    {
        "text": "different flavors So what does it mean",
        "start": 339.68,
        "duration": 5.04
    },
    {
        "text": "that there's just such explosion of open",
        "start": 342.32,
        "duration": 5.2
    },
    {
        "text": "models What's your intuition If you're",
        "start": 344.72,
        "duration": 4.319
    },
    {
        "text": "releasing an open model you want people",
        "start": 347.52,
        "duration": 3.36
    },
    {
        "text": "to use it as the first and foremost",
        "start": 349.039,
        "duration": 3.361
    },
    {
        "text": "thing And then after that comes things",
        "start": 350.88,
        "duration": 4.0
    },
    {
        "text": "like transparency and trust I think",
        "start": 352.4,
        "duration": 5.12
    },
    {
        "text": "when you look at China, the biggest",
        "start": 354.88,
        "duration": 4.159
    },
    {
        "text": "reason is that they want people around",
        "start": 357.52,
        "duration": 3.119
    },
    {
        "text": "the world to use these models And I",
        "start": 359.039,
        "duration": 3.6
    },
    {
        "text": "think a lot of people will not if you",
        "start": 360.639,
        "duration": 3.28
    },
    {
        "text": "look outside of the US, a lot of people",
        "start": 362.639,
        "duration": 2.481
    },
    {
        "text": "will not pay for software but they",
        "start": 363.919,
        "duration": 2.56
    },
    {
        "text": "might have computing resources where you",
        "start": 365.12,
        "duration": 3.12
    },
    {
        "text": "can put a model on it and run it I",
        "start": 366.479,
        "duration": 3.601
    },
    {
        "text": "think there can also be data that you",
        "start": 368.24,
        "duration": 3.36
    },
    {
        "text": "don't want to send to the cloud So this",
        "start": 370.08,
        "duration": 3.119
    },
    {
        "text": "the the number one thing is getting",
        "start": 371.6,
        "duration": 4.48
    },
    {
        "text": "people to use models use AI or use your",
        "start": 373.199,
        "duration": 4.641
    },
    {
        "text": "AI that might not be able to do it",
        "start": 376.08,
        "duration": 3.6
    },
    {
        "text": "without having access to the model",
        "start": 377.84,
        "duration": 4.0
    },
    {
        "text": "i I guess we should state explicitly So",
        "start": 379.68,
        "duration": 3.44
    },
    {
        "text": "we've been talking about these Chinese",
        "start": 381.84,
        "duration": 5.04
    },
    {
        "text": "models and open weight models often",
        "start": 383.12,
        "duration": 6.56
    },
    {
        "text": "times the way they're run is locally So",
        "start": 386.88,
        "duration": 4.48
    },
    {
        "text": "it's not like you're sending your data",
        "start": 389.68,
        "duration": 5.92
    },
    {
        "text": "to China or to whoever developed uh to",
        "start": 391.36,
        "duration": 5.839
    },
    {
        "text": "Silicon Valley whoever developed the",
        "start": 395.6,
        "duration": 4.08
    },
    {
        "text": "model A lot of American startups make",
        "start": 397.199,
        "duration": 4.641
    },
    {
        "text": "money by hosting these models from China",
        "start": 399.68,
        "duration": 4.079
    },
    {
        "text": "and selling them selling token It's",
        "start": 401.84,
        "duration": 3.52
    },
    {
        "text": "called like selling tokens which it",
        "start": 403.759,
        "duration": 3.521
    },
    {
        "text": "means somebody will call the model to do",
        "start": 405.36,
        "duration": 4.48
    },
    {
        "text": "some some piece of work I think the",
        "start": 407.28,
        "duration": 4.24
    },
    {
        "text": "other reason is for US companies like",
        "start": 409.84,
        "duration": 4.56
    },
    {
        "text": "Chad OpenAI is so GPU deprived like",
        "start": 411.52,
        "duration": 4.72
    },
    {
        "text": "they're so they're at the limits of the",
        "start": 414.4,
        "duration": 3.2
    },
    {
        "text": "GPUs whenever they make a release",
        "start": 416.24,
        "duration": 2.48
    },
    {
        "text": "they're always talking about like our",
        "start": 417.6,
        "duration": 3.84
    },
    {
        "text": "GPUs are hurting and I think there's",
        "start": 418.72,
        "duration": 5.28
    },
    {
        "text": "like like in one of these like GPTOSS",
        "start": 421.44,
        "duration": 4.4
    },
    {
        "text": "release sessions Sam Alman said like oh",
        "start": 424.0,
        "duration": 3.52
    },
    {
        "text": "we're releasing this because we can use",
        "start": 425.84,
        "duration": 4.0
    },
    {
        "text": "your GPUs we don't have to use we don't",
        "start": 427.52,
        "duration": 4.079
    },
    {
        "text": "have to use our GPUs and OpenAI can",
        "start": 429.84,
        "duration": 3.359
    },
    {
        "text": "still get distribution out of this which",
        "start": 431.599,
        "duration": 4.88
    },
    {
        "text": "is another very real thing doesn't cost",
        "start": 433.199,
        "duration": 6.0
    },
    {
        "text": "them anything and for the user I think",
        "start": 436.479,
        "duration": 4.72
    },
    {
        "text": "also I mean there are users who just use",
        "start": 439.199,
        "duration": 3.681
    },
    {
        "text": "the model locally how they would use uh",
        "start": 441.199,
        "duration": 3.921
    },
    {
        "text": "CHPD but also for companies I think it's",
        "start": 442.88,
        "duration": 3.759
    },
    {
        "text": "a huge unlock to have these models",
        "start": 445.12,
        "duration": 3.359
    },
    {
        "text": "because you can customize them you can",
        "start": 446.639,
        "duration": 4.96
    },
    {
        "text": "train them you can uh add post training",
        "start": 448.479,
        "duration": 5.041
    },
    {
        "text": "add more data like specialize them into",
        "start": 451.599,
        "duration": 4.241
    },
    {
        "text": "let's say law medical models whatever",
        "start": 453.52,
        "duration": 4.64
    },
    {
        "text": "you have and the appeal you mentioned",
        "start": 455.84,
        "duration": 4.24
    },
    {
        "text": "lama the appeal of the open weight",
        "start": 458.16,
        "duration": 4.0
    },
    {
        "text": "models from China is that the open",
        "start": 460.08,
        "duration": 4.239
    },
    {
        "text": "weight models are also the licenses are",
        "start": 462.16,
        "duration": 3.68
    },
    {
        "text": "even friendlier I think they are just",
        "start": 464.319,
        "duration": 3.761
    },
    {
        "text": "unrestricted open source licenses where",
        "start": 465.84,
        "duration": 4.079
    },
    {
        "text": "if you use something like Llama or",
        "start": 468.08,
        "duration": 3.679
    },
    {
        "text": "Gemma, there are some strings attached",
        "start": 469.919,
        "duration": 3.201
    },
    {
        "text": "I think it's like an upper limit in",
        "start": 471.759,
        "duration": 3.28
    },
    {
        "text": "terms of how many users you have and",
        "start": 473.12,
        "duration": 3.6
    },
    {
        "text": "then if you exceed I don't know so so",
        "start": 475.039,
        "duration": 3.28
    },
    {
        "text": "many million users you have to report",
        "start": 476.72,
        "duration": 3.44
    },
    {
        "text": "your finance",
        "start": 478.319,
        "duration": 3.841
    },
    {
        "text": "situation to let's say meta or something",
        "start": 480.16,
        "duration": 5.12
    },
    {
        "text": "like that and I think well it is a free",
        "start": 482.16,
        "duration": 4.879
    },
    {
        "text": "model but there are strings attached and",
        "start": 485.28,
        "duration": 4.479
    },
    {
        "text": "people do like things where strings are",
        "start": 487.039,
        "duration": 4.641
    },
    {
        "text": "not attached so I think that's also one",
        "start": 489.759,
        "duration": 4.401
    },
    {
        "text": "of the reasons besides performance why",
        "start": 491.68,
        "duration": 4.56
    },
    {
        "text": "the open weight models from China are so",
        "start": 494.16,
        "duration": 3.36
    },
    {
        "text": "popular because you you can just use",
        "start": 496.24,
        "duration": 3.12
    },
    {
        "text": "them there's no there's no catch in that",
        "start": 497.52,
        "duration": 2.56
    },
    {
        "text": "sense yeah",
        "start": 499.36,
        "duration": 2.559
    },
    {
        "text": "i the ecosystem has gotten on that front",
        "start": 500.08,
        "duration": 4.0
    },
    {
        "text": "but mostly downstream of these new",
        "start": 501.919,
        "duration": 3.84
    },
    {
        "text": "providers providing such open licenses",
        "start": 504.08,
        "duration": 2.799
    },
    {
        "text": "It was funny when you pulled up",
        "start": 505.759,
        "duration": 2.88
    },
    {
        "text": "perplexity It said Kimmy K2 thinking",
        "start": 506.879,
        "duration": 3.681
    },
    {
        "text": "hosted in the US, which is just like an",
        "start": 508.639,
        "duration": 3.52
    },
    {
        "text": "exact I've never seen this but it's an",
        "start": 510.56,
        "duration": 2.959
    },
    {
        "text": "exact example of what we're talking",
        "start": 512.159,
        "duration": 3.44
    },
    {
        "text": "about where people are sensitive to this",
        "start": 513.519,
        "duration": 4.32
    },
    {
        "text": "like Kimmy K2 thinking and Kimmy K2 is a",
        "start": 515.599,
        "duration": 4.161
    },
    {
        "text": "model that is very popular People say",
        "start": 517.839,
        "duration": 4.0
    },
    {
        "text": "that has very good like creative writing",
        "start": 519.76,
        "duration": 3.92
    },
    {
        "text": "and also in doing some software things",
        "start": 521.839,
        "duration": 3.041
    },
    {
        "text": "There's just these little quirks that",
        "start": 523.68,
        "duration": 3.12
    },
    {
        "text": "people pick up on with different models",
        "start": 524.88,
        "duration": 3.36
    },
    {
        "text": "that they like",
        "start": 526.8,
        "duration": 3.76
    },
    {
        "text": "uh what are some interesting ideas that",
        "start": 528.24,
        "duration": 4.56
    },
    {
        "text": "some of these models have explored that",
        "start": 530.56,
        "duration": 4.0
    },
    {
        "text": "you can speak to like that particular",
        "start": 532.8,
        "duration": 2.88
    },
    {
        "text": "interesting to you",
        "start": 534.56,
        "duration": 2.8
    },
    {
        "text": "i Maybe we can go chronologically I mean",
        "start": 535.68,
        "duration": 3.839
    },
    {
        "text": "there was of course Deepseek um Deepseek",
        "start": 537.36,
        "duration": 4.32
    },
    {
        "text": "R1 that came out in January if we just",
        "start": 539.519,
        "duration": 4.721
    },
    {
        "text": "focus on 2025. However, this was based",
        "start": 541.68,
        "duration": 4.719
    },
    {
        "text": "on Deepseek version 3 which came out the",
        "start": 544.24,
        "duration": 5.52
    },
    {
        "text": "year um before in December 2024. There",
        "start": 546.399,
        "duration": 4.88
    },
    {
        "text": "are multiple things on the architecture",
        "start": 549.76,
        "duration": 3.28
    },
    {
        "text": "side What is fascinating is you can",
        "start": 551.279,
        "duration": 3.68
    },
    {
        "text": "still I mean that's what I do in my from",
        "start": 553.04,
        "duration": 3.6
    },
    {
        "text": "scratch coding projects you can still",
        "start": 554.959,
        "duration": 3.921
    },
    {
        "text": "start with GPD2 and you get can add",
        "start": 556.64,
        "duration": 3.92
    },
    {
        "text": "things to that model to make it into",
        "start": 558.88,
        "duration": 4.0
    },
    {
        "text": "this other model So it's all still kind",
        "start": 560.56,
        "duration": 4.399
    },
    {
        "text": "of like the same lineage the same it is",
        "start": 562.88,
        "duration": 4.56
    },
    {
        "text": "a very close relationship between those",
        "start": 564.959,
        "duration": 5.281
    },
    {
        "text": "but top of my head deepseeek what was uh",
        "start": 567.44,
        "duration": 4.88
    },
    {
        "text": "unique there is the mixture of ex not I",
        "start": 570.24,
        "duration": 3.36
    },
    {
        "text": "mean they were not inventing mixture of",
        "start": 572.32,
        "duration": 3.04
    },
    {
        "text": "experts we can maybe talk a bit more",
        "start": 573.6,
        "duration": 4.799
    },
    {
        "text": "what mixture of experts means um but",
        "start": 575.36,
        "duration": 4.8
    },
    {
        "text": "just to list these things first before",
        "start": 578.399,
        "duration": 4.0
    },
    {
        "text": "we dive into detail mixture of experts",
        "start": 580.16,
        "duration": 4.0
    },
    {
        "text": "but then they also had a mufti head",
        "start": 582.399,
        "duration": 4.081
    },
    {
        "text": "latent attention which is a tweak to the",
        "start": 584.16,
        "duration": 5.2
    },
    {
        "text": "attention mechanism where this was I",
        "start": 586.48,
        "duration": 5.12
    },
    {
        "text": "would say 2025 Five, the main",
        "start": 589.36,
        "duration": 5.919
    },
    {
        "text": "distinguishing factor between these open",
        "start": 591.6,
        "duration": 6.48
    },
    {
        "text": "weight models different tweaks to make",
        "start": 595.279,
        "duration": 5.361
    },
    {
        "text": "inference or KV cache size We can also",
        "start": 598.08,
        "duration": 5.12
    },
    {
        "text": "define KV cache in a few moments But to",
        "start": 600.64,
        "duration": 5.52
    },
    {
        "text": "kind of make it more economical to have",
        "start": 603.2,
        "duration": 4.8
    },
    {
        "text": "long context to shrink the KV cache",
        "start": 606.16,
        "duration": 4.16
    },
    {
        "text": "size So what are tweaks um that we can",
        "start": 608.0,
        "duration": 3.92
    },
    {
        "text": "do and most of them focused on the",
        "start": 610.32,
        "duration": 3.36
    },
    {
        "text": "attention mechanism There is multiped",
        "start": 611.92,
        "duration": 4.08
    },
    {
        "text": "latent attention in in deepseek. There",
        "start": 613.68,
        "duration": 4.159
    },
    {
        "text": "is group query attention which is still",
        "start": 616.0,
        "duration": 3.68
    },
    {
        "text": "very popular It's not invented by any",
        "start": 617.839,
        "duration": 3.12
    },
    {
        "text": "of those models It goes back a few",
        "start": 619.68,
        "duration": 3.36
    },
    {
        "text": "years but that that would be the other",
        "start": 620.959,
        "duration": 4.32
    },
    {
        "text": "option Sliding window attention I",
        "start": 623.04,
        "duration": 4.64
    },
    {
        "text": "think almost reuses it um if I remember",
        "start": 625.279,
        "duration": 4.401
    },
    {
        "text": "correctly So there these different",
        "start": 627.68,
        "duration": 4.8
    },
    {
        "text": "tweaks that make the models different",
        "start": 629.68,
        "duration": 5.2
    },
    {
        "text": "Otherwise um I put them all together in",
        "start": 632.48,
        "duration": 5.2
    },
    {
        "text": "an article once where um I just compared",
        "start": 634.88,
        "duration": 4.16
    },
    {
        "text": "them They are very surprisingly",
        "start": 637.68,
        "duration": 2.96
    },
    {
        "text": "similar It's just different numbers in",
        "start": 639.04,
        "duration": 3.68
    },
    {
        "text": "terms of how many repetitions of the",
        "start": 640.64,
        "duration": 4.8
    },
    {
        "text": "transformer block you have in the center",
        "start": 642.72,
        "duration": 5.28
    },
    {
        "text": "and like just little little knobs that",
        "start": 645.44,
        "duration": 4.16
    },
    {
        "text": "people tune But but what's so nice",
        "start": 648.0,
        "duration": 3.92
    },
    {
        "text": "about it is it's it it works no matter",
        "start": 649.6,
        "duration": 4.239
    },
    {
        "text": "what You can tweak things You can move",
        "start": 651.92,
        "duration": 4.0
    },
    {
        "text": "the normalization layers around You get",
        "start": 653.839,
        "duration": 3.921
    },
    {
        "text": "some performance gains and is always",
        "start": 655.92,
        "duration": 3.84
    },
    {
        "text": "very good in ablation studies showing",
        "start": 657.76,
        "duration": 5.04
    },
    {
        "text": "what actually what it does to the model",
        "start": 659.76,
        "duration": 4.96
    },
    {
        "text": "If you move something around ablation",
        "start": 662.8,
        "duration": 3.12
    },
    {
        "text": "studies does it make it better or",
        "start": 664.72,
        "duration": 3.119
    },
    {
        "text": "worse But there are so many let's say",
        "start": 665.92,
        "duration": 3.44
    },
    {
        "text": "ways you can implement a transformer and",
        "start": 667.839,
        "duration": 4.081
    },
    {
        "text": "make it still work Big ideas um that",
        "start": 669.36,
        "duration": 4.08
    },
    {
        "text": "are still prevalent is mixture of",
        "start": 671.92,
        "duration": 4.24
    },
    {
        "text": "experts mufti latent attention um",
        "start": 673.44,
        "duration": 4.16
    },
    {
        "text": "sliding window attention group query",
        "start": 676.16,
        "duration": 3.28
    },
    {
        "text": "attention and then at the end of the",
        "start": 677.6,
        "duration": 4.56
    },
    {
        "text": "year we saw a focus on making the",
        "start": 679.44,
        "duration": 4.959
    },
    {
        "text": "attention mechanism scale linearly with",
        "start": 682.16,
        "duration": 4.48
    },
    {
        "text": "inference token prediction So there",
        "start": 684.399,
        "duration": 5.68
    },
    {
        "text": "were queen 3 next for example which added",
        "start": 686.64,
        "duration": 5.68
    },
    {
        "text": "a gated delta It's it's like um kind of",
        "start": 690.079,
        "duration": 4.481
    },
    {
        "text": "like inspired by um state space models",
        "start": 692.32,
        "duration": 4.079
    },
    {
        "text": "where you have a fixed state that you",
        "start": 694.56,
        "duration": 3.36
    },
    {
        "text": "keep updating but it makes essentially",
        "start": 696.399,
        "duration": 3.281
    },
    {
        "text": "this attention",
        "start": 697.92,
        "duration": 3.52
    },
    {
        "text": "cheaper or it replaces attention with a",
        "start": 699.68,
        "duration": 2.64
    },
    {
        "text": "cheaper operation",
        "start": 701.44,
        "duration": 3.68
    },
    {
        "text": "i and it maybe is it useful to step back",
        "start": 702.32,
        "duration": 4.8
    },
    {
        "text": "and talk about transform architecture in",
        "start": 705.12,
        "duration": 2.32
    },
    {
        "text": "general",
        "start": 707.12,
        "duration": 2.88
    },
    {
        "text": "i Yeah. So maybe we should start with the",
        "start": 707.44,
        "duration": 4.959
    },
    {
        "text": "GPT2 architecture The transformer that",
        "start": 710.0,
        "duration": 4.0
    },
    {
        "text": "was derived from the attention is all",
        "start": 712.399,
        "duration": 3.281
    },
    {
        "text": "you need paper Mhm.",
        "start": 714.0,
        "duration": 3.76
    },
    {
        "text": "i So the attention uh is all you need",
        "start": 715.68,
        "duration": 3.599
    },
    {
        "text": "paper had a transformer architecture",
        "start": 717.76,
        "duration": 3.199
    },
    {
        "text": "that had two parts an encoder and a",
        "start": 719.279,
        "duration": 5.841
    },
    {
        "text": "decoder and GPT went just focusing in on",
        "start": 720.959,
        "duration": 6.641
    },
    {
        "text": "the decoder part It is essentially",
        "start": 725.12,
        "duration": 4.959
    },
    {
        "text": "still a neurons network um and it has",
        "start": 727.6,
        "duration": 6.0
    },
    {
        "text": "this attention mechanism inside and you",
        "start": 730.079,
        "duration": 5.44
    },
    {
        "text": "predict one token at a time You pass it",
        "start": 733.6,
        "duration": 3.84
    },
    {
        "text": "through an embedding layer There's the",
        "start": 735.519,
        "duration": 3.601
    },
    {
        "text": "transformer block The transformer block",
        "start": 737.44,
        "duration": 3.839
    },
    {
        "text": "has attention modules and a fully",
        "start": 739.12,
        "duration": 3.68
    },
    {
        "text": "connected layer and there are some",
        "start": 741.279,
        "duration": 3.441
    },
    {
        "text": "normalization layers in between but it's",
        "start": 742.8,
        "duration": 3.2
    },
    {
        "text": "essentially neurons network layers with",
        "start": 744.72,
        "duration": 3.52
    },
    {
        "text": "this attention mechanism So coming from",
        "start": 746.0,
        "duration": 6.16
    },
    {
        "text": "GPT2 uh when we move on to GPTOSS",
        "start": 748.24,
        "duration": 5.52
    },
    {
        "text": "there is for example the mixture of",
        "start": 752.16,
        "duration": 3.84
    },
    {
        "text": "experts um layer it's not invented by",
        "start": 753.76,
        "duration": 5.36
    },
    {
        "text": "GPOSS it's a few years old um but it is",
        "start": 756.0,
        "duration": 6.079
    },
    {
        "text": "essentially a tweak to make the model",
        "start": 759.12,
        "duration": 6.959
    },
    {
        "text": "larger without consuming more compute in",
        "start": 762.079,
        "duration": 6.32
    },
    {
        "text": "each forward pass So there is this",
        "start": 766.079,
        "duration": 4.641
    },
    {
        "text": "fully connected layer and if listeners",
        "start": 768.399,
        "duration": 4.161
    },
    {
        "text": "are familiar with um multilayer",
        "start": 770.72,
        "duration": 4.239
    },
    {
        "text": "perceptions you can think of a mini",
        "start": 772.56,
        "duration": 4.48
    },
    {
        "text": "multilayer perception a fully connected",
        "start": 774.959,
        "duration": 3.44
    },
    {
        "text": "neurons network layer inside the",
        "start": 777.04,
        "duration": 3.359
    },
    {
        "text": "transformer and it's very expensive",
        "start": 778.399,
        "duration": 3.68
    },
    {
        "text": "because it's fully connected if you have",
        "start": 780.399,
        "duration": 3.281
    },
    {
        "text": "thousand inputs thousand outputs it's",
        "start": 782.079,
        "duration": 4.641
    },
    {
        "text": "like a 1 million connections and it's a",
        "start": 783.68,
        "duration": 4.959
    },
    {
        "text": "very expensive part in this transformer",
        "start": 786.72,
        "duration": 5.04
    },
    {
        "text": "and the idea is to kind of expand that",
        "start": 788.639,
        "duration": 6.32
    },
    {
        "text": "into multiple feed forward networks So",
        "start": 791.76,
        "duration": 4.8
    },
    {
        "text": "instead of having one let's say you",
        "start": 794.959,
        "duration": 3.921
    },
    {
        "text": "have 256, but it would make it way more",
        "start": 796.56,
        "duration": 4.399
    },
    {
        "text": "expensive because now you have 256, but",
        "start": 798.88,
        "duration": 3.519
    },
    {
        "text": "you don't use all of them at the same",
        "start": 800.959,
        "duration": 3.921
    },
    {
        "text": "time So you now have a router that",
        "start": 802.399,
        "duration": 4.401
    },
    {
        "text": "says okay based on this input token",
        "start": 804.88,
        "duration": 4.88
    },
    {
        "text": "it would be useful to use this um fully",
        "start": 806.8,
        "duration": 4.88
    },
    {
        "text": "connected network And in that context",
        "start": 809.76,
        "duration": 3.68
    },
    {
        "text": "it's called an expert So a mixture of",
        "start": 811.68,
        "duration": 4.159
    },
    {
        "text": "experts means you have multiple experts",
        "start": 813.44,
        "duration": 4.88
    },
    {
        "text": "And depending on what your input is",
        "start": 815.839,
        "duration": 4.56
    },
    {
        "text": "let's say it's more math heavy it would",
        "start": 818.32,
        "duration": 4.319
    },
    {
        "text": "use different experts compared to let's",
        "start": 820.399,
        "duration": 4.56
    },
    {
        "text": "say translating input text from English",
        "start": 822.639,
        "duration": 3.841
    },
    {
        "text": "to Spanish. It would maybe consult",
        "start": 824.959,
        "duration": 3.921
    },
    {
        "text": "different experts It's not quite clear",
        "start": 826.48,
        "duration": 4.08
    },
    {
        "text": "I mean not as clearcut to say okay",
        "start": 828.88,
        "duration": 4.0
    },
    {
        "text": "this is only an expert for math and for",
        "start": 830.56,
        "duration": 4.399
    },
    {
        "text": "Spanish is a bit more fuzzy but the",
        "start": 832.88,
        "duration": 4.88
    },
    {
        "text": "idea is essentially that you pack more",
        "start": 834.959,
        "duration": 4.641
    },
    {
        "text": "knowledge into the network but not all",
        "start": 837.76,
        "duration": 3.759
    },
    {
        "text": "the knowledge is used all the time That",
        "start": 839.6,
        "duration": 3.76
    },
    {
        "text": "would be very wasteful So you're kind",
        "start": 841.519,
        "duration": 4.0
    },
    {
        "text": "of like during the token generation",
        "start": 843.36,
        "duration": 4.24
    },
    {
        "text": "you're more selective There's a router",
        "start": 845.519,
        "duration": 4.641
    },
    {
        "text": "that selects which tokens should go to",
        "start": 847.6,
        "duration": 4.96
    },
    {
        "text": "which expert Adds more complexity It's",
        "start": 850.16,
        "duration": 4.239
    },
    {
        "text": "harder to train There's a lot of you",
        "start": 852.56,
        "duration": 3.92
    },
    {
        "text": "know that can go wrong like collapse and",
        "start": 854.399,
        "duration": 3.761
    },
    {
        "text": "everything So I think that's why almost",
        "start": 856.48,
        "duration": 4.32
    },
    {
        "text": "3 still uses uh dense I mean you have I",
        "start": 858.16,
        "duration": 4.64
    },
    {
        "text": "think models with mixture of experts but",
        "start": 860.8,
        "duration": 5.279
    },
    {
        "text": "dense models where dense means so also",
        "start": 862.8,
        "duration": 5.12
    },
    {
        "text": "it's jargon There's a distinction",
        "start": 866.079,
        "duration": 4.401
    },
    {
        "text": "between dense and sparse So mixture of",
        "start": 867.92,
        "duration": 4.479
    },
    {
        "text": "experts is considered sparse because we",
        "start": 870.48,
        "duration": 3.76
    },
    {
        "text": "have a lot of experts but only few of",
        "start": 872.399,
        "duration": 3.761
    },
    {
        "text": "them are active So that's called sparse",
        "start": 874.24,
        "duration": 3.52
    },
    {
        "text": "and then dense would be the opposite",
        "start": 876.16,
        "duration": 3.039
    },
    {
        "text": "where you only have like one fully",
        "start": 877.76,
        "duration": 3.12
    },
    {
        "text": "connected module and it's always you",
        "start": 879.199,
        "duration": 4.161
    },
    {
        "text": "know utilized So may maybe this is a",
        "start": 880.88,
        "duration": 4.079
    },
    {
        "text": "good place to also talk about KV cache",
        "start": 883.36,
        "duration": 3.52
    },
    {
        "text": "but actually before that even zooming",
        "start": 884.959,
        "duration": 5.12
    },
    {
        "text": "out like fundamentally how many new",
        "start": 886.88,
        "duration": 6.639
    },
    {
        "text": "ideas have been implemented from from",
        "start": 890.079,
        "duration": 5.921
    },
    {
        "text": "GPT2 to today",
        "start": 893.519,
        "duration": 4.801
    },
    {
        "text": "i like how different really are these",
        "start": 896.0,
        "duration": 4.399
    },
    {
        "text": "architectures picture like the mixture",
        "start": 898.32,
        "duration": 4.4
    },
    {
        "text": "of experts um the attention mechanism in",
        "start": 900.399,
        "duration": 4.88
    },
    {
        "text": "GPToss that would be the group query",
        "start": 902.72,
        "duration": 4.16
    },
    {
        "text": "attention mechanism so it's a slight",
        "start": 905.279,
        "duration": 3.201
    },
    {
        "text": "tweak from mufti head attention to group",
        "start": 906.88,
        "duration": 3.92
    },
    {
        "text": "query attention so there we have two I",
        "start": 908.48,
        "duration": 4.08
    },
    {
        "text": "think they replaced",
        "start": 910.8,
        "duration": 3.92
    },
    {
        "text": "layer norm by RMS norm but it's just",
        "start": 912.56,
        "duration": 4.16
    },
    {
        "text": "like a different normalization layer not",
        "start": 914.72,
        "duration": 4.32
    },
    {
        "text": "a big change it's just like a tweak um",
        "start": 916.72,
        "duration": 5.04
    },
    {
        "text": "the nonlinear activation function people",
        "start": 919.04,
        "duration": 4.88
    },
    {
        "text": "familiar in with deep new networks I",
        "start": 921.76,
        "duration": 4.16
    },
    {
        "text": "mean it's the same as changing sigmoid",
        "start": 923.92,
        "duration": 3.84
    },
    {
        "text": "with rely it's it's not changing the",
        "start": 925.92,
        "duration": 3.44
    },
    {
        "text": "network fundamentally it's just like a",
        "start": 927.76,
        "duration": 4.16
    },
    {
        "text": "tweak you a little little tweak um and",
        "start": 929.36,
        "duration": 4.32
    },
    {
        "text": "that's about it I would say it's not",
        "start": 931.92,
        "duration": 3.76
    },
    {
        "text": "really fundamentally that different it's",
        "start": 933.68,
        "duration": 3.92
    },
    {
        "text": "still the same same architecture so you",
        "start": 935.68,
        "duration": 4.719
    },
    {
        "text": "can convert one from one uh you can go",
        "start": 937.6,
        "duration": 4.32
    },
    {
        "text": "from one into the other by just adding",
        "start": 940.399,
        "duration": 3.361
    },
    {
        "text": "these these changes basically",
        "start": 941.92,
        "duration": 3.44
    },
    {
        "text": "i It's fundamentally is still the same",
        "start": 943.76,
        "duration": 2.48
    },
    {
        "text": "architecture",
        "start": 945.36,
        "duration": 2.479
    },
    {
        "text": "i Yep. So for example you mentioned my",
        "start": 946.24,
        "duration": 3.76
    },
    {
        "text": "book earlier that's a GPD2 model in the",
        "start": 947.839,
        "duration": 3.44
    },
    {
        "text": "book because it's simple and it's very",
        "start": 950.0,
        "duration": 4.88
    },
    {
        "text": "small Um so 124 120 million parameters",
        "start": 951.279,
        "duration": 5.601
    },
    {
        "text": "approximately but in the bonus materials",
        "start": 954.88,
        "duration": 4.56
    },
    {
        "text": "I do have almost three from scratch",
        "start": 956.88,
        "duration": 4.399
    },
    {
        "text": "Gemma 3 from scratch and other types of",
        "start": 959.44,
        "duration": 3.28
    },
    {
        "text": "from scratch models and I always started",
        "start": 961.279,
        "duration": 3.521
    },
    {
        "text": "with my GPD2 model and just you know",
        "start": 962.72,
        "duration": 3.44
    },
    {
        "text": "tweaked a well added different",
        "start": 964.8,
        "duration": 3.279
    },
    {
        "text": "components and you get from one to the",
        "start": 966.16,
        "duration": 3.84
    },
    {
        "text": "other It's like it's kind of like a",
        "start": 968.079,
        "duration": 3.521
    },
    {
        "text": "lineage in a sense Yeah.",
        "start": 970.0,
        "duration": 3.519
    },
    {
        "text": "i Can you build up an intuition for people",
        "start": 971.6,
        "duration": 4.159
    },
    {
        "text": "because sort of when you zoom out you",
        "start": 973.519,
        "duration": 4.24
    },
    {
        "text": "look at it there's so much rapid",
        "start": 975.759,
        "duration": 5.121
    },
    {
        "text": "advancement in the AI world and at the",
        "start": 977.759,
        "duration": 4.961
    },
    {
        "text": "same time fundamentally the",
        "start": 980.88,
        "duration": 4.16
    },
    {
        "text": "architectures have not changed",
        "start": 982.72,
        "duration": 5.679
    },
    {
        "text": "i So where is all the turbulence the",
        "start": 985.04,
        "duration": 6.96
    },
    {
        "text": "turmoil of the advancement happening",
        "start": 988.399,
        "duration": 6.401
    },
    {
        "text": "Where where's the gains to be had",
        "start": 992.0,
        "duration": 4.56
    },
    {
        "text": "i So there are the different stages where",
        "start": 994.8,
        "duration": 4.0
    },
    {
        "text": "you develop the network um or train the",
        "start": 996.56,
        "duration": 4.32
    },
    {
        "text": "network You have the pre-training. Now",
        "start": 998.8,
        "duration": 3.599
    },
    {
        "text": "um back in the day it was just",
        "start": 1000.88,
        "duration": 3.6
    },
    {
        "text": "restraining with GPD2. Now you have",
        "start": 1002.399,
        "duration": 3.761
    },
    {
        "text": "pre-training, mid-training and",
        "start": 1004.48,
        "duration": 4.799
    },
    {
        "text": "post-training. Um so I I think right now",
        "start": 1006.16,
        "duration": 5.119
    },
    {
        "text": "we are in the post-training focus stage",
        "start": 1009.279,
        "duration": 5.12
    },
    {
        "text": "I mean restraining still gives you um",
        "start": 1011.279,
        "duration": 5.12
    },
    {
        "text": "advantages if you scale it up to better",
        "start": 1014.399,
        "duration": 4.88
    },
    {
        "text": "higher quality data but then we have",
        "start": 1016.399,
        "duration": 4.88
    },
    {
        "text": "capability unlocks that were not there",
        "start": 1019.279,
        "duration": 5.601
    },
    {
        "text": "with GPD2. For example uh chat GBT it",
        "start": 1021.279,
        "duration": 6.481
    },
    {
        "text": "is basically a GPT3 model and GPT3 is",
        "start": 1024.88,
        "duration": 4.24
    },
    {
        "text": "the same as GPD2 in terms of",
        "start": 1027.76,
        "duration": 3.919
    },
    {
        "text": "architecture What was new was adding",
        "start": 1029.12,
        "duration": 4.959
    },
    {
        "text": "the um supervised fine-tuning and the",
        "start": 1031.679,
        "duration": 3.681
    },
    {
        "text": "reinforcement learning with human",
        "start": 1034.079,
        "duration": 2.561
    },
    {
        "text": "feedback So it's more on the",
        "start": 1035.36,
        "duration": 2.719
    },
    {
        "text": "algorithmic side rather than the",
        "start": 1036.64,
        "duration": 2.0
    },
    {
        "text": "architecture",
        "start": 1038.079,
        "duration": 2.081
    },
    {
        "text": "i I would say that the systems also change",
        "start": 1038.64,
        "duration": 3.039
    },
    {
        "text": "a lot I think if you listen to Nvidia's",
        "start": 1040.16,
        "duration": 2.72
    },
    {
        "text": "announcements they talk about these",
        "start": 1041.679,
        "duration": 3.441
    },
    {
        "text": "things like you now do FP8, you can now",
        "start": 1042.88,
        "duration": 5.199
    },
    {
        "text": "do FP4. And what is happening is these",
        "start": 1045.12,
        "duration": 4.96
    },
    {
        "text": "labs are figuring out how to utilize",
        "start": 1048.079,
        "duration": 3.681
    },
    {
        "text": "more compute to put it into one model",
        "start": 1050.08,
        "duration": 3.839
    },
    {
        "text": "which lets them train faster and that",
        "start": 1051.76,
        "duration": 4.08
    },
    {
        "text": "lets them put more data in And then you",
        "start": 1053.919,
        "duration": 4.561
    },
    {
        "text": "can find better configurations faster by",
        "start": 1055.84,
        "duration": 4.64
    },
    {
        "text": "doing this So you can look at like the",
        "start": 1058.48,
        "duration": 3.68
    },
    {
        "text": "essentially the tokens per second per",
        "start": 1060.48,
        "duration": 3.52
    },
    {
        "text": "GPU is a metric that you look at when",
        "start": 1062.16,
        "duration": 3.68
    },
    {
        "text": "you're doing large scale training and",
        "start": 1064.0,
        "duration": 3.679
    },
    {
        "text": "you could get you can go from like ask",
        "start": 1065.84,
        "duration": 4.88
    },
    {
        "text": "to ask by turning on FP8 training which",
        "start": 1067.679,
        "duration": 5.521
    },
    {
        "text": "means you're using less memory per",
        "start": 1070.72,
        "duration": 4.56
    },
    {
        "text": "parameter in the model and by saving",
        "start": 1073.2,
        "duration": 3.52
    },
    {
        "text": "less information you do less",
        "start": 1075.28,
        "duration": 3.2
    },
    {
        "text": "communication you can train faster So",
        "start": 1076.72,
        "duration": 4.64
    },
    {
        "text": "all of these like system things underpin",
        "start": 1078.48,
        "duration": 5.36
    },
    {
        "text": "way faster experimentation on data and",
        "start": 1081.36,
        "duration": 4.559
    },
    {
        "text": "algorithms that is kind of like it's",
        "start": 1083.84,
        "duration": 5.68
    },
    {
        "text": "this it's this kind of",
        "start": 1085.919,
        "duration": 5.681
    },
    {
        "text": "loop that keeps going where it's kind of",
        "start": 1089.52,
        "duration": 3.36
    },
    {
        "text": "hard to describe when you look at the",
        "start": 1091.6,
        "duration": 2.56
    },
    {
        "text": "architecture and they're exactly the",
        "start": 1092.88,
        "duration": 2.96
    },
    {
        "text": "same but the code base used to train",
        "start": 1094.16,
        "duration": 3.36
    },
    {
        "text": "these models is going to be vastly",
        "start": 1095.84,
        "duration": 3.04
    },
    {
        "text": "different and",
        "start": 1097.52,
        "duration": 3.6
    },
    {
        "text": "i you could probably like I don't the GPUs",
        "start": 1098.88,
        "duration": 3.36
    },
    {
        "text": "are different but you probably train",
        "start": 1101.12,
        "duration": 4.559
    },
    {
        "text": "GPTOSS 20B way faster in wall clock time",
        "start": 1102.24,
        "duration": 6.16
    },
    {
        "text": "than GPT2 was trained at the time Yeah,",
        "start": 1105.679,
        "duration": 4.0
    },
    {
        "text": "like you said they had for example in",
        "start": 1108.4,
        "duration": 4.08
    },
    {
        "text": "the mixture of experts this NV FP4",
        "start": 1109.679,
        "duration": 4.88
    },
    {
        "text": "optimization for example where you get",
        "start": 1112.48,
        "duration": 4.72
    },
    {
        "text": "most throughput but I I do think this is",
        "start": 1114.559,
        "duration": 5.041
    },
    {
        "text": "for the speed This is true but uh it",
        "start": 1117.2,
        "duration": 3.92
    },
    {
        "text": "doesn't give the model new capabilities",
        "start": 1119.6,
        "duration": 4.0
    },
    {
        "text": "in a sense it's just how much can we",
        "start": 1121.12,
        "duration": 4.799
    },
    {
        "text": "make make the computation coarser",
        "start": 1123.6,
        "duration": 5.199
    },
    {
        "text": "without suffering in terms of model",
        "start": 1125.919,
        "duration": 5.281
    },
    {
        "text": "performance degradation Um but I do",
        "start": 1128.799,
        "duration": 3.601
    },
    {
        "text": "think I mean there are alternatives",
        "start": 1131.2,
        "duration": 3.12
    },
    {
        "text": "popping up to the transformer There's",
        "start": 1132.4,
        "duration": 3.92
    },
    {
        "text": "text diffusion models u completely",
        "start": 1134.32,
        "duration": 4.32
    },
    {
        "text": "different paradigm Um and there's also",
        "start": 1136.32,
        "duration": 3.92
    },
    {
        "text": "I mean though text diffusion models",
        "start": 1138.64,
        "duration": 2.96
    },
    {
        "text": "might use transformer architectures but",
        "start": 1140.24,
        "duration": 3.679
    },
    {
        "text": "it's not an auto auto regressive um",
        "start": 1141.6,
        "duration": 5.28
    },
    {
        "text": "transformer and also mamba models uh",
        "start": 1143.919,
        "duration": 4.801
    },
    {
        "text": "it's a state space model but they do",
        "start": 1146.88,
        "duration": 3.919
    },
    {
        "text": "have tradeoffs and uh what's right is",
        "start": 1148.72,
        "duration": 4.8
    },
    {
        "text": "there's nothing that has replaced the",
        "start": 1150.799,
        "duration": 4.481
    },
    {
        "text": "auto regressive transformer as",
        "start": 1153.52,
        "duration": 3.36
    },
    {
        "text": "state-of-the-art model So like for",
        "start": 1155.28,
        "duration": 3.759
    },
    {
        "text": "state-of-the-art you would still do that",
        "start": 1156.88,
        "duration": 4.159
    },
    {
        "text": "go with that thing but there are now",
        "start": 1159.039,
        "duration": 3.681
    },
    {
        "text": "alternatives for the cheaper end like",
        "start": 1161.039,
        "duration": 4.721
    },
    {
        "text": "alternatives that are kind of um making",
        "start": 1162.72,
        "duration": 5.28
    },
    {
        "text": "compromises but it's not just one",
        "start": 1165.76,
        "duration": 4.64
    },
    {
        "text": "architecture anymore there are little",
        "start": 1168.0,
        "duration": 4.16
    },
    {
        "text": "ones coming up but if we talk about the",
        "start": 1170.4,
        "duration": 3.6
    },
    {
        "text": "state-of-the-art it's pretty much still",
        "start": 1172.16,
        "duration": 4.879
    },
    {
        "text": "the the transformer architecture auto",
        "start": 1174.0,
        "duration": 4.96
    },
    {
        "text": "regressive derived from GPT2",
        "start": 1177.039,
        "duration": 4.161
    },
    {
        "text": "essentially",
        "start": 1178.96,
        "duration": 2.24
    }
]