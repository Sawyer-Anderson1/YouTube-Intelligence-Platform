[
    {
        "text": " Welcome to Zcass everyone. I'm Zas Caraval from CK Research and here in Austin, Texas at Amazon's AI chip labs. I'm with Ron Diamont who VP and distinguished engineer for Amazon and we're going to talk about AI and chips and all kinds of exciting stuff and who knew Amazon actually did chips, right? but before we get into that, maybe just a a quick bio on yourself, a little introduction what you do for Amazon. Sure. So my name is Ron. I'm the chief architect of Trinium, our machine learning chips for accelerating AI workloads here at AWS. And we at Amazon have been building chips for quite a while now. We've been building chips for networking and data center operations. Graviton which are is our data center CPU and of course tranium our machine learning accelerator. Yeah. And so I want to talk about the the chips and there's these are specifically for AI, right? , but before we get into that, maybe just a quick just thoughts from you on what what's where are we with AI right now? It's like it it there's a lot of hype. I saw this MIT study that said like lots and lots of companies fail with their projects, but yet everybody I talked to is moving forward with it. So in that hype cycle, where are we? And when you think back in your career to other technologies that maybe had a hype cycle, how's this different? So if I zoom out for a second before starting to talk about hype or no hype and when I look at the AWS usage that we're seeing from customers, we're seeing an exponential increase in the amount of AI workloads that are running on AWS both on the training side and on the inference side. On the inference side, it's very clear we're seeing more and more benefits of AI algorithms across a variety of applications these days and that's what's driving the demand. And on the training side, we're seeing that the more compute we invest into the training cycle, the more intelligence we get on the other side. So seeing this exponential increasing compute requirement, we're incentivized and and and almost forced to go and provide our customers with very optimized solutions to run their their AI workloads. Okay. So while there may be some hype, it's real. people are doing it and it just seems like when I talk to customers they can't get enough AI. Correct. Yes. Yeah. Yeah. Now let's talk tranium specifically. you know when I think about the world of chips that we live in today there's so many silicon manufacturers today right what why did Amazon head down the path that they felt they had to build their own chip and and then so explain that and then what trainium exactly is so at Amazon we always think about or we always prefer providing our customers with a lot of choice. we definitely",
        "start": 5.6,
        "duration": 326.558
    },
    {
        "text": "why did Amazon head down the path that they felt they had to build their own chip and and then so explain that and then what trainium exactly is so at Amazon we always think about or we always prefer providing our customers with a lot of choice. we definitely our customers including GPUs and and a few other options but looking at the market we we're compelled to provide our own solutions as well as we're convinced that we can provide better price performance from for our customers across a wide variety of training and inference workloads. That's what triggered us and motivated us to invest a significant engineering effort into building our own chips. And right now we're seeing that many customers are using these chips in order to build state-of-the-art models. We partnered with Antropic who are building their state-of-the-art models on top of Trrenium with a Rineer cluster a cluster of about half a million devices training to devices that they use to train and serve their models. And even internally when we serve state-of-the-art models through Amazon Bedrock, more than half of the traffic that is coming in is served via the tranium chips. Okay. And where are we in the tranium life cycle right now? I think we're on the second version with third coming. This is actually our third AI accelerator. We call it trrenium 2. The first generation with inferentia, second generation with trrenium one, and the third one now is trrenium 2. We already pre-announced trrenium 3 that is coming soon, but it's not available for use just yet. Now, , inferentia and trrenium still both exist. And what's the difference between the two? Inferrenia was more optimized for a certain set of inference workloads, specifically non-memory heavy workloads. Trrenium can serve both inference and training workloads. And as of today, Trrenium 2 is highly optimized for both training and inference for large language models. Now we had we did you had a little press event here today and I thought one of the more interesting slides was when you talked about how cranium isn't a CPU it's not a GPU right in the classical sense where if you think of a CPU being a single threaded processor and for all for lay in layman's terms a GPU is a parallel processor but tranium actually works differently than both of those right and so correct so talk about that design can you maybe double click on that and how that's different than what we think of as a GPU in the way they work today. Yeah, definitely. So, first of all, when we started building Trinium, we inspected the workload that we're going to accelerate, specifically machine learning workloads and neuronet networks in particular, and we kind of took a a clean slate. We didn't want to align oursel to one of the existing architectures, but rather said, how can we best support this workload? We ended",
        "start": 168.319,
        "duration": 633.1189999999996
    },
    {
        "text": "inspected the workload that we're going to accelerate, specifically machine learning workloads and neuronet networks in particular, and we kind of took a a clean slate. We didn't want to align oursel to one of the existing architectures, but rather said, how can we best support this workload? We ended support large linear algebra workloads which are the the workhorse behind neural networks would be with an architecture that is called a systolic array. That's an architecture where you arrange a grid of processing elements in a two-dimensional array and only the edge of the array is interacting with memory and then passing the the data to the other processing elements. In that way we can achieve much superior power efficiency and of course when we when we achieve this power efficiency it drives our cost structure down and we get to pass this cost savings to our customers. Yeah and actually I find a lot of Amazon innovation is always focused on driving the customers cost down. Absolutely. you know, compared to what what's out there. And so, , you know, given that tends to be, you know, the singular focus for all of Amazon, , maybe, , can you give us some examples of customers that are on tranium and some of the results that they've seen? Yes, definitely. So, one of the the our our major partners is Entropic. They're building a model that is called Claude. They recently launched Cloud Sonnet 3.5. It's the best coding model in the industry right now and it's trained and served meaning running inference on trrenium 2. They were able to significantly improve their cost structure and as of today they're running the majority of their first party serving on trrenium. On top of that they're also serving their model via Amazon bedrock which is our AI serving service. and as I mentioned before the majority of bedroom traffic also runs on training too. Yeah. And so is it only for AM model providers or do general enterprises use this as well? I mean every every person building a machine learning model has an opportunity to drive performance up and cost down by leveraging training tool. Okay. Now and so almost certainly customers will see a pretty significant cost savings right and it's worker dependent. I want to make sure that people understand that. H so many machine learning workloads will achieve state-of-the-art performance and price performance on Trinium, but every workload behaves a little different and that's why at Amazon we we offer a variety of different hardware infrastructure platforms. So you get to choose and pick the most suitable infrastructure for you. Yeah. And what I find really interesting too is even within the compute architecture, right? You've designed the networking technology that allows you to scale up and scale across and and you'd mentioned project reineer which is really about scale across right and and so help the audience understand what reineer is and",
        "start": 323.6,
        "duration": 956.3989999999995
    },
    {
        "text": "I find really interesting too is even within the compute architecture, right? You've designed the networking technology that allows you to scale up and scale across and and you'd mentioned project reineer which is really about scale across right and and so help the audience understand what reineer is and Definitely. So, and announced at last year's reinvent, right? Correct. So, we announced project reer in last year's reinvent. It's a massive AI cluster that we built based on trrenium 2. It has more than half a million devices. And our partner Antropic is using that cluster to train their state-of-the-art models and they're also using that cluster in order to serve inference requests from their customers and directly provide responses. ML MLbased responses to them. There's a real challenge. I I I often jokingly say that many people can build a supercomputer, a single one, but there's only a few in the industry that can build a supercomputer that can scale for about a million devices. And that's what we're doing with project paneer. We're using Amazon's expertise in building large data centers and scaling them very rapidly and very reliably to provide state-of-the-art compute performance for you. Yeah. So, this will be a massive unit of compute, if you will, a single unit that spans multiple geographic locations. Correct. Which is fascinating. It is right. And so, when you think about the implications of that, there's I mean there's so many things you can do. Now, one of the things I like about Amazon's model is you mentioned Bedrock, which gives you choice of models. You also support every processor out there, right? And so, in fact, I think there's more Nvidia workloads running in Amazon than anywhere else. And so, if I'm a customer out there and I see the choice of AMD, Intel, Tranium, you know, you know, Nvidia, how do I navigate that and how do I know what's best for me? I think the best way to do and what we encourage our customers to do is to benchmark. So try the Nvidia platform, try the x86 platform, try graviton and tryum. We're quite confident for that for a wide variety of workloads. We will be able to provide you with the best price price performance out there. But there is no there is no way to replace hands-on experience and benchmarking for yourself. Okay? And of course that's easy to do in the Amazon cloud because you can just spin up those instances and try it out. Right. So I did want to ask you about Rufus which is the AI shopping assistant that runs in Amazon.com and so I'm I think everybody watching this probably uses amazon.com and has seen Rufus but that actually runs on Trrenium and Infrenia. Right. Right. It's right. And so what kind of results have you seen like how much Roffus have you seen and what kind of results have you seen there?",
        "start": 489.199,
        "duration": 1269.6000000000006
    },
    {
        "text": "I'm I think everybody watching this probably uses amazon.com and has seen Rufus but that actually runs on Trrenium and Infrenia. Right. Right. It's right. And so what kind of results have you seen like how much Roffus have you seen and what kind of results have you seen there? Rufus team quite significantly. They're they're doing something pretty unique. They're running on a mixed cluster of some inferential devices and some training devices. And why did they do that? Because they wanted a lot of capacity for for Prime. They they wanted to run on more than 100,000 devices and they just got got every chip that they could get their hands on. and anyways, they they got 70% cost savings by moving from another hardware platform into the inferential training platform. All right. Well, thanks for the update on Rufus. , I did want to come back to the acquisition of Apanorma Labs. , from what I understand, it's year 10. Happy birthday. Yeah. 10. It turns 10 within Amazon. , you know, it is an Israeli based company and I'm just curious as to , you know, with so much geopolitical stuff going on in the supply chain. , where's the manufacturing done? , you know, how much is done in the US versus overseas, things like that. Yeah. So, so it is our 10 10th anniversary within Amazon. We actually had a visit from Andy Jesse and Matt Garmin in Israel with the the to visit the core and Auna team which the team appreciated quite a lot. with regards to where development is done, we own the entire manufacturing process design silicon design and manufacturing which allows us to provide better cost structure for our customers. And as for the AI chip itself, tranium and inferrenial chips themselves, we build them entirely out of the US. So they're specifically not built in Israel. Yeah. More than half of the development is done here in Austin where we are right now. And some of it is done in the Bay Area as well. other product lines are are more distributed across the world with some development happening out of Israel as well. Yeah. And I think when everybody thinks about Amazon, they know at either AWS or Amazon.com and you guys build a lot of chips though, right? Can you do you have any kind of metric that helps understand like exactly how many, you know, chips have been, you know, produced by you? Yeah, sure. So, so first of all, I kind of jokingly say that we're the biggest semiconductor company that no one ever heard of because we're kind of tucked in AWS, but we're one of the the top five customers for TSMC for advanced process nodes, five and three nanometer. We're doing things a little differently than others. We we really value speed in order to get to scale. And because of that, out of the last 12 chips that",
        "start": 647.2,
        "duration": 1561.6800000000003
    },
    {
        "text": "AWS, but we're one of the the top five customers for TSMC for advanced process nodes, five and three nanometer. We're doing things a little differently than others. We we really value speed in order to get to scale. And because of that, out of the last 12 chips that more than 1 million units in the very first what we call a zero silicon. And that's quite unique in the semiconductor industry. Yeah. All right. well, that's probably a good place to wrap up. Just last question without getting into anything that's going to get you in trouble from an NDA perspective, we got reinvent coming up. what's next for you guys? What are you thinking about? So, so we are definitely going to announce new products in pre-invent. You'll see announcement across all three product lines that we discussed. Nitro, graviton, and trrenium. And that's no secret because there has been every year for the last for the foreseeable that I can remember. Yeah. And what I can say and I don't think I'm spilling much that they'll be faster, they'll be more cost efficient and there's going to be a lot of exciting innovation that we we achieved in the last year. And with all the stuff going on with AI, you need more speed. We need to bring cost down and we need to bring power utilization down. And it seems like you're tackling all those 100%. Yeah. All right. Anything else you want to add? Nothing else. Come come and try training month. Yeah. Okay. And all right. So on that note, on behalf of Ron Demon, I'm Zo Valor from ZK Richards and thanks for watching. give us a like and also hit that subscribe button and I'll see you next time on my next episode of ZCAST. [music]",
        "start": 794.16,
        "duration": 1711.1090000000006
    }
]