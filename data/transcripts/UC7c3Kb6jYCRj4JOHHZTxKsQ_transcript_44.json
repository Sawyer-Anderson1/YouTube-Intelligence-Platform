[
    {
        "text": " Here's my interview with two of the leaders of the Genie 3 team. Genie3 came out just a couple weeks ago and it is an incredible text to fully 3D controllable world and it is highly accurate and has so much potential for video games, agent training, world simulation, and so much more. So, in my interview, we talk about what went into training Genie 3, what their plans are, the future of world models, and of course, we talk about simulation theory. Here's the interview. All right, so thank you, Jack Schlomi, for joining me. Jack Parker Holder is a research scientist at DeepMind. Schlommy Fer, am I saying that correctly? Yeah, close. Okay, research director at DeepMind. Thank you guys so much for joining me. , I'm really excited to talk about Genie today. Awesome. Thanks for having us. Yeah, absolutely. So, okay, when I first saw Genie, I was pretty blown away. I'd never I've never really seen fully controllable textto world models before. First is what is the long-term goal for Genie? Is it more of a research tool for agents? Is it for creators? Is it for general simulations? Like what is your overarching goal? Yeah. So the way I'm thinking about is the capability to generate the world just from text. it is kind like a fundamental capability that can be used for many things basically everything that you've mentioned. in a way if we can create a realistic world that is you know you can interact with and you can walk around and maybe it simulates all of the physical phenomena that we have in our own world. Then based on that you can use it for training agents for reasoning about what might happen if an agent takes an action in the world for entertainment you know very long list of applications. So basically the answer is pretty much everything you can think about. Yeah. Okay. All of the above. Is that when you first started this the Genie family of models was that the intention or was it more of just hey let's explore this research area and see what's possible? yeah so I'd say when Genie models research first started roughly 3 years ago it was very much focused on the AGI and agent centric angle. So a few of us had been working on like automatic curriculum learning for agents with reinforcement learning. So agents that can learn from their own experience in simulation. , and we basically been trying to find different environments where we could generate diverse range of different kind of worlds and tasks, typically procedure generated worlds, right? And we basically got to the point where we couldn't really design or or like handcode an environment that was rich enough that you could basically even if you sold everything in it, it would get an agent that was able to transfer and do diverse things in the real world. And so there were multiple",
        "start": 0.08,
        "duration": 369.0400000000001
    },
    {
        "text": "point where we couldn't really design or or like handcode an environment that was rich enough that you could basically even if you sold everything in it, it would get an agent that was able to transfer and do diverse things in the real world. And so there were multiple was maybe to keep building richer simulations. One was to maybe start using real real video games because they're quite can quite realistic. but at the time sort of language models had really emerged as something that were becoming quite useful and quite effective. Text image models were just starting to be pretty pretty good. And so it kind of seemed like a it would take a bit of a bit of bit longer, but like maybe if we could do full world generation that would basically solve the environment problem for agents and then we would get like much much richer agents than we would do any other any other way. So it actually seems like the fastest way to get general agents was to not work on them but to work on the environment model first. so that was the original motivation. I think it's it's not the case that that interest has gone away. I become increasingly convinced and as a team I think we're all pretty convinced that this is like a really exciting direction but it's just sometimes when you pursue interesting research you don't know necessarily what it's going to be used for right and there's been a bunch of other things that have emerged like I I I wasn't as in the know with sort of the interactive human like use cases but those have become like pretty obviously the case in the last year or so right so and actually even with with Genie free. There's been a bunch of use cases that we would weren't even really thinking about that people have asked us about that sound pretty exciting as well. So, I think it's one of those ones when you do something new, you often get some unintended consequences that could even be more interesting than your original intent. So, so, so my understanding is to scale up to reach AGI, we need some kind of reinforcement learning feedback loop that doesn't have maybe a human in the loop limiter to it. And and that's really where Genie 3 and the Genie family of models comes into play. It generates these environments. Agents can go into the environments and explore and learn what kind of signals is the environment giving to the agent because you know, forgive my my lack of understanding here, but the the output is is still maybe a black box or how it's generated as a black box. So what kind of signals are is the agent actually able to retrieve from these worlds? So today you know basically we're operating only in the visual domain right. So the output of the world model",
        "start": 185.04,
        "duration": 659.358
    },
    {
        "text": "is still maybe a black box or how it's generated as a black box. So what kind of signals are is the agent actually able to retrieve from these worlds? So today you know basically we're operating only in the visual domain right. So the output of the world model right. So the focus on on visual outputs was just out of because there is a lot of progress in this space right we see video models that really made a lot of progress in realism. So we have models such as Velo and and others that are able to take text and just output something that looks very realistic. and and we we thought that we can push on what's possible in terms of the ability to actually walk around in like walk into a video and actually explore what's what's the world that's generated, right? so I think this is still limitation when it comes to maybe training agents but we can go pretty far just by using visual visuals because for example by looking at the world we can know if like how fast something's moving. We can an agent can understand if it can if it has to maybe get from one point to another what are the obstacles in its way. So even without you know more kind like physical responses that you might expect for robots we can still do a lot like start exploring the capabilities of of such environments and we're very excited about that as kind of the first modality from many that can be simulated in a realistic way. Okay. And and just to clarify agents when they are training within the environment of a G3 world they are simply looking at it. It's through the pixels presented to what we would see on the screen. Yeah. Maybe to clarify a little bit. So I I I think right now we are posing G3 as one one of the potential application is to train agents in it. Right. We are this is something we're just starting to explore. and we built on on a very line very long line of work by Deep Mind and other others that used simulations to train agents. Right. So we can you know starting from Starcraft and Go and Alpha Go and then and then basically using some game simulation or environment to train an agent to solve the game, right? and that's that's kind like the initial what what kind of like one of the h seeds that led to to investments in simulation for for Google deep mind and now we're trying to build simulations that can you said before like taking the the the like removing a human in the loop but I think it's more about removing the the need to actually go out into the real world to train an agent right because it's very expensive to deploy for example a robot in in in some like in a",
        "start": 331.039,
        "duration": 980.3990000000002
    },
    {
        "text": "the the the like removing a human in the loop but I think it's more about removing the the need to actually go out into the real world to train an agent right because it's very expensive to deploy for example a robot in in in some like in a to train a robot to to handle a new environment that it was never able to explore. One way is just to put it in this environment and it will make mistakes, right? But maybe it will be much cheaper and much more safer if it can still first experiment learn in in this simulated environment before it gets deployed into the real environment. So that's kind of like in a nutshell what we're having in mind. Okay. And and Jack, you mentioned you were a bit surprised at the use case of of the human interaction, right? First of all, are you are you guys gamers? Do you play video games? I used to play a lot of strategy games actually in the past like few my my background is mostly in programming 3D engines. That's kind of where so it was mostly a a hobby to develop the games, but today I'm less of a gamer. Yeah. Okay. Yeah. as a kid I played a lot of that especially the driving games but honestly it's been quite a while so that's and I guess this is quite different to gaming like it's kind of a new it's a new type of thing like it's interactive world generation rather than there being like a hours long like I remember playing games for hours right like exploring all of the like side quests and all of things and like clearly our models can't do that so it's not obviously a drop in for those things but I think it it's surprising that how kind fun and engaging it is to play with it already. , so and I think we've seen that reaction from quite a lot of people that they just have fun like interacting with it. , and that obviously when you're so focused on the work, it's hard to understand how much someone else who doesn't get the context if they get put it in their hands, how compelling they'll find it. And we have had like pretty nice feedback. People do find it quite like interesting to to play with. I think that's actually something quite exciting as well. but yeah, and and and Jack, I mean it's it's not a drop in replacement for video games today, right? So let's say you know a a standard video game, 20 hours of gameplay, lots of different levels, a lot of rules, not just an open world typically, but there is at least you're starting to see hints, especially with with Genie, where it could eventually evolve into that. it could grow into that. Do you is is there kind of a clear",
        "start": 495.599,
        "duration": 1260.8799999999985
    },
    {
        "text": "gameplay, lots of different levels, a lot of rules, not just an open world typically, but there is at least you're starting to see hints, especially with with Genie, where it could eventually evolve into that. it could grow into that. Do you is is there kind of a clear it's going to maybe not replace but supplement traditional games and is is that something that you're excited about? Yeah. So, I think on this one like I wouldn't say we're really considering it as a path to replace games in terms of being an additional an additional tool for sort of prototyping things. I think that's kind of where it's already at, right? Right. So, I think if you're someone who's like, \"Oh, imagine if I could, you know, what were some of the examples? I could be an origami lizard, with a kind of platformer style, then instead of spending some time to create that, you could have it in 10 seconds, you know?\" and like that's pretty pretty cool, right? Because you can immediately see things. , we had someone in our like initial cohort of testers, , Julian Tagalas, who, , I think is far more of an expert to me on AI and gaming, and he wrote a really nice blog post, and he said like from his time playing with it, and he had a great time, , afternoon playing with it, , is it's kind of already there for prototyping, like you have an idea of a random thing that like you maybe wouldn't want to invest much time in, and straight away you can try and interact with it. , now I think to really like narrow it down almost to being sort of looking at existing existing games and think it's trying to target towards that. I think it's like maybe maybe one path that but for us I think we're most interested in the more general capabilities of the model and there's quite a few new things that we can try and do with it. I think yeah I'll just I'll just add that you know we simulating games or like specifically requests much more than than genius capable today. I I think if you look at the game as you said there is logic there is maybe a plot there is some so what we have today is basically a a sim a word simulator that can take text and just let you walk around and interact with with the environment right it's definitely a potentially a comp a component that can later on be used to build various experiences right it can be a game it can be maybe exploring some you know in an educational setting. So I think at this point it's still very early to know exactly how people will be using it. and we actually have some collaborations with we've enter we've been in the entertainment space with with various",
        "start": 638.16,
        "duration": 1538.2399999999989
    },
    {
        "text": "game it can be maybe exploring some you know in an educational setting. So I think at this point it's still very early to know exactly how people will be using it. and we actually have some collaborations with we've enter we've been in the entertainment space with with various also hearing from researchers as as Jack mentioned that give us some kind of like a feedback of what they think such systems can be useful for. I think for specifically for simulating maybe environments that are engaging I think there is like there is a lot of like space to explore how we can use those systems for that. for example for making it more interesting as you walk around you can you but but I think the key is that we can use it already for things that you cannot achieve it in in any other way. And for me this is the most exciting thing. Not just to try and do something that's already existing, right? But creating in a way a new media, a new experience, for example, something that is not exactly a film and it's not exactly a game, something that you can maybe do only using generative models. That's a slightly better way of saying what I was hoping to get at. So I'm glad I'm glad that's why it's good to have two of us. Just everything that the stream just said also applies to agents as well, right? Because what you were basically saying before is is actually kind of a similar question. It's like what's the the goal of the agents like how what's the feedback they get like is there any sort of like progression metric or these kind of things is kind of similar to what a human might want as well. And then also the richness of the worlds for as a as a limitation right now the richness of the worlds for humans is also a critical thing that if we want agents to learn about human interaction and these kind of things in the model then that's another area that we need to improve. So I think the nice thing is by building this new thing we actually make it more interesting for these use cases but I don't think we're like targeting one like we're kind of trying to just build the new thing. Yeah. and and Shalomi the as you were explaining it I realized I kind of fell into the trap that a lot of people do when a new technology comes about they try to fit it within kind of the the framework of of an existing piece of technology so you know when the internet first came about it was like okay put the magazines on the internet it's like well actually there's probably a lot more and and novel things we could be doing with it and so I I certainly fell into that trap of thinking okay",
        "start": 779.44,
        "duration": 1797.5979999999986
    },
    {
        "text": "so you know when the internet first came about it was like okay put the magazines on the internet it's like well actually there's probably a lot more and and novel things we could be doing with it and so I I certainly fell into that trap of thinking okay do with video but you're right like maybe there's something completely completely different, completely new that we haven't even thought of yet. And that's super exciting. , I want to I want to talk about the capabilities for a moment. So, , Genie 2 to Genie 3 was a pretty big pretty big leap. , and, , correct these numbers. I have them written down. So, Genie 3 24 frames per second, 720p consistency for a few minutes. , what hardware latency, budget, batching tricks made that possible? whether kind of independently in Genie 3 or as compared to Genie2. So a lot of you know I think overall what we try to do with Genie 3 is to push all of the dimensions of of you know the the system to to the next level. So you know I tried to think about you you can think about the resolution the the memory and and along all of and how many actions you can do every second right so we have a few actions that you can you can so every all of those dimension if you multiply them it's like so almost a 100x kind of improvement and and this is you know this is in a way one way to think about the sleep right so like to quantify it a little bit And I think the lot very key kind of key element in our research was to try and balance in one way the quality and the latency. This is something that we worked on a lot because because low latency is a challenge right as you said you know there hardware we are meeting hardware limits that we have to work with and really leverage a lot of the kind like best-in-class hardware and and kind like architectures that Google has across the different models and we invest a lot in trying like this is some even like building on multiple years of of learning what makes models moreffic efficient. So I I you know without going into all of these details I can just say that this allowed like leveraging all of this work that happens across different modalities really made a difference in terms of the efficiency of the system. Okay. , and then like by the way, I saw there's like a demo that you guys released and it was of a jet ski at night on on this I don't know like a a huge river and you see this jet ski bumping into things and the physics look incredible. I remember pointing out in one of my videos where I covered Genie",
        "start": 912.56,
        "duration": 2100.478
    },
    {
        "text": "you guys released and it was of a jet ski at night on on this I don't know like a a huge river and you see this jet ski bumping into things and the physics look incredible. I remember pointing out in one of my videos where I covered Genie lights and the lights clearly moved out of the way as the jet ski was going past it. And I it was such like a minor thing, but it just made the entire video and and worlds look so real. When you first saw things like that, like what did you think? What was your initial reaction? I It's Yeah, I mean probably similar to yours. Like I think we obviously set these goals for ourselves that were pretty ambitious, right? Like coming after Genie2 and and V2 at the beginning of the year. Shomi obviously was co-leading V2 and myself on on Genie2 and we were talking about what we thought thought was possible and we kind of had this sense that a model like this would be achievable but it it's still like quite a like it was quite a surprising thing when you actually end up achieving it, right? because obviously during the project you have ups many ups and downs right research is not a straight line so you get to the end and you're like kind of you're happy it's like looking pretty good right and then suddenly you keep seeing these new things and you're like oh actually this is really good and some of the things you see like cuz obviously other people play with it too right and you almost kind of kind of forget it's your own work sometimes because you see someone create something like for me the painting one was one of those yeah where I just like found it really hard to believe that the model did it like like really cuz obviously like it's we're going for quite a general thing right we're really seeing it's like a new kind of foundation model rather than targeting one application so it has a lot of abilities that are we we overly use this word emergent but it has a lot of abilities that we didn't directly train for that it's pretty amazing that it can do right so I think we were kind of almost every day there was a couple of weeks where someone did something really cool with it that we didn't know it could do. There's a bunch of others as well, but we could talk about those maybe later. sure. okay, you mentioned V3. Nano Banana came out just about a week ago. How much of the research efforts for V3 and potentially Nano Banana intersected with what you were doing with Gen3? Is it like some parts of it are incorporated into it? Was it just you kind of took learnings from it and like Yeah. How do you how do you think",
        "start": 1064.4,
        "duration": 2379.196999999999
    },
    {
        "text": "much of the research efforts for V3 and potentially Nano Banana intersected with what you were doing with Gen3? Is it like some parts of it are incorporated into it? Was it just you kind of took learnings from it and like Yeah. How do you how do you think coming together? Yeah. So the the story, you know, it goes back a little bit to to the story of, you know, how Jack and I kind of started working together. so I was mostly coming from Conver and Jack from the you know really awesome line of work for for Genie. And at some point we were just we just realized that you know like that it's it's just it could make go very far if we kind of like combine the learnings that we have across the board and you know the teams you know obviously we're not very far physically and and you know in in in organizationally and we thought it's really something that we can try and and combine and try to push to the next level. so in a way I think there are a lot of similarities between video generation and word simulation. There are also some differences. in in a way it was challenging to try and understand what should be similar, what should be different, what we should try and explore different approaches that we than we used for veil for example. and and yeah so definitely there there are similarities. , specifically you mentioned Nano Banana and which I I I I feel like the team is just it's just a trick to get us say this this ridiculous name, right? But but yeah, it did catch up. , so there are definitely a lot of similarities at the fundamental levels, but it's a different product after all. but I think what unifies all of those you know maybe systems or models is the ability to just take something that is very sparse like some text from some from from the like the the user and build something that is so so elaborate and creative and if it's an image if it's if it's a video if it's a world that you can walk around all of those things are really very creative and in a way to me it's it's every time I see it I'm very pretty much amazed sometimes like as just like I'm like forgetting that you know forget all of the work that we've put into it and just kind like surprised by it because there is something very surprising about the fact that a system like a model is able to generate something that is so spectacular that any other way to generate that would require so much work right I used to work on 3D engines as I mentioned and it it's so much work just to get the lens flare to to look re realistic, right? And now it just works,",
        "start": 1207.52,
        "duration": 2693.4389999999994
    },
    {
        "text": "that is so spectacular that any other way to generate that would require so much work right I used to work on 3D engines as I mentioned and it it's so much work just to get the lens flare to to look re realistic, right? And now it just works, So, yeah. Yeah. and and I I want to talk about Google's custom hardware with the TPU. How how integral was having custom silicon like the TPU to the training and the actual inference running Genie 3? So I think I mentioned it before and and every you know all of the models that we develop are basically running ultimately on on Google's TPUs. and and I think having this good synchron synchronization and alignment between the hardware and the software gave Google like a really good advantage and and you know we're basically able to optimize across the stack. So that's the very good and then I think it also translates to to other to the ability to ship models fast but also you know make this hardware available to others and potentially let allowing them to build their models to run on TPUs. So yeah I think I think you know GPUs and TPUs ultimately try to do the same thing. I think that if know there are a lot of I won't go into all of the technical differences but definitely having our own hardware and the teams that build it and and that's that's something that opens a lot of optimizations directions and as you're training this model like if you're training a textbased model you have certain benchmarks that you can compare it against but like with a world model what kind of benchmarks would you be testing against? How do you know during the training process if things are working well? If if the product is baking and it's going to come out how you think it is? And then finally, when it is actually delivered, what kind of tests are you running? Is it all just vibe testing? , yeah, I think it's probably, as you would imagine, it's really a mixture of many things. , I think it's not just it's not just the comparison of text versus world model. It's probably also comparison of an established field where there are just existing benchmarks versus something that's kind of new, right? So, it's not necessarily obvious what you should expect when you're doing something completely new. Obviously, we we we worked very hard on trying to get some like quantitative things we can trust. and I think we've done have some pretty good ideas here and like we've been able to use some stuff but ultimately different capabilities of the model will require different metrics and so you you really have to like all as a group be sort of discussing the pros and cons of some things and like looking at like you can't necessarily have one there's",
        "start": 1365.84,
        "duration": 3018.159
    },
    {
        "text": "able to use some stuff but ultimately different capabilities of the model will require different metrics and so you you really have to like all as a group be sort of discussing the pros and cons of some things and like looking at like you can't necessarily have one there's necessarily climb as easily as you can with I don't know like a chess ELO or something like that you know It's definitely a complicated picture, but I think in the end we managed to make make sense of it. So, it's also really depends on how we use it. I I think the the more it's the more early the research is and and you know in this case as suggested we're not trying to necessarily improve an existing capability but we also added new capabilities. So for these we had some internal benchmarks but we definitely know it doesn't mean that those are the best that's possible or like you know some some some golden stand gold standard right so unlike you know maybe I think even for but I would say that even for text models right LLMs and and video models image models it takes quite a lot of time it took quite a lot of time for you just recently video models became maybe first class citizens in all of you know in LML arena for example right it takes a lot of time because there is no it's it boils down to human preference all of these models eventually yeah you can measure on some some task but people care about how they use it and I think for because it's still early in in the in the kind of in for world simulators to be interfacing with with users and that's so it's a bit hard for us to define the metrics so we're going to do it we'll do it probably as we as we go and as more people can use it we will have better understanding of how they're going to use it and we'll be able to create metrics that will be more useful to guide further development. Okay. Can you can you envision even if it's not possible today a a more kind of direct benchmark that isn't just based on user preference. I think predicting the future is a good one, right? Like in in a way predicting the future few seconds into the future, right? This is a bit of a typical benchmark, right? But if you can predict the future of like given so that's something that we also you can also use for video models, right? if you can wait you have to break that down a little bit. Predicting the future. Please please explain what you mean by that. Sure. Sure. Sure. Of course. So when I think about predicting the future, I'm not predict I'm not thinking about predicting the stock market tomorrow. Although that's going to be useful.",
        "start": 1530.559,
        "duration": 3300.639
    },
    {
        "text": "have to break that down a little bit. Predicting the future. Please please explain what you mean by that. Sure. Sure. Sure. Of course. So when I think about predicting the future, I'm not predict I'm not thinking about predicting the stock market tomorrow. Although that's going to be useful. predicting maybe imagine that you have a ball a ball on on the floor and somebody is kicking it right now. You can just stop at this second and just let the model guess what's going to happen next. Right? So as a person you can imagine that the ball is probably depending on the angle of of the of the kick maybe you can imagine where it's going to go and land and a good model that can simulate an environment will probably be able to predict the trajectory right and this can be a really like a bit it can be just a fil video that you're taking in the real world and you can actually compare what happens next. So those benchmarks exist and and they're useful for for basically understanding how well the the model is able to predict the the the next set of of frames. Right? So that's one one way to consider it. But it doesn't cover a very important aspect of Ford's models is that it doesn't get the input from the user or agent. Right? So if you want to walk around and or interact with that's where we are hitting usually challenges in evaluation because there is no obvious way to to evaluate it. I guess another interesting angle which we've been talking about a little bit is like obviously we designed the environment to be useful for training and evaluating agents but then we can use the agents to evaluate the environments too, right? So can the agents achieve goals in the world, right? If they can then maybe the worlds are sufficiently consistent to to like enable enable that that means that must be a good thing. You can also say to the agents things like, you know, pick up a feather and drop it on the floor and does it kind of like float down. You can say like like throw the ball. You can say put mentos in Coca-Cola and see what happens, you know, like and you can maybe then say like if you had if you had an intelligent agent, you could even ask it like did the world behave how you thought it would or something like that. , so I think this really like sort of twoway thing of like agent environment being a duality and like both kind of depending on each other and evaluating each other. , I think is something that could be really powerful. , and it kind of at a meta level goes back to your previous kind of questions like does it benefit you from having these other teams like Nano Banana? Does it",
        "start": 1673.76,
        "duration": 3586.319
    },
    {
        "text": "depending on each other and evaluating each other. , I think is something that could be really powerful. , and it kind of at a meta level goes back to your previous kind of questions like does it benefit you from having these other teams like Nano Banana? Does it benefit from you having agents to evaluate? I think that's the really cool thing about building a model like this at GDM is without being too much of a of a Kool-Aid person, but like having all these amazing teams with other things that can integrate with the model, like clearly we benefit from all of this. , so I think it's pretty exciting. Yeah. And okay, Schlom, I have to go back to the predicting a few seconds ahead. I'm sure you've had this conversation internally, right? Immediately when I hear that, I think, okay, let's talk simulation theory. yeah, like it just if we if a world model can sufficiently predict the future like isn't isn't that proof of simulation theory at that point. So the okay you know it it depends how how deep you want to go but you know predicting the yeah yeah the future I think the f future given is not that I don't I think we we know that the future is not necessarily deterministic right based on on our understanding of physics so not necessarily there is a single like single trajectory but at the macro level if you sufficient understanding and knowledge of of like the trajectory of the ball right so kind like Newtonian mechanics can give you the the the right like if you know everything if you know the mass etc you can actually predict so I think it depends on the phenomena right and and the time horizon already today there are simulators that simulate physical environments and that's you know if you if you play a game you typically games have some kind of a physics engine that just you know computes some some the force and maybe velocity and gets you some some prediction. So I don't think the term of predicting like some some physical phenomena necessarily is very powerful on its own. I think the the the power of world simulators that are maybe learned is that they're very diverse. So you can do things that are that go much beyond a specific formula such as a ball you know going in one direction or the other. It can be I think for example fluid simulation is something that is often being mentioned as as a very complex phenomena. you know there their equations never stop that kind of like govern the behavior of of fluids and we can see that sometimes our the models video models and genie are able to simulate fluids and or in a very sophisticated and realistic way like I think we have this kind of like ocean kind of like someone got walking around",
        "start": 1819.36,
        "duration": 3923.28
    },
    {
        "text": "govern the behavior of of fluids and we can see that sometimes our the models video models and genie are able to simulate fluids and or in a very sophisticated and realistic way like I think we have this kind of like ocean kind of like someone got walking around hurricane I think one of the samples that we have and that's just it looks very realistic Right. So if you want to create a simulator for that, it's going to be pretty hard. , so I think what's exciting is that if we have simulators that can do that and learn to simulate to simulate physical phenomenons for example such as fluid dynamics without doing any explicit calculations we don't you know we don't teach the model I don't know enough physics so I can't even if I want I can't teach them all to do right. So, so this is something that I it's in a way I don't think it necessarily leads to any conclusion such as simulation theory or anything but it just means that it is possible to simulate some aspects of the reality at some fidelity with maybe less compute than we would have thought before. whether we live in a simulation or not it's I'm happy to say what I think but just like it's a bit of a different question. I I would like to know what you think. so every time I get asked this question I'm like I'm making it more elaborate and and far-fetched. So let's see but my current thinking is first I don't think we can know because in a way you know I think there is like there is no actual way to answer this question for us even if we are if we do live in a simulation but I think one interesting thing is that physicist physics who test our reality and run experiments you know they observe some limit limits of of like you know quantum mechanics And if those are being able like if there is some machine that is able to simulate all of that I I think it's it's there is a question of how much compute is needed to actually being able to simulate that and and I think this is like if you apply it in some like I I think that's actually in a way counter the the argument right like because you need a very elaborate machine to to to to pro to make all of those simulations and then the next level of reality also requires that. So, you know, I I basically believe that we don't we do not live in a simulation. Yeah. It's interesting though because as you're describing what Genie 3 is capable of simulating fluid dynamics, for example, it's not actually calculating atom by atom. It's calculating enough to render something that is believable by the viewer. And so maybe it doesn't require such a",
        "start": 1989.76,
        "duration": 4216.801000000003
    },
    {
        "text": "live in a simulation. Yeah. It's interesting though because as you're describing what Genie 3 is capable of simulating fluid dynamics, for example, it's not actually calculating atom by atom. It's calculating enough to render something that is believable by the viewer. And so maybe it doesn't require such a calculations. But you can still I think this is the point that you can still run an experiment like so if you take the simulation you actually try and and and you walk like you will hit in a way the limits of the simulation right and I think and and it's not that hard to to see like that's my point like I think while maybe we can take video models as an example they're very realistic like when we see like some some of the videos are just like you know I worked on free and sometimes like you asked what we think about the outputs of Genie. So Genie and V like sometimes like I I can't believe this is generated like sometimes it's just you're truly mind-blowing. And I think the key is that it's still even though though it looks very realistic, you can still pretty easily prompt it in ways that will break, right? Like even the best models in the world at the moment can still break in some settings. And I think that's just the fact. And it does highlight that our simulations are still limited. I I there is a question how far we can fetch push it and and if at some point it will actually be as realistic as our our experiences. Yeah, I I have no answer for that. We went pretty far on the Yeah, I know. I'm going to I'm going to bring it back to Genie, I promise. But I just want to close. Jack, I want to get your thoughts. are we in a simulation? Do you think it's possible or or just a hard no against? I thought I'd avoided this one. I took it usually. yeah, you always take this one. , I actually don't have a strong answer. I'm going to I'm probably going to say no. , exclusively because I just feel like there's far too much going on and it's consistent. , it's so consistent everywhere. I feel like there would be at least one glitch somewhere and maybe you can identify some things that would have been that, but like having worked on these models like they're they're really good, right? But like to get the complexity of the real world and had pretty much zero things that like look obviously go slightly wrong. I'm like everyone makes a little mistake, you know. so so that's my only real thing I can see that this proves it. But ultimately I think Shane made the point before that we wouldn't really know, right? So yeah, it's maybe maybe the mistake is like a",
        "start": 2138.48,
        "duration": 4492.481000000003
    },
    {
        "text": "slightly wrong. I'm like everyone makes a little mistake, you know. so so that's my only real thing I can see that this proves it. But ultimately I think Shane made the point before that we wouldn't really know, right? So yeah, it's maybe maybe the mistake is like a remember that scene like that was no mistake in the simulation. Exactly though. So like even in the even in the most compelling movie about living in the simulation, there are some glitches, right? So I'm like where and and I haven't spent a great deal of time like researching times when people have been like had conspiracy theories about glitches in the world. Maybe there are some and like no and no one paid attention, but like I'm not aware of any. So all right. All right. I promise we'll get off the simulation topic now. I just it's it's hard once I start thinking about it I want to talk about it. No but we know we do have the joke that the next version of Genie will be developed in the simulation. So we we perfect. Yeah. Genie will develop Genie will develop and infinitely on from there. Okay. one of the most impressive things that I I hadn't even thought of when Genie 3 launched and I saw it was the ability to have promptable world events during inference. Is that correct? Is it during inference? How does that work? What does the actual user interface look like during that exchange? Because obviously from from the outside all we got was the demos, but like what does it actually look like interface wise? And then what did it take to accomplish promptable world events during inference? Yeah. So I think there's kind of two things here. So in terms of the user interface, we didn't really have one, right? So like Genie 3 is not like at this point a product, right? So it's still sort of a research preview. So in this case like the actual way that the text gets sent to the model for those things like it wasn't like there was some shiny widget or something like that or someone speaking into it or anything. It was more just like someone sent sent the commands to the to the model via like a research researcher worked on that. , but in terms of the mechanics of it, it it really is just like sort of the model's just receiving a text prompt like during the generation. So the frames before that prompt haven't seen it and the frames after have, right? So so interesting. essentially the model it's each frame is conditioned on an action but then sometimes it will be have the text. So essentially then it has this like causal nature, right? So like the event happens and then from that point onwards it can impact the world. Right? So if we if we think about this as an",
        "start": 2278.32,
        "duration": 4777.6820000000025
    },
    {
        "text": "conditioned on an action but then sometimes it will be have the text. So essentially then it has this like causal nature, right? So like the event happens and then from that point onwards it can impact the world. Right? So if we if we think about this as an it would be almost as if you can insert new text during each token that's being generated. Yeah. Yeah. Exactly. In a way in a way think of it like if a text generation model tries to predict the next token, right? So Genie is trying to predict the next frame based on a sequence of inputs and and here we're just allowing the user to say okay that's what's going to happen next. It doesn't say exactly when. So the model can interpret it in in some way. And and it's a very interesting if you think about it like it's very interesting because it doesn't say exactly how something should should be materialized in the world. And and it's you know it's it's actually quite an open like the model can decide for example if you ask it okay a camel comes into the view right or something like that then it does it is are you going to see it when you look to the left or just going to materialize like there is a bit of like it's underspecified so so I think this is part of the challenge is to figure out the right and that's why you know when we you mentioned evals or ways to evaluate models benchmarks I think we're still trying to figure out the interfaces even right like how what what what goes in what goes out to the model and this is one idea that we had that we really wanted to explore and I think you know we have some other ideas how we can how can you can integrate interact with this world right it's not obvious text like when you walk around you don't just like we interact physically with the world so I think text is in a way a proxy for that and and yeah and and so let's say you have this initial prompt and it generates the world and then during the generation of the world you give it a conflicting prompt. Let's say it's a simple example in the original prompt ball is red. In the subsequent kind of in in in in generation prompt you say ball is blue. How how does the model handle those conflicting instructions? my guess is that the model would try and make it happen. Right? So we didn't try that exact kind of thing where like events change change what was in in the world already. , we tried more things that were like adding to the world, like a a dragon appearing or like the person in the chicken costume for instance. But if you if you said now the river turns",
        "start": 2423.92,
        "duration": 5069.840999999999
    },
    {
        "text": "thing where like events change change what was in in the world already. , we tried more things that were like adding to the world, like a a dragon appearing or like the person in the chicken costume for instance. But if you if you said now the river turns already a bit brown. It was in London. , he said it said it turns crystal clear like the Caribbean. That would be nice. , I think it would just I think it would just do that, right? like it would just do what it like it would follow the text that it's given. now the question there's another cool example where obviously the model takes text as as the world description but actually you can also prompt it with a video as well right so you can prompt it with frames and there was an example that we posted on social media where someone in the team had gave it some frames of of like one world and then actually prompted with text that was not consistent with that. , and so we have evidence that the model what model does is try to make it work, right? So it it just like in the most natural way possible, it kind of blends these two worlds together, right? So you get examples like the one where I think it's in a room watching people playing actually with the genie model. I remember that. Yeah. And then and then the prompt is about like a jungle with dinosaurs and something like that and then you turn around and you're in the jungle, but then you look back and you see the room. So the model just kind of makes the text work in the world. And that's I think kind of what show me was getting to before is that like you don't tell it, oh by the way your your video frames may show a room, but blend it this way. It just kind of does it and makes it work. And like that's kind of the exciting magical part in a sense because it's sort of like creating something that you didn't really know what it was going to do exactly. , like even the one where it's like the dragon lands in the canal and like the way it just kind of plunks down and splashes the water everywhere. I think it's like when it actually happens, you kind of all like, well, that's really cool. I didn't know what it was going to do, but that's really cool. , so I think yeah, you're I think you're right to highlight that feature as like probably one of the most open-ended exciting parts of it. And and so for for what you can share, , what does the underlying training data look like? What format is it? Meaning is it is it just all video game simulations? Is it movies? Like kind",
        "start": 2571.359,
        "duration": 5320.960999999998
    },
    {
        "text": "feature as like probably one of the most open-ended exciting parts of it. And and so for for what you can share, , what does the underlying training data look like? What format is it? Meaning is it is it just all video game simulations? Is it movies? Like kind share. What does the training look like? What does that training data look like? Yeah. So we train on mostly on publicly available video data sets. and and that's pretty you know pretty much what we we can share at this point. but the model I think the the interesting thing is that the model is able to really learn from very diverse video models about the world. I think this is something that's a little you know maybe again a little similar to how LLM's learn from just web you know text on the web potentially or other sources to predict the next token and from that it can infer a lot about how the world works how people express themselves and how maybe even how to think and how to solve problems and I think in a way what we're seeing is that learning from kind of those you know sources of video data also allows the model to learn like representations that are very useful potentially for creating this interactive environments. Right? So I think I think this is a very surprising kind like result. There are a lot of people who say that text generation models aren't necessarily generalizing that far outside of their data set. Do you think that video or textto world models can generalize more effectively than text models? That's a really cool research question to be honest. I think it's quite hard to quantify that one. , but like intuitively, so I'm not actually super expert on language models, so probably don't want to talk too much about what they can't do or can do. But in our space, like I think it's certainly the case that a lot of things that the model can do already are things that don't I I don't think it's seen before. I think it's generally a new thing that it's creating. Like the dragon and the canal or even the origami lizard, I think is a pretty new idea. And then like there was the one with the skiing and I think and the the deer and like I don't think these things have really existed before. and then I can say this a bit more like objectively cuz I mean I wasn't part of the V3 team. And obviously Shomi was one of the leads of the project. Like some of the things video can do absolutely incredible, right? So I think there's many things there that like did not exist like all of the this this the vlog videos, right, of like different animals I don't think can speak English. talking about stuff and I it's it's really amazing and like",
        "start": 2700.88,
        "duration": 5633.520999999999
    },
    {
        "text": "things video can do absolutely incredible, right? So I think there's many things there that like did not exist like all of the this this the vlog videos, right, of like different animals I don't think can speak English. talking about stuff and I it's it's really amazing and like is that you can you can create completely new things, right? Whereas I'm not so convinced that like maybe other kinds of models it's about creating new things. It's more about doing things that we already want pretty well. so I think that's maybe the like more creative element that comes from these models is from the fact they can generalize. It kind of reminds me I had a debate I think it was a few years ago when LMS were still just beginning to to get traction then there was this argument that maybe you know okay they're just we're just predicting next token you don't really learn anything new and just kind like repeating the the training data. I think ultimately the test is whether to me is whether it's useful right we can debate how much you know some like maybe it's repeating the training data maybe not and we can try and come up with some tests that are outside of the training data which is obviously very difficult the larger the training data is but ultimately if a model is useful I think that's that's to me the the bar right and and I think for video models we see that they are useful like people are creating things that they couldn't have done before. So I hope that with world simulation models and like Genie we will also see like people just using it and researchers using it for useful stuff and I think that will be a success for me. Yeah. Another thing that's been kind of trending in text generation models is using synthetic data. So generating data with one model, using it to train another, and potentially like debate on whether that can actually scale up to more capable models. Do you see Genie 3 generating worlds to to to train Genie 4? that's a great great question again. I think so I think there's a different angle to this. Obviously we we're very focused on using Genie 3 to generate synthetic data for agents, right? and so I think that in some sense the initial design of it was actually ahead of its time. It was always about synthetic data generation for training other AI models. now indirectly if if I said if you combine that with what I said before and those AI models then are used to evaluate future versions of the genie models then indirectly that has contributed. , in terms of the specific method of like Genie3 to Genie4 data generation, I'm I'm not sure can really speculate on that one too much at this point. Okay. , and then and then",
        "start": 2859.76,
        "duration": 5941.282000000001
    },
    {
        "text": "are used to evaluate future versions of the genie models then indirectly that has contributed. , in terms of the specific method of like Genie3 to Genie4 data generation, I'm I'm not sure can really speculate on that one too much at this point. Okay. , and then and then preview, small cohort of people get are getting it. What are the specific milestones that would unlock broader access or andor an API to specifically people like me and me me in particular actually want to use it. So we definitely we definitely want to bring it to more to more people, right? Like it's it's it's not we while it's a research preview, it's definitely something that we're exploring. we don't have a concrete timeline at the moment. Of course, once once we know we we'll share it. but it's definitely the goal is not to build something that cannot be used, right? that like from we are we were build like I think now general theme for us is that we want to build things that people can ultimately use and make and build on top of okay but without speaking about specific dates or timelines are there specific milestones or achievements internally that you'd like to see whether it's safety scalability that would you know lean more towards general availability yeah I don't I don't I there definitely we need to get some feedback that's why we we started with a smaller cohort and I think generally when there is a new you know application or a new kind like capability set of capabilities we first want to test and and hear from from people in as you said smaller cohorts and potentially diverse cohorts as much as we can to hear what they think what are the risks what are the the potential applications and I think that's pretty much the the main part of it just to to to get this feedback and to be able to process it before we expand. All right. Jack Schlommy, I want to thank you so much for chatting with me today. This has been a pleasure, especially talking about simulation theory with you. Yeah. Yeah. Yeah. It left me pondering. Yeah. Thanks for your time. Thank Thank you guys.",
        "start": 3014.8,
        "duration": 6184.561999999998
    }
]