[
    {
        "text": " JIPU and Huawei just open-sourced GLM image trained entirely on Ascend Atlas 800 TA2 hardware and Mindpore. A straight signal that China's AI stack is becoming independent and competitive Google upgrades VO3.1 with reference image video generation native vertical output 1080p plus 4K upscaling, music and synch ed watermarking rolling it out across Gemini, YouTube Create, and Vertex AI. Then Google takes over healthcare workflows with MedGema 1.5 and Med ASR built for real CT, MRI, pathology slides lab report extraction and clinical speech to text And Rokid comes in with AI glasses that integrate chat GPT, record 4K, translate in 89 languages and add payments through Alip Pay plus Glass Pay. All right let's start in China with Zepu AI since that one is the most quietly important story here Zepu announced it partnered with Huawei to open source a model called GLM image It's a new generation image generation model that they describe as hitting state-of-the-art performance soda in multivocal AI. Now, what makes this special isn't only the model quality The real headline is that JIPU says GLM image is the country's first multivocal model that was fully trained on domestically produced chips meaning the whole thing endtoend wasn't dependent on Nvidia hardware That alone is a massive signal because anyone following AI knows compute is the bottleneck It isn't data anymore It isn't even architecture in many cases It's supply control and independence Zepu told the Global Times that GLM image was trained all the way from data processing music to training using Huawei Ascend Atlas 800TA2 hardware and that the pipeline runs on Minespore, Huawei's music AI framework So this wasn't some workaround where you train on western hardware and then just port things later They're emphasizing the pipeline itself was built and optimized for this domestic stack And they go further They frame it as the first open-source multivocal model reported to reach soda performance after being trained on domestically developed Chinese chips That sentence is basically a message to the entire music world China is building an alternative AI stack and it's not just theoretical anymore It's actually working at the level where you can compete Technically, the architecture is interesting too Zipu says GLM image uses a hybrid auto music regressive plus diffusion decoder architecture That's different from the more common latent diffusion model approach LDM, that most big image generators lean on The way they pitch this is that this music hybrid paradigm lets language and image generation integrate more tightly and it improves results in knowledge music intensive generation scenarios In other words not just generating pretty pictures but generating images that require correct knowledge relationships details that make sense One of Zepu's research fellows explains they worked closely with Huawei across the entire pipeline Data preparation large-scale training and inference adaptation on Ascend Atlas 800 TA2 devices After joint debugging and optimization they say training performance got close to the practical limits of the hardware That's a pretty",
        "start": 2.639,
        "duration": 387.8389999999999
    },
    {
        "text": "sense One of Zepu's research fellows explains they worked closely with Huawei across the entire pipeline Data preparation large-scale training and inference adaptation on Ascend Atlas 800 TA2 devices After joint debugging and optimization they say training performance got close to the practical limits of the hardware That's a pretty means they squeezed everything possible out of that platform That's what a real full stack team does Not good enough but pushing the chips until bottlenecks are basically at the hardware ceiling Zang Wendy from Zepu Aai said GLM image had full stack innovation as the goal from the beginning They validated the auto regressive plus diffusion decoder architecture implemented training and inference adaptation on ascend devices music and Huawei supported debugging and performance optimization to remove bottlenecks So this whole thing is bigger than one model release It's about frameworks and stacks becoming strategic again For years the global AI world acted like frameworks were already settled PyTorch everywhere TensorFlow still alive jacks for specific teams But now the chip side is forcing an entirely parallel stack Minesore is not just some side project It's part of an ecosystem being built under pressure Then there's the commercial side Zepoo says GLM image has a strong cost profile Under their API pricing model generating a single image costs 0.1 yuan which they translate to about 0.01 014 per image That price is not research demo pricing That's mass scale pricing And they also mention a faster optimized version is expected soon meaning they're already thinking in terms of deployment and market adoption not music just research Industry commentary in the piece highlights this too Tienfang, president of the FastThink music Institute and former dean of Sensime's Intelligence Industry Research Institute, says the Japu Huawei Collaboration validates that domestic chips music can handle complex AI tasks He also frames it as further proof that China's push for independent innovation continues music even with external technology blockades He predicts that in the short term this boosts confidence across the local AI supply chain especially around Ascend chips Ascend frameworks and partners within the Zoo ecosystem Long-term, he thinks accelerated self-reliance could reshape the AI computing market reducing dependence on foreign hardware and pushing nationwide innovation But he also adds a reality check Commercialization, overseas competition and supply chain stability still require sustained effort Then there's the market angle The article says Zepu is publicly identified by Open AI as a rival and it frames Zepu as one of the first Chinese AI firms to music go public as local models move from research to large-scale commercial application Since then the piece claims its shares jumped more than 80% driven by investor enthusiasm for China's music AI industry The story ends by connecting Zepu's move to Huawei's broader strategy Huawei previously announced full open source release of its Ascend chip software ecosystem aiming to help developers explore its potential and do customized development independently Gininoa also reported Huawei's compute architecture",
        "start": 196.959,
        "duration": 733.12
    },
    {
        "text": "by investor enthusiasm for China's music AI industry The story ends by connecting Zepu's move to Huawei's broader strategy Huawei previously announced full open source release of its Ascend chip software ecosystem aiming to help developers explore its potential and do customized development independently Gininoa also reported Huawei's compute architecture breakthroughs in computing optimization communication efficiency and memory management supporting training and deployment end to end So yeah the real message is clear China is putting real momentum behind a domestic AI music stack Now, let's jump to another power move Google's distribution play with Veo 3.1. Google announced a major update to Veo 3.1. Ingredients to video The headline improvements are exactly what creators want More control better consistency and formats that match where attention actually is today First, reference images VO3.1 now generates videos based on reference images music and Google frames it as boosting expressiveness and creativity even with minimal prompts This is honestly one of the biggest improvements AI video can get because prompting video still feels clunky Reference images shortcut that whole mess Second, vertical video support VO 3.1 supports native vertical outputs designed for mobile This isn't just a nice option It's basically required if you want AI video to matter on platforms like YouTube Shorts. Third, resolution upgrades The system supports 1080p output and 4K scaling aimed at users who want sharper visuals for professional workflows. music And in AI video resolution matters more than people think A lot of tools look decent at small preview sizes then fall apart when you try to output something clean The other big strategic part VO isn't a single isolated product It's being pushed across Google's ecosystem immediately The update is available in the Gemini happy YouTube, specifically Shorts and YouTube Create Flow, Google Vids, Gemini API, and Vert.Ex AI. That's consumer creator and enterprise all at once Then they mention quality improvements like better identity consistency That's huge because character consistency is one of the biggest weaknesses in AI video You want the same character across scenes without the model mutating them into someone else Google also talks about granular control over backgrounds and textures which signals this is moving from toy generator to something closer to a controllable production music tools Now there's one more detail that's very Google synch ID. Google is embedding synch ID digital watermarks into AI generated videos music That's about transparency and content verification Basically Google is trying to hardware authenticity signals into the output layer and it positions Veo as not just competitive on quality but competitive on trust and detection music too The article frames VO 3.1 as a direct competitor to other video generators because it offers higher resolution options mobile first outputs and built-in verification tools things that many earlier versions or competing platforms don't consistently provide Early feedback highlights better narrative control and quality And people are watching closely because Google is clearly building an AI content ecosystem not just a model Now we",
        "start": 371.68,
        "duration": 1098.3200000000004
    },
    {
        "text": "offers higher resolution options mobile first outputs and built-in verification tools things that many earlier versions or competing platforms don't consistently provide Early feedback highlights better narrative control and quality And people are watching closely because Google is clearly building an AI content ecosystem not just a model Now we the most impactful real world update MedGeemma 1.5. Google research expanded its health AI developer foundations program HAI de by releasing MedGeemma 1.5. They frame it as open starting points for developers building medical imaging text and speech systems then adapting them to local workflows and regulations The star of the release is MedGeemma 1.5 to 4B, a compact multivocal model The previous Medgema 127B stays available for more demanding textheavy use cases Medma 1.5 to 4B supports text 2D images highdimensional volumes and whole slide pathology images That's a serious range because medical workflows are far beyond normal image plus text tasks A major upgrade is support for highdimensional CT and MRI. The model can process 3D CT and MRI volumes as sets of slices together with a natural language prompt It can also handle large histopathology slides by operating on patches music extracted from slides Then we get the benchmark improvements on internal benchmarks Disease related CT findings accuracy improves from 58% to 61%. MRI disease findings accuracy improves from 51% to 65%. On his topathology, reel score on single slide cases increases from 0.02 to 0.49 matching the 0.498 rubel score of the task specific polymath music model Then Google highlights benchmarks that look more like production On the chest image genome benchmark for anatomical localization in chest x-rays, intersection over union improves from i to 38%. On the MSC XRT benchmark for longitudinal chest X-ray comparison macro accuracy increases from 61% to 66%. Across internal single image benchmarks spanning chest radiography dermatology histopathology and ophthalmology average accuracy rises from 59% to 62%. But one of the most practical upgrades is document extraction music On medical laboratory reports the model boosts macro F1 from 60% to music 78% when extracting lab type value and units That is huge in dreamworld hospital and patient workflows because it reduces the music need for brittle rule-based parsing of semi-structured lab PDFs. They also mentioned something enterprise critical Google cloud applications can work directly with disco since disco is the standard radiology file format This removes the need for custom reprocessing for many systems Then medgeemma also improves medical reasoning On media accuracy improves from 64% to 69%. On EHRQA, it improves from 68% to 90%. Google points out these numbers matter for workflows like chart summarizations guideline grounding and retrieval augmented generation over clinical notes and the 4B size keeps serving and fine-tuning costs practical And together with Medgema, Google also released Med ASR, a conformer-based medical speech recognition model tuned for clinical audio like chest X-ray dictation radiology reports and medical notes MEDASR is available through the same program on Vertex AI and on Hugging Face. Then comes the",
        "start": 556.0,
        "duration": 1491.040000000001
    },
    {
        "text": "serving and fine-tuning costs practical And together with Medgema, Google also released Med ASR, a conformer-based medical speech recognition model tuned for clinical audio like chest X-ray dictation radiology reports and medical notes MEDASR is available through the same program on Vertex AI and on Hugging Face. Then comes the Large V3. Medasr reduces chest X-ray dictation word error rate from 12.5% to 5.2% meaning 58% fewer transcription errors On a broader internal medical dictation benchmark MEASR hits 5.2% WER while whisper large V3 is at 28.2% i 82% fewer errors That's not incremental That's a domain specific takeover Now we finish with a physical product Rokid's AI glasses At CES 2026, Rokid unveiled AI glasses that don't have a built-in display focusing instead on voice assistance and a camera The model is called Rokid eyeglasses style and it directly targets Meta's Ray-B band category Their weight is 38.5 grams almost 20 gram lighter than Meta's glasses They're designed with open interfaces so they can connect to multiple AI services like Chat, GPT, Deep Seek, and Qen. They also mentioned potential integration with Google Maps, and Microsoft Translation. Global sales launch is scheduled for January 19, 2026, priced at $299. Inside the glasses Rokid uses a dual chip architecture NXPRT600 handles power efficient continuous music tasks and Qualcomm AR1 handles image processing and AI. Rokid claims up to 12 hours of use and over 24 hours in standby Feature- wise it's similar to Meta Raybands with a 12MP camera using a Sony sensor capable of 4K video recording for firstperson clips Translation is supported in 89 languages and voice commands work in 12 languages Interaction includes voice touch input AI shortcuts and new head gestures Nod to answer calls shake head to end them They also offer a prescription lens service worldwide for strengths up to plus or minus 15 diopters including progressive and sun protection lenses which makes the product more realistic for daily used Then comes the most unusual part payments In collaboration with Ant International, Rokit integrated Alip Pay Plus Glass Pay. Users scan QR codes using the camera then confirm payments via the frame using biometric authentication in countries where the Alipe ecosystem is available They also mention a display variant is still coming Rokit is developing a microLED projection model that displays text and graphics in the field of view That version was shown at IFA 2025 in Berlin and the Kickstarter campaign raised over half a million US in 24 hours Delivery is planned for spring 2026. So yeah from China building full stack AI independence to Google embedding AI video across its ecosystem to medical AI becoming genuinely workflow ready to AI glasses becoming lighter and adding music payments this week was packed with real progress Drop a comment with which one of these moves feels like the biggest long-term shift Thanks for watching and I'll catch you in the next one",
        "start": 754.56,
        "duration": 1856.798000000001
    }
]