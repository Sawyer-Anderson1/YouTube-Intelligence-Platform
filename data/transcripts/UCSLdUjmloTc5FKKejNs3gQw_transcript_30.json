[
    {
        "text": " If you're still sending JSON to AI models, you're wasting money. Every bracket, every quote, every piece of punctuation, those become tokens, and tokens are what we pay for. But we can instantly reduce this with a little new formatting method called Tune. And Tune can instantly reduce these anywhere from 30 to 60%. Heck, I even got mine all the way down to 63%. And in today's video, I'm going to share with you guys what this is and how you can start using Tune to fine-tune these for your LLMs. Enjoy this video because after today, JSON is going to become a thing of the past when you're working with your LLMs. Guys, we have videos coming out all the time. So, be sure to subscribe to follow along. Welcome to the channel. Here I have some normal JSON data. Now, sure, it's clean. I know that. But notice that on all of these, I'm just repeating all these keys again and again. And every time, I want you to look at how many quotes do we have. Our keys have quotes, the values have quotes, how many commas are there, how many brackets, how many braces. This is all noise to an LLM. And when you scale this up, logs, metrics, chat histories, the token cost skyrockets. And that's what we're paying for if we're using JSON still cuz all of these are just tokens to an LLM. Now, Tune is different. It uses indentation like YAML and it uses a tablelike format for arrays, which is where it absolutely crushes JSON. This is just a quick converter website that I found. On the left is all the original logs that I just showcased you guys in our JSON file. And then when we convert that, this is the tune output of that. And you can see how much shorter it is, how much cleaner it is. Instead of repeating keys for every row, tune defines the structure once and then it gives the model the data. Right? So here is that data it's giving us. All right, guys. Enough theory. Let me show you exactly how Toune stacks up against JSON data with our actual data. I'm going to write a tiny TypeScript script, no pun intended. And what we're going to do is I'm going to convert our JSON data into tune data and we're going to actually visualize the output in our terminal. This is going to be an absolute gamecher. Let's jump right into our code. We already saw our JSON data that we're converting here. I just have a TypeScript file. This is just going to help us visualize the output to show the comparison. All I'm going to do here is I'm going to create a new file called convert.ts for our TypeScript if I can spell convert right. That's going to be the big thing here. Okay, we spelled that correctly. Now that we have that,",
        "start": 0.4,
        "duration": 340.5610000000001
    },
    {
        "text": "visualize the output to show the comparison. All I'm going to do here is I'm going to create a new file called convert.ts for our TypeScript if I can spell convert right. That's going to be the big thing here. Okay, we spelled that correctly. Now that we have that, Make sure npm is installed and and then we're going to go through a few dependencies here. I am just going to run npm install and we're going to go at tune format /tune. Get this onto our system. And then we're going to install our GPT encoder. So I'm just going to install let's go with our GPT. We'll do three encoder just to output the comparison here. Okay. Once we have that ready to go, I'm going to close my terminal. And up here in our code, we're going to import FS from FS. We're going to import encode from our tune import that we have. This is going to help us encode it. And then we're going to import encode as tokenized. So I'm just giving this a nickname here from our GPT3 encoder. Okay, now that we have everything to go, I am going to import my other TypeScript file. This is just for the visualization part. this is just called print comparison from our token utils file going on here. What we want to do is I'm going to create a constant variable just from my JSON here. And we're just going to say JSON.parse. Inside here, we'll give it SF. I want to read the file sync. And we're going to tap into our JSON data. So, I'm just going to go data.json. We have that. Okay. And then just for encoding purposes, let's just do UTF8. And then we're going to create our constant for our tune. And this is going to be much simpler. We just have to use their encode function. And we're going to give it our JSON. This is really going to handle all the tuning tuning all the encoding for us. From this I'm going to create a JSON string and we will go JSON to stringify this and I'm going to give it our JSON nil and we need to insert a two here. Excellent. Now, we want to count our tokens here. And I want to get my JSON token count as well as my tune token count. So, I'll say here tune tokens. For our JSON tokens, all we're going to do here is use tokenize from the GPT3 encoder. And I'm going to give this our JSON string, the variable that we created. And we just want to get the length of that. For our tune tokens, we are going to do the exact same thing. We're going to use tokenize. We can give it our tune data that we have there converted and we're going to get the length of that. Really, I just want a clean way to now console",
        "start": 173.92,
        "duration": 665.041
    },
    {
        "text": "that. For our tune tokens, we are going to do the exact same thing. We're going to use tokenize. We can give it our tune data that we have there converted and we're going to get the length of that. Really, I just want a clean way to now console here and console log. let's just do something like our JSON. We're going to console log our JSON string. So we're going to go here JSON string and then we will actually do our tokens here. So let's console log those. I'm going to do the same thing for tune. And then finally I'm going to call off our print comparison import here. I'm going to give it my JSON tokens and we're going to give it our tune tokens. Let's run this and actually see how much this breaks it down. Open our terminal and let's just go here. npxtsx convert.ts. TS. Boom. Look at this. This is insane. Okay, so it it outputs everything for us. So if we go up top here, all of our JSON data, it's still outputed. It counted that as 1,236 tokens. Tune breaks that down to 454. That's insane. And there's my over 63% reduction. And you can see how much tune is actually saving us on all those tokens, which in the end is lowering the cost that we're paying for these tokens. Why does Tune do so well? Well, JSON forces the model to actually parse tons of structural syntax we have over here and it sees those quotes and braces as actual tokens. Over here, Tune strips all that away. for arrays, especially uniform ones like logs. Tune uses a tabular format, which is massively token efficient. LLMs process this way more reliably and they make fewer mistakes. Now, there is no tune extension. I just made this to see this side by side. I got this tune data down to 11 lines. Our JSON data is still at 94 lines of code. All right. Now, Tune is perfect for things like logs, metrics, events, product cataloges, configuration tables, you name it. All that info you're sending to the LLM, but it's not great when we have heavily nested data structures cuz it's going to get rather confused. If we do have heavily nested data that we're trying to process and send over, we might want to stick with JSON for that. Now, let's talk about money cuz we all love money. If you reduce your tokens by anywhere from 40 to 50% across all your pipelines, that can end up saving you thousands of dollars depending on how much you're actually processing and you're giving to your LLM. So, tune is going to be a huge moneysaver if that is how much info you're actually processing. It's insane. Guys, go out there and try with your own data from logs, analytics, you name it. Go try Tune and see how much you can",
        "start": 344.08,
        "duration": 995.3600000000002
    },
    {
        "text": "you're giving to your LLM. So, tune is going to be a huge moneysaver if that is how much info you're actually processing. It's insane. Guys, go out there and try with your own data from logs, analytics, you name it. Go try Tune and see how much you can you're out here building AI tools, then Tune is exactly what you should be using when interacting with your LLMs. Guys, I hope you enjoyed today's episode and we'll see you in the next one.",
        "start": 513.519,
        "duration": 1019.1220000000002
    }
]