[
    {
        "text": " Where do you think the biggest bottleneck is today? We're heading to a world where energy is going to be a massive bottleneck. Do you ever consider some of the newer players like a Cerebrus or a Grock? So in 2017, we got super excited when we saw Cerebrus. 2017 Open AI was I guess very different than today Open AI. You'd be surprised. People still don't always listen. Is my job in danger? AI is going to change a lot of jobs. Are there still developers? I think that we are going to change a lot of fundamentals of the social contract. Do you still have the same definition of AGI? I really used to think of it as this destination, but instead we really think of it as this continuous process. Let's say we were to 10x our comput supply right now. Would we 10x revenue? I'm not sure if we 10x, but would we 5x? Chat GBT really makes you realize how unnatural it is to go to a static website to just read stuff. Do you think that software is going to be fully generated? I think so. I think it's going to be super cool. What is the conversation internally at OpenAI like? pain and suffering. It's the real truth. All right. Greg, thank you so so much for joining me today. Thank you for having me. It's good to be here. I have a bunch of questions for you. I wanted to start first with scaling and specifically Sora. So Sora 2 released last week. what is it like thinking about scaling a model like Sora and how is it different from a text or image model? Well, I guess the way I would think about it fundamentally is that, you know, at a broad level, everything is still just deep learning, same mechanics, same sort of underlying principles. You got to scale up massive with a massive amount of compute, a forward pass, a backward pass, gradient step. , at a more detailed level, still a transformer, right? Which is actually quite amazing. Okay. Yes. And you know, you train it in a different way. you're using you know sort of a different different process much more you think about things like diffusion or sort of a different way of of thinking about how to you know sort of pour compute into these models. but fundamentally the thing that I find so amazing is even though you're talking about text versus video which are seemingly as different of modalities as you could expect that you have this massive overlap in terms of the actual underlying computational process to learn and generate them. And so there's something like really really deep about that fact. So are you pretty bullish that the transformer architecture is going to get us to really the next level maybe full world models which is you know Sora 2 is obviously a big step in that direction.",
        "start": 0.16,
        "duration": 296.40099999999984
    },
    {
        "text": "generate them. And so there's something like really really deep about that fact. So are you pretty bullish that the transformer architecture is going to get us to really the next level maybe full world models which is you know Sora 2 is obviously a big step in that direction. is that I think there's a lot of questions around like are we missing big ideas? Do we need like another innovation on the level of a transformer? I think that there is lots of room for innovation and we've seen it right that I think the algorithmic progress has kept a pace. I you know we've done studies over the years to see exactly like what the curves are. I don't see those stopping. the scaling curves also continue right the data curves also continue and that's the thing that has driven this revolution is that there's multiple limiting reagents and each one you can just keep tuning and you'll just see the performance of the models just increase appropriately. So I think there's so much more to be built. I wouldn't be surprised if AGI looks kind of similar to the models we have today, but I would be shocked if it was exactly the same architecture. And then when you're looking at these different types of models, although they are all transformer-based, are they pretty significantly different in cost and and like how how do you measure the unit economics of the different types of models that you're serving today? Yeah, there there's definitely different performance characteristics that sometimes we have a different inference stack associated the optimizations are different that different models are maybe a metabol to different types of hardware that you can really see that like the exact balance between like the memory and the compute and all those things those can be different. so there's a lot of systems work that looks quite different at a detail level and when you're really trying to extract screaming performance out of the hardware that it pushes you in very different dimensions. , but I think at the end of the day, we really view it as like the fundamental driver of all of this innovation and really bringing it to the world is compute. So you got to build as much as possible. And there's some specialization of different accelerators and those kinds of things. But if you zoom out, everything is just doing matrix multiplies, doing some, you know, some some , you know, attention mechanism, that kind of thing. and so that we you know we do a lot of of juggling internally of capacity and u you know sort of prioritize you know five different demands on one person's time who can go and optimize five different models. it's tough but that's what we're signed up to do. All right let's let's continue on hardware. a big announcement with AMD. So is it is it fundamentally",
        "start": 147.76,
        "duration": 550.24
    },
    {
        "text": "you know sort of prioritize you know five different demands on one person's time who can go and optimize five different models. it's tough but that's what we're signed up to do. All right let's let's continue on hardware. a big announcement with AMD. So is it is it fundamentally hardware? Is it just okay now we just have this increasingly massive pool of resources we can pull or is there like deep technical changes that need to be made? So, we've actually been investing in AMD software in many ways for a long time, right? Because we build on top of Triton. That's something that a project that we've sponsored and and and really helped develop and that the vast majority of our GPUs are running Triton Triton kernels effectively. And I'd say the way I' I'd look at it is that there's inference and there's training. There's a massive fixed cost to getting inference working. there's an even more massive fixed cost to getting training working and we're at a point where we actually have been able to use with very small amount of work to be able to use AMD software this year and get good performance out of it and so a lot of that is enabled through just like this partnership we've had for a very long time we've provided a lot of feedback and we're at a point now where from an inference perspective we feel quite good about scaling and there's different niches for different pieces of hardware and that we're excited about the MI450 series that there's a lot of of good innovation in there and that you know we will also scale up a ton of of NVIDIA for inference and training as well. Do do you ever consider some of the newer players like a Cerebrris or a Grock like kind of the like wafer scale computing and do you ever consider those? Yeah. So since so in 2017 we got super excited when we saw Cerebras because it was just this totally new paradigm. you looked at the numbers, you're like, \"Wow, like if we could have like a million of those things, we could build AGI, right?\" It's just like you just realize it's a very different very different sort of platform. And I think that the it's turned out that building nonGPU architectures has been way harder than we expected in 2017. , but from the very beginning, we really mapped out the whole ecosystem. We tried to talk to all the different chip players and try to give them some advice, talk about here's what the shape of the workload is. And honestly, most of the companies wouldn't listen to us, right? It's like 2017 Open AAI was, I guess, very different than today Open AI. You'd be surprised. People still don't always listen. , okay. But I think it was to some extent it wasn't even that they thought we were",
        "start": 276.56,
        "duration": 815.1210000000004
    },
    {
        "text": "honestly, most of the companies wouldn't listen to us, right? It's like 2017 Open AAI was, I guess, very different than today Open AI. You'd be surprised. People still don't always listen. , okay. But I think it was to some extent it wasn't even that they thought we were come from the chip world and have a particular way of looking at the problem and don't understand the workload and you try to say, \"No, no, no. This perspective is backwards, right? right? That you really need to think about it in this other way that it's going to be about big models, not, you know, small models, whatever. , those kinds of design inputs. If you don't buy into it, it's very hard to then rebase your whole worldview on top of it. And so I think that what's been what's really distinguished the successful players in the space has been people who bring in people with a deep learning perspective or really try to pay attention to where the workloads are going. H and when you're looking at the entire pipeline of building out compute all the way to serving inference, where do you think the biggest bottleneck is today? Well, I mean, I think we are heading to a world of absolute compute scarcity and I think that we're heading to a world where energy, certainly the US, is going to be a massive bottleneck, but there's all sorts of parts of the supply chain right now that have not yet adapted to the demand shock that we see coming. And so that that's what we've been really trying to back to us saying things continually over and over for many years now. We've been saying, \"Hey, we need to build more compute.\" Yeah. And I I there's been rumors that OpenAI is developing its own chips, but do do you ever see maybe investing in in your own energy grids and systems inventing new things on that front? If you had talked, you know, fast forward or rewind 10 years back to, you know, 2015, 2016, , me and ask what we're going to do. We're here to build AGI. And we thought of it very much as a software endeavor. We thought of it in fact as just a we need to come with some new ideas. We'll click it into place. AGI created. We started to find that actually it's a comput endeavor that that is like this fundamental reagent that is much easier to scale than many other things. It's that's why you press so hard on comput. have to push it to the limit. And then you start realizing that actually there's this massive physical infrastructure build you have to do. And so we're now getting into that world. We're doing things like Stargate, starting to build our own data centers. And so I think that the where does it stop is really about what is the",
        "start": 410.96,
        "duration": 1072.0800000000008
    },
    {
        "text": "start realizing that actually there's this massive physical infrastructure build you have to do. And so we're now getting into that world. We're doing things like Stargate, starting to build our own data centers. And so I think that the where does it stop is really about what is the world if the market does wake up to the demand that we're really very loudly trying to say is coming, not just from us, but from the whole industry, then great. Like I would love not to have to go and figure out how to how to build energy ourselves. But we're here to do the mission. And so with the limited GPU, limited compute that you have now, you have a lot of conflicting needs, , consumer products, enterprise products, developer APIs, training. , how do you like what is the conversation internally at OpenAI like when you're trying to decide where that compute investment belongs? Pain and suffering. It's the only it's the it's the real truth. It's like it's so hard because you see all these amazing things and someone comes and pitches another amazing thing and you're like yes that is amazing and you guys are doing so much. So like how how did what is the I mean you know we we have a tiny company and we're we can't decide of of the many things what to do and so I can't even imagine that at open AI scale. What is that? Give me a little bit more about what that internal conversation is like. Yeah, the mechanics the mechanics more more specifically you know we've evolved them over the years but right now there's I you know Yaka our chief scientist and Mark who who runs research together decide the compute allocations but more broadly actually there's there's first a split between the research side and the applied side and that usually gets adjudicated at you know sort of the you know SAM you know Fiji like you know that set of people usually is making that call and then I you know within research I described how that gets allocated. , at a mechanical level, there's a bunch of people on my team who are really dedicated to this hard task of actually shuffling around the GPUs. , so you know, it's like it's really amazing to watch. , so you know, Kevin is an example. Kevin Park is someone on my team who just like you go to him and you're just like, \"Okay, like we need like this many more GPUs for this project that just came up.\" And he's like, \"All right, there's like these five projects that are sort of winding down and this one's got to finish at this point and so we can sort of make the Tetris work.\" , and it's it's just really amazing to see both the intention part of this of like just like",
        "start": 541.519,
        "duration": 1322.5590000000009
    },
    {
        "text": "right, there's like these five projects that are sort of winding down and this one's got to finish at this point and so we can sort of make the Tetris work.\" , and it's it's just really amazing to see both the intention part of this of like just like then the actual solver and some of that is human, some of it is spreadsheet, some of it is like you start I think it'll be very interesting to see if we get our models in there. , but so I just say it's like it's not as easy process. , but I think it is just like it's been very interesting to watch because I think that compute is such a driver of teams productivity and so people really care like that the the sort of energy and emotion around do I get my computer or not is something you cannot understate. All right, let's let's shift the gears for a second. So you made an announcement you're now bringing kind of lack of a better way to describe it the web into Chad GPT. You showed off the Zillow example. , as apps continue to migrate towards like a more native experience within chat GPT, how how are you thinking about this like decoupling of the human to internet experience that seems to be happening like as agents continue to increasingly browse on our behalf. The the the time where an actual human is going on the internet browsing traditional websites seems to be decreasing. What do you think the next 18 months look like? looks like and just quickly actually I want to add one thing to the previous answer which is also I think where we're headed is a world where compute is the driver of economic productivity throughout the whole economy and so this microcosm that we've seen within OpenAI that you said you see within your company I think we're going to see everywhere and so I really view this like we got to build compute as a way to alleviate this compute scarcity and this compute sort of you know clash overall allocation before we move move on to to that next question. What do you think the ratio is between supply and demand right now? How far off are we? Oh, I think I think we're quite far off. I don't know orders of magnitude. I I so I was going to say I don't know if we were let's say we were to 10x our compute supply right now. Would we 10x revenue? I'm not sure if we would 10x but would we 5x maybe? Yeah, maybe. Right. Because we have so many products in the hopper that we cannot release and you know you just see it very concretely things like pulse it's like pro only right. pulls us such a great product. Yeah, we're going to talk about that. That is a really cool product.",
        "start": 668.959,
        "duration": 1580.0780000000004
    },
    {
        "text": "Right. Because we have so many products in the hopper that we cannot release and you know you just see it very concretely things like pulse it's like pro only right. pulls us such a great product. Yeah, we're going to talk about that. That is a really cool product. compute. I need to get you a sticker that says that. Okay. Okay. So, talk to me about the decoupling of this the internet because it just seems like the internet the fundamental way that we browse the internet is changing drastically right before our eyes, especially with with agents being able to browse on our behalf and now with bringing the the the kind of traditional website into chat GPT. So, how what are your thoughts on that transition that we're seeing? Well, I feel like chat GBT really makes you realize how unnatural it is to go to a static website to just read stuff, right? Just like static information, like a single fact that you're looking for that you're kind of mining through a big page that is kind of not relevant to what you actually want. And I think we've sort of almost moved past that still happens, right? That's not the like the dominant paradigm or the thing that people really want to do and you just realize there was a bunch of time you were spending. It was not value ad time. It was this like sifting through this haystack for a needle and like a machine should really do that for you. It really should. And I think what you're going to start to see with apps in chatp with these dynamic apps is that going to a website to like click a bunch of buttons to like do something dynamic also just feels like this like total like backwards like thing that we should have moved past a long time ago. Yeah. And so I think that we're moving to a world where people are going to be much more protective of their time, right? Because there's sort of no excuse anymore things that are not adding value, where the human is not thinking hard or providing creativity or somehow providing direction, feedback, something like that. If you're just sifting through a big list of stuff, that's what an AI is for. And then how does that change monetization on the web which is you know traditionally CPM based advertising based you you give the website your eyeballs and in exchange they give you some free content as well as advertising but when agents are browsing on your behalf and especially as you're bringing like a Zillow.com into chatpt then there's all these conflicts of okay were they serving ads and then what does that look like and so like how how do you think about the changing monetization layer of the web as as these changes occur? Well, the truth is I think no one knows yet,",
        "start": 799.76,
        "duration": 1833.6780000000008
    },
    {
        "text": "chatpt then there's all these conflicts of okay were they serving ads and then what does that look like and so like how how do you think about the changing monetization layer of the web as as these changes occur? Well, the truth is I think no one knows yet, think we're gonna have to explore. We're gonna have to figure out what the right way to to, you know, what the right new monetization paradigm, , the right way to scale all of this. But I think that fundamentally there's new pressure from these technologies to make sure that you're adding value to the user. And if you look at chatbt itself, right now it's a subscriptionbased product, right? We maybe would not have predicted that when we launched it 3 years ago, but people are willing to pay because it adds value. Adds value in your professional life, your personal life, just sort of across the board. And so I think that that's not to say that advertising doesn't have a place, right? But I think that advertising that is really about you're just mindlessly scrolling through, you know, things to try to find the like sentence you care about and it's just you happen to be on that page and click a thing. it just like that that doesn't feel like quite the the fundamental driver of value anymore. , but I do think that there's going to be new revenue models. There's going to be new ways to monetize. And I think that it's it's honestly like I think probably the most exciting time to build yet. And so I if you rewind, you know, a decade plus and you look at publishers during the mobile transition, a lot of them became beholden to Apple because they were in their app store and and what would you say to them on why this is different where Chad GBT really becomes the first place the homepage of your artificial intelligence experience? Well, I think the story is not yet written. And one observation that I have about AI is it seems to always play out in a surprising way, right? Just totally different from anything we've seen before. It has elements that are reminiscent, but I think there's no like one paradigm that you be like, \"This is exactly like the internet or this is exactly like mobile or this is exactly like the app store.\" I think it is something different. So, what is the way that you want to interact with an AI? Is it that there's like one website that you go to that just sort of mediates your interactions with everything else? Like I'm not entirely sure because AI the point in many ways is that it brings the machine closer to the human, right? Rather than you having to contort yourself to think about, okay, there's like a website with a URL and I have to go to this thing.",
        "start": 927.519,
        "duration": 2083.3580000000006
    },
    {
        "text": "else? Like I'm not entirely sure because AI the point in many ways is that it brings the machine closer to the human, right? Rather than you having to contort yourself to think about, okay, there's like a website with a URL and I have to go to this thing. what you ask. And in fact, the machine should proactively think about what you might want and go do it for you. And I think that these shifts in the paradigm I think probably will also shift how we think about what the entry point is, where the opportunity is. And so I just view it as there is so much surface area here to build that I it is not obvious to me that it's even possible to have one portal that is going to be the interface to all of it. I want to actually continue on that for a moment. What like how how far do you think we are from AI being able to predict most of my needs? So you know when Chad GBT first came out, it's very reactive. I'm going to prompt it. is going to give me something in return. Now with things like Pulse, it's starting to be much more proactive. Like how do you see that ratio between proactive and reactive playing out over the next 24 months? I I see proactive starting to become much more of the focus or you know you give a small task and then the AI goes off and thinks for a day, for a week, for a month and we have aspirations that we want to build AIs that can productively think for like a year. like for 10 years. I mean like like this is humans do this right with no human interruption during that year period. Well, I I I think maybe I'd think of it a little bit like think about a human solving like like I think Andrew Wild solving for M's last theorem, right? I think he was like, you know, famously like spent like 10 years like working on it basically by himself. That's not literally there was no human interaction, right? He's probably thinking about sub problems and he like ask people about it. and I think that that's what we want to achieve. We want AIs that can help us solve grand problems. And it's nice to have AIs that are able to go off and do productive work without us having to constantly micromanage. Like it's not very fun to micromanage humans, not very fun to micromanage AIs. But I think that that this kind of world that we're heading towards is one where you could micromanage if you want to, right? Whereas like for a productive human worker, if you micromanage them all the time, they're probably not going to be happy for very long. And so I think that it really is going to open up the",
        "start": 1054.24,
        "duration": 2320.718
    },
    {
        "text": "heading towards is one where you could micromanage if you want to, right? Whereas like for a productive human worker, if you micromanage them all the time, they're probably not going to be happy for very long. And so I think that it really is going to open up the really be able to choose where you want to spend your time. You know, I've seen a lot of u highlights on on like, hey, our new AI can think for x number of hours independently, autonomously. , how do you think about the trade-off between the the duration that an AI can think autonomously versus what it actually gets done in that period? Because if it takes 30 hours to do 1 plus 1, it's a little different than solving cancer. So h like how do you think about the compression of intelligence within the given window and extending that window and the trade-off between the two? Yeah, I think it's a great question. It's very easy to have these benchmarks that are extremely misleading. to to your point, I I think that it's very clear to me that there are like you can think of problems as almost having this computational complexity behind them or something, right? there's just some problems that are going to require more thought, more power, more compute behind them. And kind of what you want is you want an AI that can productively go and think for a day on one of those hard problems, but it would be nice if it would just solve it, right? And 10 seconds. Yeah, that'd be great. And so I think the answer is these are two different dimensions and it's important that we keep pushing on both. Okay. Well, with that, how long has GBT5 thought for? Codeex thought for with full autonomy? Oh, I actually What's the record right now? I actually don't know what the record is. I think I think we've published it. I know I know that we've seen like a number of people have reported seeing like seven hours, but I'm not sure that that's actually the limit. so you you can find it somewhere online, but I mean we're at a point now where it's like we're starting to be able to spend quite a lot of compute on interesting problems. Let's let's talk about Sora 2. really fun, awesome. I think some of my team might be a little addicted to it, but that's okay. It it is really like just a a blast to use. what like as you were developing this the the new model the coming off of Soro 1, why did you decide to build it into this social experience versus just taking the path that Sor1 took and releasing it for usage in kind of a more traditional way? Well, the way that we usually think about what surfaces to build is it really comes down to what is the",
        "start": 1173.679,
        "duration": 2589.917999999999
    },
    {
        "text": "you decide to build it into this social experience versus just taking the path that Sor1 took and releasing it for usage in kind of a more traditional way? Well, the way that we usually think about what surfaces to build is it really comes down to what is the much how we ended up with chat GPT in many ways like I remember you know we've been kind of working on this infrastructure for doing chat stuff and then we had GPD4 and I remember doing we did the first post train of it and the at the time we were just doing instruction following which is you have a data set of here's a question and train the AI to just provide an answer and I for trying. Well, what if you just provide another question that kind of depends on the context of the previous question and answer? Will it actually generalize to know that it should utilize that information? And it did. And you're like, wow, this model is smart. It's like capable of generalizing to this. It like it wants to be a chat model. Like it just is like so clear that the technology is shaped such that you should release this as a chat system. And I think that for Sora 2, there's a little bit of that that vibe to it, right? Where it's like what are the strengths and weaknesses of the model? what are the things you could do with it? What is fundamentally new? , and so I think that there were a lot of directions we could have gone and there still are a lot of directions we can go like the model. The the thing that to me on the inside is always a little sad about any interface, any post- training of the model is you actually have really narrowed down the capabilities of the raw model in deep ways. Like think of like interesting. Yeah, it's it's really really interesting. These raw base models when you play with them, they're incredibly hard to use, but they have like a universe of possibility within them and there's probably so much behind each of the decisions that leads into deciding what to filter down into. Yes. Talk a little bit about that. Sorry to cut you off there. No, I I think this is actually something people don't really understand from the outside. And to me, it's a very sad thing because we used to release base models. GPD3 was a base model. No post incredibly hard to use. Like did you did you use GPD3 back in the day with like all the prompt engineering? Yeah. Yeah. Like you had to like provide like six examples of like solving a task and then like you know you have to that's so that's a function of the model being a base model versus just it getting have gotten better over multiple iterations.",
        "start": 1310.64,
        "duration": 2820.2389999999987
    },
    {
        "text": "all the prompt engineering? Yeah. Yeah. Like you had to like provide like six examples of like solving a task and then like you know you have to that's so that's a function of the model being a base model versus just it getting have gotten better over multiple iterations. it is that these base models you know we train them to just do nextstep prediction in they're almost sort of observing humanity's thoughts and feelings and you know all the public data available. And so it's just trying to say like given this prefix, what comes next? What comes next? And so that at inference time, it's almost like you're dropping it in the middle of a document that it found somewhere out on the, you know, publicly available data and you're asking it what comes next. And so then you have to like kind of think like, okay, how do I format my query in a way that could occur in this naturally occurring distribution? And so turns out that just like a list of if I have a question and an answer and a question and answer and a question and answer and I have a question, probably what comes next is an answer. But if I only have a question, maybe what comes next is another question, right? And so it's like you're kind of trying to like roleplay the AI into sort of thinking that it's in the middle of like some sort of reasonable document that that looks like it's training distribution. And so because this is so hard to use, it's a poor interface. It's not a good product. And it's also something where we have no control over the the the the behaviors and the values that we'll express, right? It's a little bit like, you know, for a human who grows up observing the world, it's going to have knowledge of everything, right? And and to some extent, like one analogy that I've heard that that actually Alec Bradford like to use is that these base models are much more like training a humanity than a human, right? It's got everything in there. It's got every sort of value set. It's got every world view. And so to the question of how it will respond in a certain instance, it is almost anything that a human could respond there, you could set up the model so it would do so. But if you really want to narrow down to a consistent set of values because you have some guardrails, you have, you know, we have a model spec that says how we're supposed to behave in certain cases, then we need some other step on top of it. And that's what post training is, is really saying we're going to take this raw universe of this raw intelligence and we're going to refine it to an almost consistent personality or consistent set of behaviors.",
        "start": 1428.88,
        "duration": 3062.3989999999967
    },
    {
        "text": "behave in certain cases, then we need some other step on top of it. And that's what post training is, is really saying we're going to take this raw universe of this raw intelligence and we're going to refine it to an almost consistent personality or consistent set of behaviors. more social product came before post training or or like did you discover something oh it kind of has this knack to do imitation really well? Like yeah what was the order of operations? It usually goes in it's it goes in a bit of an iterative loop, right? It's like you take the base model, you kind of see like okay, like you kind of prompt it in a certain way like this is interesting. Actually, this would be so cool. What if it was reliable at this thing? You didn't have to do all this work. And so it's a little bit like the base models are the world's best prototyping engine, but they are not reliable, right? Because it is so hard to like figure out the right prompt to get them to like do the task you really want. So it's almost this communication problem and then the post training is this communication. Is your cameo public? My cameo is not currently public. I put mine public and I think Sam Alman mentioned this as well. You know, it's actually like surprisingly comfortable having people manipulate your likeness. Yeah, I I think it's pretty easy. It's pretty fun. Yeah, I think I mean honestly there's not that much thought behind behind my my cameo status. because I think in 6 months, no matter what we do, I think someone else is going to release a video model that lets you do cameos and is not restricted. And so I think we are heading to a world where all of our likenesses are going to be cameoed. , open AI like I think part of what we stand for is really trying to let people know where this technology is going and try to release it in a way that we think is beneficial. And so you can really see that in our choices, but we also don't believe that we have sort of full control over this technology. We're not the only ones building it. Yeah. So when you when you look at Sora 2, it's it's a world model. It's able to simulate the world. Yan Lun has said like LLMs are not sufficient to reach AGI just because language alone is not sufficient to model the world. Do you agree with that? Why or why not? And and how are world models really the future of of AI and reaching AGI? Well, look, I like to look at what things have we learned over the past 5 years, 10 years of AI progress. like what are the things that now we have seen empirical evidence for one way or another. And I",
        "start": 1551.12,
        "duration": 3315.438999999999
    },
    {
        "text": "really the future of of AI and reaching AGI? Well, look, I like to look at what things have we learned over the past 5 years, 10 years of AI progress. like what are the things that now we have seen empirical evidence for one way or another. And I have a world model. You know, there's not enough information in written language to build a world model perspective. This, by the way, a long-standing debate. This isn't a 10-year thing. This is like, I don't know, like 50 years, 100 years. It's like a long time. , I think that would have predicted that you would not be able to do half the things that GPD4 could do, right? Because it's like, you know, you ask it questions like, you know, like I put the water bottle on the table and then I, you know, take the bottle the cap off the bottle and then I put it under the table, like where's the cap? Like, I haven't tested that specific query, but like, do you think it's going to get that right? Probably. And and yeah, I used to have a a test I gave. You know, there's a a marble in a ball. You take or in, excuse me, a marble in a cup. You pick the cup up off the table. Where's the ball now? And Right. it's still on the table. So it I remember three 35 didn't get it. Four got it. And then 40 and beyond just nailed it after that. Spatial awareness is there. Exactly. So what does that tell you? Right. Even if it's not perfectly reliable at your current super advanced task, like you had a super advanced task that was your test and now it's conquered it. Right. It's like it shows you a trajectory. And so to me, like I think that it's very easy to get sucked into semantic debates of like what does it mean to understand? Like are these models really understanding? Are they just simulating understanding? I'm like what does that even mean? Like I don't know what I don't know what those words mean. But I do know you show me an eval that sort of captures this task that you think should be impossible for the models and that we're seeing the models start to creep up into the right and then blow past it, saturate it. I'm like, I think it's got it. It's kind of like what Sam Alman said earlier. say intelligence is really just prediction. Prediction is intelligence. And it it seems like a sim similar argument to large language models actually will be able to be AGI and okay so I I selfishly want to ask you is my job in danger? you know Mr. Beast said AI is a threat to content creators livelihoods and now that's my job. What do I have to be worried about? Do I am I worried? What do you",
        "start": 1679.919,
        "duration": 3557.998999999997
    },
    {
        "text": "AGI and okay so I I selfishly want to ask you is my job in danger? you know Mr. Beast said AI is a threat to content creators livelihoods and now that's my job. What do I have to be worried about? Do I am I worried? What do you definitely true that AI is going to change a lot of jobs, right? And there'll probably be some jobs that right now are something that many people do that won't, you know, that that will either Yeah. just be totally changed in an unre recognizable fashion or just won't won't be there afterwards. There will be new jobs that we're not even thinking of that will be created. What will the balance be? What is the shape of these new jobs? How to think about it? , and I think that fundamentally the way that I think about it is that the shape like one thing I I do think is true about this AI revolution is that I think that we are going to change a lot of fundamentals of the social contract. I think we are going to go to a world that is one of abundance, right? Right. And I think one where you know that you should have like this amazing quality of life even if you are not economically working because there's just so much but that if you are striving you know you are playing the status game you are doing the whatever I think that there's going to be so much more available and so much so much to gain and so much to build and so much value to add and so my honest answer is that I think no one knows exactly what lies before us on the other side of the AI event horizon. , the one thing I know is I think it's going to be stranger and probably more delightful than we can imagine right now. I just started my job, so I'd like to keep it. I mean, I I do think for what it's worth of things that I expect like to be hard for AI to to change is actually things about human connection, right? It's like that's something very interesting. I also think for what it's worth worth just like skilled trades like plumbers and electricians like I think it's they're already in short supply and I think it's going to be really difficult for us to go and build AIs that make dents in you know that's even able to add value in those domains. let's talk about codecs and other products that OpenAI releases. you know we're at a developer event where we have a room full of developers. You announced agent kit. h how should developers building on top of OpenAI think about potential platform risk? And I'm sure you think about this internally. I'm sure you've gotten this question before, but you know, I think",
        "start": 1803.279,
        "duration": 3812.6399999999967
    },
    {
        "text": "at a developer event where we have a room full of developers. You announced agent kit. h how should developers building on top of OpenAI think about potential platform risk? And I'm sure you think about this internally. I'm sure you've gotten this question before, but you know, I think day, there's a thousand startups that died. I don't believe that, but I I wanted to get your thoughts on or like what does that conversation internally look like? Where is that line being drawn between this is what we're building and this is what we provide a platform for others to build? Yeah, we get this question a lot. We think about it a lot and it's also very important to us. Like we want ultimately to help transition the world to this AI first economy, right? And have that be something that uplifts everyone and we can't do that alone. We just absolutely cannot, right? We really work with developers. We need people building on top of our platform figuring out how to connect this technology to the real world. And we have to pick, right? Because we are a company, you know, we're a few thousand people now. Sounds like a lot of people, but if you look at the size of the whole economy, it's tiny, right? You look at the number of domains, the amount of expertise required to do a good job in each one. So, we have to be very choosy. And what we really try to think about is what are domains that synergize and ones that we either have expertise or that we can really see you know, example coding, right? That's something where we know a lot about coding and also if we do a good job of coding, it speeds up our own work, right? Nice synergy right there. And so I'd say that we try to like really think about how to amplify as many people as possible and then go deep in specific domains that we feel like we can really add value to. Do you think code is the language of AGI? It's a great question. I honestly think that just natural language is going to be the language of of AGI. Like I think that they'll talk to each other might be like slightly more optimized English or something. , if you look at, for example, some of the the math proofs that we produced for the IMO this year where we got the gold medal, you can see it's like like the the proofs are actually quite readable. , and but they're they're like very tur and it's it's just like an interesting dialect that the AI has discovered. Are humans going to be in the loop for a while? I think as these models get better, I'm seeing kind of the middle to middle push outwards, but there still is room for humans currently on the beginning prompting in the end",
        "start": 1934.0,
        "duration": 4066.720999999997
    },
    {
        "text": "interesting dialect that the AI has discovered. Are humans going to be in the loop for a while? I think as these models get better, I'm seeing kind of the middle to middle push outwards, but there still is room for humans currently on the beginning prompting in the end the case? Is it going to be the case forever? Like how do you see that playing out? Well, I do think that the fundamental purpose of this technology is to benefit humans, right? And really not just humans, right? animals like just like all all living beings that can that can sort of you know experience pleasure and joy and I think that that is something where AI should be something that uplifts everyone and so the question is well what does that mean I I don't think that we want to be in a world where it's humans have to like craft a prompt and like write the code for context engineering like these are the mechanical details that to me feel legacy they feel like what computers were not what computers are supposed to be. And so what I want and what what I think that the world should want is AI tools that make the machine move closer to the human and understand what is it that are your goals and help you achieve your goals. And so I think that that to me is the real key. I think that we want to make sure that that we are uplifting humanity. That's the whole mission of Open AI and trying to move the technology in a direction that achieves that. Okay. , so it's for someone who thinks about coding often, you obviously spend a lot of time in codeex and thinking about building out what is, you know, the natural language coding language. , do you I actually asked you this in person a few months ago when I met you. Do you think that software is going to be fully generated at a certain point all the way down to the operating system level? Every single pixel on the screen that you see is going to be generated in real time assuming we can solve for consistency. I think so. I think it's gonna be super cool. Like it's actually a little bit mind-bending to think about what a fully generative UI would look like, right? Just like a real-time Sora like thing that you're like doing stuff like are there buttons? Are there not buttons? Like like what's the most natural thing? Like it makes you start to realize that probably a lot of the interfaces we build are around the proclivities or like sort of the the way our operating systems actually work right now. But if you could reimagine it from scratch, there's no legacy code. there's no concept of like files and folders and whatevers. What would it look like? And I don't feel like I",
        "start": 2063.76,
        "duration": 4328.879999999998
    },
    {
        "text": "around the proclivities or like sort of the the way our operating systems actually work right now. But if you could reimagine it from scratch, there's no legacy code. there's no concept of like files and folders and whatevers. What would it look like? And I don't feel like I guarantee it's going to be something like totally surprising. So, okay, let's let's envision that future a little bit. Are there still developers? Are there still applications in that world? It it just seems like probably not, but what am I missing? Well, take a look at something like Sora, right? It's like fully. And by the way, Sar is something that's really interesting for me because I I remember watching one of the one of the promo videos that we did where where you know, Bill's driving around on a snowmobile and he takes his helmet off and I was like, \"Wow, like Bill's like so good at snowmobiles and I'm like, wait a second, he did not do this, right?\" And you realize that that it's like exactly how the human is involved is just very different, right? It's very different from a movie where he actually was snowmmoiling, but he's still involved. Like he was thinking about the creative process of this. Like it's his cameo. It's like there's there's something about him that is present within this this video and you make a sore video with a cameo of someone. You're sharing it. You're excited about it. And the fact that you were excited about it actually is something that makes me excited about it. And we actually had this lesson from I like from earlier this year with the the when our image gen went viral, right? And everyone was generating these like, you know, portraits of themselves and their family. And the thing is we realized that if you just make a generated image, just some image that has no grounding in like being a picture of your dog now turned into, you know, some cool anime style, no one cares. It's boring, right? Just like it's like cool, whatever. But as soon as there's some humanity in it, as soon as there's some element of like that grounds it with this this connection, then people are really interested. And I think you can have this amplification of like it was like a picture of your, you know, of your of your kid and now there's like a ton of AI interesting stuff that happened and that that artifact is something that people connect to. But I guess that to to bring it back to software, I wonder if that will be the case too that people are going to build apps because it'll be this like you imagine how some sort of you know dynamic system could work or you know whatever the future of apps are. Wouldn't it be cool if you had something like you share in a certain",
        "start": 2197.92,
        "duration": 4564.560999999994
    },
    {
        "text": "the case too that people are going to build apps because it'll be this like you imagine how some sort of you know dynamic system could work or you know whatever the future of apps are. Wouldn't it be cool if you had something like you share in a certain developer and you outsource to them and they produce this like massive piece of code or you know fully generative UI but then you share on the catch app store perhaps. It really sounds like curating a great human experience or you know even a shorter word taste is going to be critically important in the future more so than any of these more hard skills like being able to actually develop the app. It's you just you know curating this experience. Is that is that what you believe? I think I believe a shape of that. Yeah, I think that's a good high orderer bit. I do think there are a bunch of mechanical skills that I think will transfer and we see that from generation to generation of models of people who really try to go and explore what the model's capable of somehow tend to get the best results reliably. But yeah, fundamentally knowing what you want, having good judgment and taste, I think those are critical. So you were a CTO of Stripe and now you just announced Aentic commerce protocol. Is that something that you thought about a long time ago or is this more of a recent discovery internally where you said, \"Hey, this is such a a cool thing that we can be doing allowing agents to be able to browse and then purchase on on our behalf is or is that something you thought of a while ago?\" I mean, all the thing about this field is there are no new ideas. All of these ideas are things that people have thought about, we've thought about many times. The thing that's new is models that are capable enough to actually make good use of them. And you can see this with plugins, right? We, you know, did plugins a couple years ago. The models just were not ready for it, right? That just like we couldn't have more than like three plugins active at a time because if you have too many functions, then the model gets confused, doesn't know how to call them. And just like today's models, infinitely more reliable than where we were before. And so I think it's like the thing that's new is the time, but not necessarily the idea. Okay. And do you shop through Chad GPT? I know Sam said he does. Well, so so the funny thing is I don't I don't do very very much shopping. So, I I'd say 100% of my shopping recently has been through ChachiBT. Okay. Okay. , let's talk about the future for a second. , I mean, I guess that's all",
        "start": 2316.96,
        "duration": 4807.920999999996
    },
    {
        "text": "said he does. Well, so so the funny thing is I don't I don't do very very much shopping. So, I I'd say 100% of my shopping recently has been through ChachiBT. Okay. Okay. , let's talk about the future for a second. , I mean, I guess that's all much has happened from last year's Dev Day, which was, I think, GPT40, and now we're a year later, and you you've released so many things. What does next year, 2026 dev day, look like, and then what does 2030 dev day look like? Tough questions. I mean I do think next year we will have some incredible models. the milestone I am most excited about is really having models that can solve hard problems, right? And the the analogy I like to make is think back to Alph Go, you know, 2016 move 37, right? That just like you know change people's understanding of the game and imagine that in coding, imagine that in material science. Imagine that in medicine. And so I think having real breakthroughs that are either potentially the AI by itself, but I think the AI assisted with top humans. , and I think we'll start seeing that. So what would be the utility of that for developers? I mean, it's almost indescribable. It's like for any sort of domain. It's like you want to help someone in finance and you can build like the most amazing application that can go solve their hardest finance problem. Like you know, maybe we won't be at the top finance problem, but I think we're going to be at like really hard problems. And by the way, it's going to be a lot of compute. So that's one thing is like we got to make sure that these tasks that people apply these things to are economically valuable because otherwise no one's going to be willing to to fund that compute. So you know maybe there will be some that that that we'll be able to do because they're you know something that people want to do for the world. and that's something that I think would be very aligned with our mission. But I think it'll just be like something that we're going to have to really think about how to bring this technology and this kind of breakthrough machine to life. 2030 I think is really hard to predict. Like I think that's within well past many people's AGI time time horizons at this point. What's your AGI time horizon? I have to ask you. Yeah, of course. Of course. And I mean, you know, it's all the usual like it's a fuzzy definition, whatever. But I think we're in the like 1 to three year time horizon. I think maybe more at the the the three than the one, but like I I would be kind of surprised. It would feel like something kind of went wrong if we were not there by 2030.",
        "start": 2439.76,
        "duration": 5049.840999999996
    },
    {
        "text": "But I think we're in the like 1 to three year time horizon. I think maybe more at the the the three than the one, but like I I would be kind of surprised. It would feel like something kind of went wrong if we were not there by 2030. of AGI where I can do most economically viable human work? What is your current definition? Has it changed? I do think the fundamental thing that has changed is that I really used to think of it as it's this destination. We're just building open AI to complete the mission. Yeah. But instead, we really think of it and I think I've sort of grown to maturity in how we think about it as this continuous process, right? And there's a particular point which is, you know, AI that can, you know, match humans at economically valuable work or whatever the definition that we used in 2018 is. And it's an important milestone, but it's not the end. And I think that this is really important because it's really about that follow-through, right? It's really thinking about, you can see people starting to shift from, okay, talking about AGI to super intelligence or people who reject all of these words altogether. To me, that's not the most important thing. To me, the important thing is saying, can you make AI progress? Can you uplift the whole economy? Can you actually deliver these benefits to people? And you need to think about what that means, right? You think about something with like Sora, right? You think about how we're approaching safety and chatbt. like these are important topics. They're very core to our mission and so we really try to think about the whole end to end. but yeah, I think that there's going to be a point there that we'll look back and say that that was basically what we're talking about in 2018. And I think it's not that far away. All right. everybody, Greg Brockman, thank you so much. Appreciate",
        "start": 2563.44,
        "duration": 5218.721
    }
]