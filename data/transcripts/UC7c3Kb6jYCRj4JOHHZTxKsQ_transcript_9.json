[
    {
        "text": " OpenAI just announced that they are partnering with Cerebrris on a three-year multi-billion dollar deal to deliver the fastest inference possible. There is so much to unpack with this story. I'm so excited to tell you about it. This whole thing starts with Gemini 3 and Google. Google in November of last year released Gemini 3. The model was incredible. It was absolutely frontier, topping a lot of the benchmarks. And what was unique about it is it was trained using TPUs, not Nvidia GPUs. And I'll explain the difference in a moment. So for the first time, we had an incredibly powerful model trained on something else than a GPU. And all the inference, that's when you ask it a question and the AI gives you an answer, was run on TPUs as well. And Nvidia started looking around thinking, \"Oh, this is actually a big deal. We're going to need to compete somehow. Our GPUs are great.\" and they're generalized, but they're definitely never going to be as fast as a specialized chip, especially at inference time. Fast forward to Christmas Eve, just a few weeks ago, and Nvidia announced that they acquired Grock. And the reason why I'm putting it in air quotes is because it wasn't a true acquisition in that they bought the whole company. Rather, everybody who was working for Grock basically quit, joined Nvidia. They did a $20 billion licensing deal for the technology. But the company of Grock still exists even though it's basically a shell of itself. And this was all to get around any antitrust regulation. So they got the deal done. Jensen basically admitted, yeah, there's something really special with specialized chips. And so what the entire industry realized and what probably Jensen knew a while ago is that the money is in inference. It is not in training these models. The models are trained once. It costs a large amount upfront to train the model, to bake the model, but then you serve it indefinitely. And being able to serve it very quickly and very inexpensively means that your margins as a frontier lab increases dramatically. Inference is where the revenue is. And as user demand increases, revenue increases. And that's not the case with training. Training is done once and it is basically a cost center. But once that model is done baking, once it's done training, you're just going to continue to serve it. And the more you serve it, the more ROI you get on that original training. And so Jensen dropped 20 bill on Grock. Then fast forward to today, OpenAI partners with Cerebrris. According to the Wall Street Journal, this deal is worth more than $10 billion. And they're agreeing to purchase 750 megawws of compute power over 3 years. So, a relatively short amount of time, but I can clearly see a future in which OpenAI acquires Cerebrris. And interestingly enough, we just had Andrew Feldman, the CEO of",
        "start": 0.16,
        "duration": 364.71999999999997
    },
    {
        "text": "deal is worth more than $10 billion. And they're agreeing to purchase 750 megawws of compute power over 3 years. So, a relatively short amount of time, but I can clearly see a future in which OpenAI acquires Cerebrris. And interestingly enough, we just had Andrew Feldman, the CEO of Friday, but he didn't spill the beans at all. Andrew, I'm a little upset that you didn't mention it. And so why did Sam Alman choose Cerebrris? Well, they couldn't have chosen Grock because Grock just went to Nvidia. And Sam Alman, who is widely known as a master dealmaker, probably thought, \"Wow, that's way too much dependency on Nvidia. We're getting all of our chips from them. We're buying inference from other companies built on top of Nvidia chips.\" And now, if we were to buy Grock inference, it's basically all going to a single company. way too much platform risk. Also, the Cerebrris chips are incredibly unique in many ways. One, they are the fastest chips on the planet. Check this out. This is GPTOSS. This is the open-source model from OpenAI. And as you can see here, Cerebrris absolutely dominating in terms of speed. The Cerebrris trips are much faster than anything else. In fact, even when we look at Grock right here, 465 tokens per second. And here's Cerebras at well over 3,000 tokens per second. And I'm going to talk about infant speed in a moment because I feel like I'm finally vindicated, having been a speed maxi for a while now. And one of the places you're going to be able to use some of this new increased speed is with using OpenAI models. And the sponsor of today's video, Zapier. With all of the models getting so insanely good, saturating all of the benchmarks, it probably feels like we can't get much better than we are getting right now. But as I've said, the key to unlocking the true value of AI lies in the tooling. That's why I'm excited to tell you about Zapier Agents. Zapier Agents gives you the best of everything. Frontier models, plus over 8,000 different apps, aka tools, that can easily be plugged into your agents. This is true AI orchestration. Plus, with a tight integration into Claude, Cursor, and Windsor via MCP, you can take your tools and agents anywhere you want. Gmail, Slack, Calendar, Notion, Docs, anything. It is dead simple to connect them to your agents and give them access to the tools they need. Try Zapier's AI orchestration platform for free today. The link is in the description below. They've been a fantastic partner. I've been using Zapier for years and I love it. Thanks again to Zapier. And now back to the video. Okay, so Cerebrus chips incredibly fast. Now we all know that, but they're special in another way. They bake everything, including memory, onto the wafer itself. And so while many chip makers and gamers seem to be affected by",
        "start": 182.239,
        "duration": 686.24
    },
    {
        "text": "it. Thanks again to Zapier. And now back to the video. Okay, so Cerebrus chips incredibly fast. Now we all know that, but they're special in another way. They bake everything, including memory, onto the wafer itself. And so while many chip makers and gamers seem to be affected by and by the way, that's why you're seeing RAM prices spike like crazy. Cerebras is not. They don't even use that type of memory. In fact, Andrew Feldman, the CEO of Cerebrris, just talked about this on the live show. Play the clip. You you've obviously seen memory prices just absolutely skyrocket and boom. I mean, it's it's it's insane and people can't get enough. How does that affect the kind of broader GPU market? But and then also like how does that affect Cerebrus? We don't use it. So, it benefits us, [laughter] right? I mean it if other people are paying more if other people are are waiting 6 n 12 or 15 months to get a part that is needed to make their GPUs valuable that is obviously a benefit to us. Okay so incredibly fast not affected by memory inventory and not overly dependent on Nvidia. And so imagine this chat GPT but 100 times faster maybe even more. Now, let me gloat for a second. I have been talking about how important speed is in the kind of speed, cost, quality equation for a while. In fact, I put out a poll on X a little under a year ago, and I asked, \"What's most important to you? Cost, quality, or speed?\" And quality was by far number one. In fact, speed was only about 2% of the votes if I remember correctly. But speed is incredibly important, especially for the coding use case. The most frustrating thing was submitting a task and just sitting there having to wait for your AI agent to complete that task. Now, it got much better when you could start queuing up tasks and running parallel agents. But really, the ultimate unlock was to have the AI running at, you know, 100 times speed. Then all of a sudden, you'd be able to iterate on your code so much more quickly. And in fact, that applies outside of code as well. It applies to every use case. There are very few use cases where I am okay just waiting for a response. The exception being deep research. If I want research done on a topic, I usually just kick off a task, deep research, and just let it go. And I'm okay with it coming back when it's ready. But almost everything else, I want the answer as quickly as possible. All right, enough gloating. What does this mean for the future? Well, we all know now specialized chips are going to play a big part in the AI life cycle. Generalized chips will probably still be used for a while on the training front,",
        "start": 345.12,
        "duration": 987.1989999999997
    },
    {
        "text": "the answer as quickly as possible. All right, enough gloating. What does this mean for the future? Well, we all know now specialized chips are going to play a big part in the AI life cycle. Generalized chips will probably still be used for a while on the training front, don't necessarily need an Nvidia chip to train major models, but these specialized chips have a major part in inference. And inference is where the money's at. And not only that, OpenAI is going to have more capacity. All of a sudden, they have all of this additional capacity on the inference side. So now they can dedicate all of the GPUs, the generalized chips to training. So that likely means we're going to actually get better models from OpenAI because right now they have a choice. They can either dedicate their GPUs to training the future model or they can dedicate their GPUs to serving the incredible demand that they have on the inference side. Of course, they need to make money. So they have chosen to invest those GPUs in inference. So we can expect better models in the future. And on the Cerebras front, they probably are now accelerating towards IPO. They did file for an IPO a while ago and they actually had to pull it. Then they recently raised a bunch of funding, but now I believe they're back in the process of trying to achieve an IPO. And this is going to be a huge boon towards that because now every frontier model lab is looking at Cerebrris and thinking oh I probably could use some of that capacity and open AAI has given the stamp of approval that yeah it is possible. So we are at an allout arms race for compute capacity. In fact, just a few months ago, I asked Greg Brockman directly about some of those trade-offs that they have to make internally while thinking about where are we going to invest our capacity resources. And I asked him, why don't you guys use Gro or Cerebras or other specialized chips? And I thought, oh, maybe they have to rewrite a lot of the software to make it work. And he told me, no, actually we don't. It's all the same. and he actually said something interesting about their conversations with companies like Grock and Cerebrris. Let me play that clip. Do you ever consider some of the newer players like a Cerebrris or a Grock? Like so in 2017 we got super excited when we saw Cerebrris because it was just this totally new paradigm. You looked at the numbers you're like wow like if we could have like a million of those things we could build AGI, right? It's just like you just realize it's a very different very different sort of platform. It's turned out that building nonGPU architectures has been way harder than we expected in 2017. , but from",
        "start": 498.319,
        "duration": 1277.5189999999998
    },
    {
        "text": "like if we could have like a million of those things we could build AGI, right? It's just like you just realize it's a very different very different sort of platform. It's turned out that building nonGPU architectures has been way harder than we expected in 2017. , but from the whole ecosystem. We tried to talk to all the different chip players and try to give them some advice, talk about here's what the shape of the workload is. And honestly, most of the companies wouldn't listen to us. To some extent, it wasn't even that they thought we were wrong, but that if you have people who come from the chip world and have a particular way of looking at the problem and don't understand the workload, and you try to say, \"No, no, no. this perspective is backwards, right? That you really need to think about it in this other way that it's going to be about big models, not, you know, small models, whatever. Those kinds of design inputs. If you don't buy into it, it's very hard to then rebase your whole worldview on top of it. And so, I think that what's been what's really distinguished the successful players in the space has been people who bring in people with a deep learning perspective or really try to pay attention to where the workloads are going. So, I couldn't quite understand why Frontier Labs weren't going to companies like Grock and Cerebrris and using their capacity, purchasing their chips or buying their inference services. And it turns out they're kind of getting the message now. Nvidia Bach Grock, OpenAI's partnering with Cerebrris. And so, when we look at OpenAI, the only limiting factor they have for their revenue is their capacity. And so, they're looking for it any possible place they can find it. And all of this ultimately benefits us, the users of these systems. If you enjoyed this video, please consider giving a like and subscribe. and I'll see you in the next",
        "start": 645.839,
        "duration": 1449.9189999999999
    }
]