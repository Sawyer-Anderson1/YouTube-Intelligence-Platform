[
    {
        "text": " This month, an AI has gone completely off the rails. A new humanoid robot with synthetic skin looks disturbingly real. While Unit's latest upgrade, might have officially hit human level ability. Microsoft dropped Cosmos, an AI reaching 80% of human performance. And Google answered with a world-aware agent that's shockingly close to real intelligence. Meanwhile, GPT 5.1 and Gemini 3 releases broke the internet, and Google's new Nano Banana 2 Pro just blurred the line between reality and AI. So, let's break it all down. Let's start with the one that shocked the research world, Cosmos. It's backed by Microsoft researchers, and it's basically the first real AI scientist that actually does science from start to finish. You give it a goal in a data set, say brain scans, genetics data, or some complex material science problem, and it locks in for 12 straight hours. During that time, it reads over 1,500 research papers, writes around 40,000 lines of Python code, runs analysis, tests hypotheses, and then generates a full research report complete with citations and executable code. No humans guiding it midway, just pure autonomous scientific work. In early trials, Cosmos actually made new discoveries across biology, neuroscience, and clean energy materials. One of its experiments uncovered how cooling protects the brain. When temperature drops, brain cells switch into an energy saving mode, recycling molecules instead of creating new ones. Another project revealed that humidity beyond a certain threshold destroys Paravskite solar cells during production. A real world factor that later got confirmed by human researchers. Cosmos even found a shared mathematical rule for how neurons connect across species, humans, mice, flies, showing that all brains might follow the same wiring pattern. And the discoveries just kept coming. It pinpointed a heartprotecting protein called S OD2, showing how it prevents scarring. then traced a DNA variant that helps people resist diabetes by regulating a stress response or gene in insulin cells. It even mapped out the exact moment brain cells begin collapsing in Alzheimer's and explained why some neurons age faster than others, linking it to lost flipase genes that trigger the brain's immune system to destroy them. Now, what makes Cosmos so capable is the way it's structured. It runs hundreds of smaller AI agents, each handling a part of the process. Some read and summarize papers, others analyze data, others write code. They all share a single internal memory called a world model. That world model keeps track of everything. What's already done, what worked, and what needs testing next. It's like a brain made of subbrains, planning long investigations without losing context. When independent scientists reviewed Cosmos reports, they found that nearly 80% of its scientific statements were accurate. An insane rate for something running completely on its own. One 12-hour session of Cosmos produced about the same amount of work as 6 months of human research time. The reports looked like early stage academic papers, complete with graphs, statistics, and",
        "start": 2.48,
        "duration": 392.8000000000002
    },
    {
        "text": "nearly 80% of its scientific statements were accurate. An insane rate for something running completely on its own. One 12-hour session of Cosmos produced about the same amount of work as 6 months of human research time. The reports looked like early stage academic papers, complete with graphs, statistics, and reading the work of a junior researcher, except this one can run hundreds of experiments overnight and never get tired. Of course, it still needs humans in the loop. The best results came from a scientist in the loop setup. Humans define the goal and data. Cosmos does the deep grind and humans check what's worth pursuing. It struggles with messy or unlabeled data sets, and it can't yet handle raw images or files larger than about 5 GB. It also can't take midexperiment instructions. Once it starts a 12-hour run, it goes allin. But the bigger limitation isn't computing power, it's judgment. Teaching Cosmos how to recognize which ideas are meaningful, not just statistically correct. Still, this marks a turning point. AI is conducting real research with measurable outcomes. All right. Now, Abacus AI just pushed AI agents into a new lane with something surprisingly powerful. Deep agent with full browser use. And this version doesn't just browse when you ask. It can schedule its own actions. Open a real browser, click through tasks, finish the job, and deliver results automatically at the exact time you choose. That level of hands-off automation puts it far ahead of like Perplexity Comet or OpenAI's Atlas. Deep Agent sits inside Abacus AI's chat LLM ecosystem, combining reasoning, browser control, and scheduling. Instead of simply summarizing a web page, it performs actions directly on the page. Ask it to find business leads on LinkedIn, and it literally opens LinkedIn, searches, collects the leads, and even drafts personalized outreach emails. You can then schedule the same workflow to run every morning at 9:00 a.m. No prompting. It wakes up, does the job, and emails you the results. That's the real advantage. Persistent recurring browser automation. You can have it check your website for errors, scrape data, post on X, download invoices, test login, or apply for jobs, all on a schedule. Creating a task is simple. Open your Abacus dashboard, start a Deep Agent task, toggle browser use, and describe what you need. Deep Agent handles the full workflow from navigation to reporting. It shines for business tasks. Tell it to find 10 to 15 blogs in the luxury accessories niche. Collect contact details and write outreach messages. Seconds later, it's browsing, checking relevance, building a spreadsheet, and generating tailored email templates. And if you want fresh leads every day, you just turn on scheduling, and let it run. Even complex workflows like automated job applications work flawlessly. Upload your resume, connect a sheet of job URLs, and Deep Agent opens each listing, fills the forms, answers questions, and logs details like role, company, salary, and posting date. You can even run it",
        "start": 198.8,
        "duration": 722.0800000000003
    },
    {
        "text": "on scheduling, and let it run. Even complex workflows like automated job applications work flawlessly. Upload your resume, connect a sheet of job URLs, and Deep Agent opens each listing, fills the forms, answers questions, and logs details like role, company, salary, and posting date. You can even run it You can watch the AI move the cursor and click through pages. Every result is saved, structured, and attached to email reports. And because it's powered by Chat LLM, you can switch between Claude, Gemini, GPT, or Grock depending on the task. Deep agent also pairs with the Deep Agent listener, a desktop companion that transcribes meetings, summarizes content on your screen, reads documents, and creates follow-up actions. Together, they act like a full AI employee. At $10 per month, the value is obvious. If you've been waiting for an AI that actually performs real tasks, this is the one. And speaking of anthropic, there's new competition coming straight out of China. Moonshot AI just released Kimi K2 thinking, and it's openly challenging the reasoning models from both OpenAI and Anthropic. The company calls it their best open-source thinking model. And what makes it interesting is that it doesn't just generate text. It thinks stepby step, uses external tools, executes code, and reasons across hundreds of sequential steps completely on its own. Technically, it's impressive. K2 thinking scored 40.9% on humanity's last exam. That's a benchmark with thousands of expert level questions spanning over a 100 disciplines. On browse comp, which tests continuous browsing and research ability, it hit 60.2% double the human bench bench line of 29.2. And on SWEV verified, a major coding benchmark, it scored 71.3%. What's even crazier is its ability to execute up to 300 sequential tool calls without human input. That means it can chain hundreds of reasoning steps, reading, planning, searching, coding, verifying until it reaches a solid conclusion. Moonshot AI showed this off by giving it a PhD level math problem from hyperbolic geometry. K2 thinking went through 23 nested reasoning and tool calls, searched papers, executed Python code, verified intermediate results, and derived the correct formula. That's the kind of multi-step planning even top commercial models struggle with. In real use, it can build full websites or Word style editors from a single prompt, doing complex front-end work like React components. It can also run research tasks that involve multiple moving parts. One example had it identify an actor Jimmy Garry Jr. from a vague multiclu description involving a university degree NFL career and film roles. The model ran more than 20 searches checked sources like Wikipedia and IMDb and assembled a coherent answer. This ability for long horizon reasoning thinking across hundreds of steps is quickly becoming the next frontier in AI. Moonshot AI is betting on open- source as its edge while US labs keep their reasoning models proprietary. They're also exploring test time scaling, which basically means giving the model more reasoning tokens",
        "start": 365.52,
        "duration": 1083.6809999999994
    },
    {
        "text": "for long horizon reasoning thinking across hundreds of steps is quickly becoming the next frontier in AI. Moonshot AI is betting on open- source as its edge while US labs keep their reasoning models proprietary. They're also exploring test time scaling, which basically means giving the model more reasoning tokens battlefield. How long and coherently an AI can plan before it loses focus. Then there's Google and they're taking a completely different approach with something called DSTAR. If Cosmos is an AI scientist, DSAR is an AI data scientist, a system that can take messy business data and turn it into working Python code all by itself. Most AI data tools today work with clean SQL databases, but DSTAR was built for chaos. CSVs, JSON logs, markdown files, text reports, random spreadsheets. You ask it a question in plain English like which products performed best in Q3 based on sales and reviews and it figures out where that data lives, writes Python code to combine it, tests the results, fixes its own bugs, and returns the answer. No human analyst needed. Underneath it runs a swarm of six specialized agents. One scans and summarizes every file, noting column names, data types, and snippets. Another plans the steps. Another writes the Python code. Then there's a verifier that checks if the code worked. A router that decides what to do if it didn't. And a finalizer that formats the output. This loop can repeat up to 20 times for each task. Meaning DSAR keeps rewriting and testing until the result makes sense. And when the code crashes, missing columns, wrong types, bad joints, an A debugger module automatically repairs it. It studies the error logs, reanalyzes the files, and patches the script. Meanwhile, a retriever module uses Google's Gemini embedding 001 to rank the most relevant files, so it doesn't waste time on irrelevant data. It only pulls the top 100 files into context, keeping everything efficient. The whole system runs on Gemini 2.5 Pro, Google's most advanced model for reasoning and code generation. On its own, Gemini 2.5 Pro performs well, but paired with DSAR's multi-agent structure, the results skyrocket on DABEP, a tough benchmark for data analysis reasoning, Gemini alone scored 12.7% on the hardest tasks. With DStar, that jumped to 45.24%. On Chroma Bench, which tests file retrieval from huge data lakes, it scored 44.7% compared to 39.8 for the best competing system. On DA Code, it reached 37.1% versus 32% for the next best. That's a massive 30point leap in real world data performance. What makes DSAR so important is its independence from perfect data. Most enterprise information is messy, scattered across cloud drives, shared folders, random Excel sheets. DSAR reads and adapts to that chaos like a human analyst, but faster and with its own self-debugging loop. It's also model agnostic, meaning it can plug into Gemini GPT5 or Claude 4.5. The architecture stays the same. Tests showed the real advantage isn't",
        "start": 548.64,
        "duration": 1471.121
    },
    {
        "text": "drives, shared folders, random Excel sheets. DSAR reads and adapts to that chaos like a human analyst, but faster and with its own self-debugging loop. It's also model agnostic, meaning it can plug into Gemini GPT5 or Claude 4.5. The architecture stays the same. Tests showed the real advantage isn't self-correcting process itself. Xpang unveiled a shockingly lifelike humanoid robot wrapped in flexible synthetic skin built with a biomimetic spine and muscle system, a curved display in its head, and customizable design options that even include different body types and colors. Unitry introduced its new G1 embodied avatar, a robot that mirrors human motion in real time and learns everyday tasks like cleaning, organizing, and even handling objects with human level precision. And that viral robot kidnapping story from last year where a bot led 12 others out of a showroom just got new evidence confirming parts of it were actually real. Now, let's start with the biggest one, cuz Xpang's new humanoid is unlike anything we've seen before. The same company known for electric cars and flying vehicles is now doubling down on humanoid robots. During their AI day in Guanghou, they unveiled the newest generation of their Iron Humanoid. And this one's completely different from what anyone expected. So, Xpen calls this the eighth generation of their robotics program and the third with a humanoid design. They're aiming for mass production by late 2026. And this model feels like a direct statement to the rest of the industry. Instead of the usual industrial or mechanical look, Xpang's robot has a full body, synthetic skin, customizable body types, and even options for hairstyles and clothing. Seriously, users will be able to pick from body types like athletic, chubby, tall, or short. choose different hair designs and later even change its wardrobe. The company says the synthetic skin makes it feel warmer and more intimate, clearly trying to move beyond the cold metallic stereotype of robots. But that's not all. The robot has a biomimetic spine and muscle system that mimics human motion. You can see the way it bends and twists almost like a real person. Inside the head, there's a 3D curved display built into the face, giving it more expressive capabilities. Its shoulders move like human joints, and each hand comes with 22 degrees of freedom, which means it can handle delicate tasks like picking up small objects or gesturing naturally. Xpang loaded it with serious computing power, too. Three Turing AI chips that deliver a total of 2,250 tops. For context, that's the same class of power found in their autonomous cars. So, it's no surprise that the robot runs on Xpang's own VLT, VLA, and VLM systems. Those stand for vision, language, transformer, vision, language, action, and vision language model. Basically, a full stack AI architecture that allows the robot to see, interpret, and act in real time. So, it's not just following pre-coded commands. It's processing the world visually, and linguistically while it",
        "start": 744.72,
        "duration": 1806.6789999999999
    },
    {
        "text": "and VLM systems. Those stand for vision, language, transformer, vision, language, action, and vision language model. Basically, a full stack AI architecture that allows the robot to see, interpret, and act in real time. So, it's not just following pre-coded commands. It's processing the world visually, and linguistically while it Xpang's not going after the same use cases as companies like 1X or Figure. CEO Hi Ziaoong said straight up that humanoids aren't actually great for factory work or repetitive tasks. Instead, Xpang wants its robot to work in social spaces as a receptionist, tour guide, or shopping assistant. In fact, the previous generation of this robot was already giving tours at Xpang's headquarters, speaking in a perfect American accent while walking visitors through the building. This next version, though, will take over those roles completely. The company's plan is to deploy the new iron robots in showrooms, museums, and shopping centers where they'll interact directly with people. There's also a partnership with Bea Steel, one of China's biggest steel producers, where the robots will perform inspections and other realworld trials. So, Xpang's clearly thinking beyond labs and show floors. They want this thing working out in public. And they're not doing it halfway either. The robot will be powered by a solid state battery, something we rarely see in humanoids, which makes it lighter and longerlasting than lithium packs. Heiz described their approach as fusion and invention. In his words, cars require integration and invention, but robotics requires fusion, meaning the software and hardware have to evolve together. He said, \"A robot's hardware must literally be designed around its AI brain for it to work properly.\" Which is why Xpang insists on doing everything inhouse. Full stack development from chips to algorithms ensures that when mass production starts, the robot won't just move, it'll move intelligently. There's also a safety and ethics angle baked in. The CEO mentioned that the robot follows Isaac Azimoff's three universal laws of robotics. And then they added a fourth. It can't disclose its owner's data. It's an interesting line to draw, especially when competitors like 1X are asking customers to give them full access to their homes so their robots can learn. Xpen's clearly taking a different stance, keeping data privacy front and center, while making the robot more personal, almost companion-like. Now, while the robot looks impressive, not everyone's buying the idea. Critics say Xpen might be solving a problem that doesn't exist, questioning the need for such a human-like machine that comes with so many customization options. The company hasn't released a price yet, but with that kind of tech, three high-end AI chips, solidstate batteries, and all that articulation, it's not going to be cheap. Still, the combination of personality, design, and in-house AI could set Xpang apart from the usual utility focused robotics we've seen so far. And let's be real, that walking demo at AI day, even though it ended with a slightly awkward dance to Taylor",
        "start": 916.959,
        "duration": 2149.958
    },
    {
        "text": "it's not going to be cheap. Still, the combination of personality, design, and in-house AI could set Xpang apart from the usual utility focused robotics we've seen so far. And let's be real, that walking demo at AI day, even though it ended with a slightly awkward dance to Taylor definitely got people talking. Some even asked if there was a real person inside it. So, while the world debates whether we need humanoids with synthetic skin and customizable body shapes, Xpang's already moving ahead, building one of the most ambitious full stack robots in China and maybe the world. Meanwhile, another Chinese company, Unirit, is heading in a completely different direction with what they call the embodied avatar. Their latest demo honestly blew up online. It starts off like a typical robot video. a Unitry G1 kicks a soccer ball. Nothing crazy. But then you realize it's not running on pre-coded routines. There's a human wearing a motion suit and the robot is mirroring his every movement in real time. The operator swings a staff in martial arts moves and the G1 mirrors it perfectly. And then they push it further. Two teley operators, each controlling their own G1, step into what's basically a robot sparring match, throwing punches. It looks like the future of human robot connection, and people online immediately picked up on the potential. Some joked, \"This is how you'll skip the gym, send your robot to train for you.\" Others called it the birth of embodied robot combat. But the real innovation comes next. In the next scene, there's a note on the screen that says, \"Realtime learning, full body movements from videos, and the robot shifts from TA operation to true embodied learning.\" Clips show the G1 in a home environment. It's quietly moving around an apartment, cleaning up, wiping counters, taking out the trash, even fluffing a pillow, and placing a Coke in the fridge. These movements aren't clunky or robotic anymore. They're smooth, intentional, and clearly learned from human teleoperation data. Every action refineses its motor control. The more people operate G1 units, the more data they collect. Real human dexterity captured and turned into machine learning fuel. Unitry calls this a fullbody teloperation and data acquisition platform. But what it really is is a long-term strategy. They're teaching the robot to move like us. So one day it won't need us to move at all. That's the big difference between what Unitary is doing and what Xpang is doing. Xpang wants emotionally intelligent humanlike service robots. Unit's training mechanical avatars, machines that learn our motion directly, whether it's for home work, combat training, or even remote labor. It's one of those projects that feels like the groundwork for something much larger. Now, speaking of Chinese humanoids, that old robot kidnapping story just took a new turn. Remember the viral clip from last year? The small robot named Herb that supposedly led 12 others out of a",
        "start": 1091.039,
        "duration": 2499.957999999999
    },
    {
        "text": "labor. It's one of those projects that feels like the groundwork for something much larger. Now, speaking of Chinese humanoids, that old robot kidnapping story just took a new turn. Remember the viral clip from last year? The small robot named Herb that supposedly led 12 others out of a new info from Chinese sources finally confirmed parts of it were real. Turns out the robot itself actually exists. developed years ago as an intelligent companion prototype for a woman named Highway, who still posts videos of it on Doian. She'd been showing Urbi interacting with pets and talking for years before that clip went viral. Investigators now traced the footage back to a real showroom belonging to Kenchi Robotics, whose small humanoids match the one seen in the video. Over the last few days, humanoid robotics basically split into two timelines at the same time. One side shows China pushing out breakthroughs so fast it almost feels unreal. Mind on turning the unitry G1 into a full environment worker navigating homes like it actually lives there. Unitry rolling out a wheeled G1D built for non-stop industrial speed. And UB shipping hundreds of Walker S2 humanoids in what looks like the first real mass deployment in the entire industry. And then right in the middle of all that progress, Russia tried to introduce its first AI powered humanoid on stage and the robot collapsed in front of everyone. Add the chaos from Brett Adcock calling out fake robots, predicting Agility Robotics will go bankrupt, and starting a full-blown feud on X, and you get one of the most explosive weeks this space has seen all year. So, let's talk about it. All right, so everything basically starts with Mindon and the Unit G1. This robot has always been a strong platform on paper with its 23\u00b0 of freedom, depth cameras, 3D LAR, hybrid force position control, and a mechanical structure built for dynamic movement. People already knew it could run stable demos in a lab. The shock came from how Mindon decided to train it. Instead of the usual rehearsed motion loops that humanoids tend to repeat for cameras, they drop the G1 into what looks like a fully functioning home, and let it handle an entire chain of tasks without cutting to a new scene every 5 seconds. You watch it walk up to a window, align the hands, open the curtains in a smooth, continuous motion that does not feel robotic at all. Then it shifts into checking plants, adjusting its grip so the leaves are not crushed, and watering them without spilling anything. After that, the G1 transitions into office-like tasks, moving light packages with enough precision to keep balance steady while still planning foot placement for each step. The textile part is probably the most impressive because humanoids have historically struggled with soft materials. But here, the G1 pulls sheets off a bed, shakes them out, lays them flat, and navigates around furniture with movements that",
        "start": 1268.0,
        "duration": 2811.715999999998
    },
    {
        "text": "to keep balance steady while still planning foot placement for each step. The textile part is probably the most impressive because humanoids have historically struggled with soft materials. But here, the G1 pulls sheets off a bed, shakes them out, lays them flat, and navigates around furniture with movements that There is no jerky stiffness, no torque spikes, no hesitation. Even when it interacts with kids in the demo, the movements stay inside predictable limits, which hints at a safety focus control layer specifically tuned for soft interactions. Mindon still has not released a full technical breakdown of their training pipeline. But based on the sensory setup and the smooth transitions between tasks, it looks like they leaned heavily into environmental generalization instead of memorizing one-off tricks. That approach makes sense because a real home is basically unpredictable and if a robot hesitates on every micro variation, it becomes unusable. What Mindon showed here looks like one of the closest examples so far of a humanoid functioning inside a real space without breaking the illusion that it belongs there. This whole moment also pushes the conversation into the realworld challenges that decide whether humanoids actually make it into mainstream use. Battery life needs to last long enough for a full work session. Reliability has to hold up across hundreds of repetitions. The robot needs to behave safely around children or elderly people. Maintenance cycles must stay affordable and the price point has to land in a range that normal households or small businesses can justify. Mindon's demo does not answer every one of those questions, but it does show a direction where the promises finally start matching what consumers actually need instead of what companies like to advertise. While that whole thing was going viral, Unitry dropped a completely different kind of announcement. The G1D, their first wheeled humanoid designed for fast, repetitive tasks. This thing is built for industrial zones, warehouses, service environments, retail spaces anywhere companies want a robot that can run non-stop without the limitations of bipeedal walking. The standard version stands in place, while the flagship version rides on a wheeled base using differential drive. That mobile version can hit 1.5 meters per second, which is about five feet per second, making it fast enough to actually keep up with real logistics workflows. Both versions range between 49.5 and 66 in in height and weigh up to around 80 kg, roughly 176 lb. The standard version carries 17\u00b0 of freedom, while the mobile version has 19, not counting the aectors. The arms are identical across the lineup, each with seven degrees of freedom. and the ability to lift about 3 kg or 6.6 lb. The waist joint moves along the Z-axis up to 155\u00b0 and along the Y-axis from about -2.5 to 135\u00b0, giving it a strong vertical range of motion that reaches up to around 2 m. Vision is a big part of what they are doing with the G1D. You",
        "start": 1428.0,
        "duration": 3126.116999999997
    },
    {
        "text": "6.6 lb. The waist joint moves along the Z-axis up to 155\u00b0 and along the Y-axis from about -2.5 to 135\u00b0, giving it a strong vertical range of motion that reaches up to around 2 m. Vision is a big part of what they are doing with the G1D. You the head and then two extra cameras mounted on the wrists. This setup lets the robot analyze objects from multiple angles. handle inspections and carry out repetitive manual tasks without drifting off target. Theectors are interchangeable with two-finger grippers, three-finger manipulators with or without touch sensors, and a five-finger dextrous hand so the robot can switch between industrial tasks and more delicate actions. The flagship model also includes an NVIDIA Jetson or an NX pushing around 100 tops of AI compute, which is plenty for onboard real-time decision-making. Battery life goes up to 6 hours of autonomous operation, giving it enough uptime for full shifts in warehouses that need fast cycles. Unitry also released a full software platform alongside the hardware. It handles data collection, annotation, task management, simulation environments, distributed training and deployment. Trained models can be exported into the robot with almost no extra integration work. This is part of a bigger strategy where they want the G1D to not just run tasks, but also act as a data generation tool for companies that want to build new AI pipelines around realworld performance. Now, while China was showing off polished demos and industrial rollouts, Russia somehow delivered the exact opposite energy. Their first humanoid robot with artificial intelligence called A Idol fell straight to the floor during its official debut in Moscow. This happened on November 10 while staff members were guiding it on stage to the Rocky soundtrack. The robot lost balance, collapsed, and even left pieces on the floor. Immediately after the fall, the staff rushed to cover it behind a screen while dragging it off stage. Video of the whole thing spread across Russian tech forums and social media, turning the unveiling into a spectacle for all the wrong reasons. Adol is supposed to represent Russia's attempt to enter the global humanoid race using mostly domestic components. The developers claim it integrates movement, object manipulation, and human-like interaction using embodied artificial intelligence. According to the company, the fall was caused by calibration issues, and the robot is still in its testing phase. They said the machine runs on a 48vt battery that gives it up to 6 hours of operation, and that about 77% of its components are Russianmade with plans to push that number to 93%. They also highlighted its 19 servo motors and the ability to display more than a dozen basic emotions along with hundreds of micro expressions, all supported by silicone skin engineered to mimic human facial movements. During the presentation, CEO Vladimir Vitukin said the robot can smile, think, and look surprised, similar to a person. But after the fall, critics focused mostly on its instability and the decision to",
        "start": 1587.12,
        "duration": 3444.037999999998
    },
    {
        "text": "along with hundreds of micro expressions, all supported by silicone skin engineered to mimic human facial movements. During the presentation, CEO Vladimir Vitukin said the robot can smile, think, and look surprised, similar to a person. But after the fall, critics focused mostly on its instability and the decision to public event. Right after this somewhat chaotic moment from Russia, China delivered another milestone. This time from UB Tech. The company confirmed that hundreds of Walker S two humanoid robots have been shipped to real industrial facilities. This is one of the first moments where mass deployment of humanoids looks like an actual thing instead of a marketing slide. UB said production scaled up in mid- November, and the first batch already reached their partners. Demand this year alone hit around $113 million. orders came from companies that want stable, non-stop labor on their assembly lines. Some of the major deals include a roughly $35.3 million order from a well-known Chinese firm in September, another $22.45 million agreement in Sichuan, a $17.8 million project in Guangshi, and over $14.1 million from Miato in Hube. UB expects to send out around 500 walkers by the end of December and claims it is on schedule. Automakers appear to be pushing this surge more than anyone else. BYD, Gil Auto, Val Volkswagen, Dongfang, Lujo Motor, and even Foxcon are adding these robots to support logistics and non-stop operations. Early tests show strong performance in both factories and warehouses. One of the standout features of the Walker S2 is its battery swapping system. The robot can remove and replace its own power pack within minutes without any human help. This single upgrade removes downtime almost completely, making it way more practical for long industrial shifts. The Walker S2 is tall, sturdy, and designed to handle heavy objects while still keeping precise finger control. UB says humanoids now make up about 30% of their sales, up from 10% last year, which they take as proof that demand is coming from real world needs instead of hype. Financial results reflect all of this. In the first half of 2025, UBC pulled in around $87.7 million in revenue, a 27.5% increase from the previous year. Gross profit hit about 30.6 million, up 17.3% and losses narrowed by roughly $62.1 million. Their stock climbed more than 150% this year to 133 Hong Kong dollars. and analysts at City and JP Morgan still list the stock as a buy with expectations that it could climb past 170 Hong Kong dollars. UB was the first robotics company to trade on the Hong Kong exchange back in 2023. And the recent momentum suggests they are strengthening their lead. Even with this large-scale rollout happening, a big wave of skepticism hit the scene when Brett Adcock, the CEO of Figure, commented on a video that allegedly showed UB Tech delivering hundreds of robots. He hinted that some of the robots looked fake, pointing out inconsistencies and raising questions",
        "start": 1748.32,
        "duration": 3767.6369999999974
    },
    {
        "text": "lead. Even with this large-scale rollout happening, a big wave of skepticism hit the scene when Brett Adcock, the CEO of Figure, commented on a video that allegedly showed UB Tech delivering hundreds of robots. He hinted that some of the robots looked fake, pointing out inconsistencies and raising questions operating or just staged. This connects to a broader conversation in robotics where companies release polished videos, but the realworld performance sometimes does not match the footage. Adcock has been vocal about the challenges of true generalpurpose robotics and about the gap between marketing videos and actual deployments. That whole thing spilled directly into a heated online feud between Adcock and Agility Robotics. After Adcock posted that a figure humanoid had been working on BMW's production line for five consecutive months straight, Agility responded with a sarcastic comment comparing his claim to someone saying they invented lemon water. Adcock fired back by saying Agility would be bankrupt in under a year. The argument spread quickly with other industry figures jumping in. A VP from 1X commented that kindness matters more than aggression. Agility responded again with a Ted Lasso GIF and a message saying they will check back next November. They also added that despite the drama, they appreciate the work of all teams pushing the field forward. Adcock ended it with a Sopranos meme captioned with, \"Next time you come in, you come heavy or not at all.\" It is a strange moment, but it shows how intense the competition has gotten as billions of dollars and the future of general purpose robotics sit on the line. Open AAI and Google are going head-to-head again, and this time it's a full-on showdown. OpenAI's leaked GPT 5.1 thinking model looks ready to take on Google's upcoming Gemini 3 Pro, while Google is also cooking up Nano Banana 2, a nextg image generator built on its Gemini tech. And what's crazy is that both could drop within days of each other. So, let's talk about it. All right, let's start with OpenAI. hidden deep in the chat GPT backend. Developers recently spotted traces of something called GPT5.1 thinking and it looks like a completely new kind of model, not just a speed or parameter upgrade. The leak came from internal code showing GPT51 thinking listed next to other chat GPT variants, which basically confirms OpenAI's next major release is already being tested behind the scenes. What's interesting is that this thinking model isn't about writing faster replies. It's about reasoning, taking its time to think through complex tasks the way humans do. Early indicators suggest this version could use multi-step reasoning, meaning it breaks down your prompt into smaller parts before forming a complete answer. It might also introduce something called thinking budgets, where the model allocates more computation or time to difficult problems, just like a person would pause before answering a tough question. And there's strong speculation that Open AI tuned this model for better context handling,",
        "start": 1912.08,
        "duration": 4095.2379999999944
    },
    {
        "text": "parts before forming a complete answer. It might also introduce something called thinking budgets, where the model allocates more computation or time to difficult problems, just like a person would pause before answering a tough question. And there's strong speculation that Open AI tuned this model for better context handling, more clarity, similar to how Anthropics Claude recently improved its chain of thought reasoning. So, while the thinking label sounds a bit theatrical, it could be OpenAI's most deliberate step toward cognitive depth. Instead of trying to be the fastest, it's aiming to be the most thoughtful. And the timing of this leak couldn't be more strategic. Google is right on the verge of releasing Gemini 3 Pro, the next major model in its lineup. And OpenAI's move seems almost designed to intercept the spotlight. Gemini 3 Pro is expected to feature a massive 1 million token context window that's long enough to process entire books, code bases, or large projects in one go. So, while Google bets on scale and memory, OpenAI's response seems to be depth and reasoning power. Two very different directions, both trying to solve the same core problem, making AI feel more human in how it thinks and reacts. The GPT 5.1 label itself isn't just speculation anymore. References to multiple new versions of GPT 5.1, GPT 5.1 Reasoning, and GPT 5.1 Pro have been found directly in Open AI's own codebase. Even more interesting, enterprise logs reveal that larger organizations will soon have control over which models they deploy, letting them opt out of experimental releases. This is big for companies running AI in production since they'll finally be able to freeze stable versions without worrying about sudden upgrades breaking workflows. The same code points to a November 24th rollout date for the GPT 5.1 family. And while that's probably targeted at enterprise customers first, insiders think individual plus and pro users could see it even earlier. That would put OpenAI's launch window right in the same period as Google's Gemini 3 Pro almost like a coordinated duel. Another hint came from the open- source community. On Open Router, a mysterious model named Polaris Alpha started showing up and users quickly realized it behaves far beyond GPT4 class models. Many now believe it's actually GPT 5.1 thinking in disguise. The model performs exceptionally well in creative writing and benchmark reasoning test, which adds weight to the theory. If that's true, public access to GPT 5.1 might already have started quietly. For OpenAI, this roll out fits its new pattern. Instead of one giant reveal, they're moving toward incremental but visible improvements segmented by purpose. There'll likely be smaller mini models for speed, thinking models for reasoning, and pro models tuned for enterprisegrade reliability. The interesting part is how these models may behave differently depending on the type of task you give them. You could have one model that responds instantly for chat while another takes longer but delivers a more structured, deliberate analysis.",
        "start": 2077.28,
        "duration": 4452.757999999995
    },
    {
        "text": "models for reasoning, and pro models tuned for enterprisegrade reliability. The interesting part is how these models may behave differently depending on the type of task you give them. You could have one model that responds instantly for chat while another takes longer but delivers a more structured, deliberate analysis. lined up. New leaks confirm that Gemini 3 Pro and another model named Nano Banana 2 are in the final testing stages, both expected to roll out around November and December. Gemini 3 Pro recently appeared on Vertex AI, Google's cloud platform, under the label Gemini 3 Pro Preview1 2025, lining up perfectly with the company's usual release cadence. While Gemini 2.5 Pro, the current version, is only about 8 months old, it's still one of the strongest large models in use, especially for coding. On the SWE verified benchmark, it scores 63.8% using a custom agent setup. That's below Claude Sonnet 4.5 at around 77%, but it shows how tight the competition is. Demini 3 Pro is expected to push those numbers significantly higher, especially with its larger context window and multimodal reasoning features. But the bigger surprise is Nano Banana 2, Google's new generation of AI image generation technology. Internally known as Geix 2, it's built directly on the Gemini 3 Pro image model, the same architecture that will power Gemini's upcoming visual features. The original Nano Banana was a cultural moment earlier this year. It let users turn selfies into glossy 3D style portraits and it exploded in popularity inside the Gemini app. That single feature alone brought over 10 million new users within weeks and actually helped Gemini surpass Chat GPT's download numbers for the first time. Even Nvidia CEO Jensen Hang called it a breakthrough in user creativity and joked that he'd gone nano bananas playing with it. Now the sequel is aiming way higher. Nanabanana 2 will support native 2K renders with 4K upscaling, giving professionalgrade quality right from a phone. Prompt accuracy is also getting a big boost thanks to Gemini 3 Pro images improved text to image pipeline. For the first time, AI generated posters, UI mockups, or magazine graphics should show perfectly legible typography instead of that weird mangled text we're used to. One of the most talked about upgrades is cultural context awareness. The model understands geographic and cultural nuances. So if someone types street wear shoot in Berlin winter or family picnic in Tokyo springtime, it generates visuals with regionally accurate lighting, scenery, and fashion details. It's trained on far broader geographic data sets to make outputs feel real and localized rather than generic. Google's also fixing one of the most annoying issues with image models, subject consistency. Earlier versions sometimes changed a person's face or outfit between scenes, but Nano Banana 2 keeps characters coherent across multiple prompts. Developers say it now behaves almost like a lightweight visual storytelling tool, maintaining continuity for creators working on campaigns or animations. And the editing tools are stepping up,",
        "start": 2258.96,
        "duration": 4823.3179999999975
    },
    {
        "text": "consistency. Earlier versions sometimes changed a person's face or outfit between scenes, but Nano Banana 2 keeps characters coherent across multiple prompts. Developers say it now behaves almost like a lightweight visual storytelling tool, maintaining continuity for creators working on campaigns or animations. And the editing tools are stepping up, that lets users highlight parts of an image to modify instead of starting over. You can tweak outfits, adjust lighting, or replace backgrounds while preserving the rest of the composition. Combined with a new rendering system that completes complex prompts in under 10 seconds, down from 20 to 30 previously, Nano Banana 2 is now as fast as MidJourney 6 or Adobe Firefly. Under the hood, all of this runs on Gemini 3 Pro Image, a multimodal architecture that merges image, text, and vision language reasoning. The underlying pipelines include text to image, image to image, and even multi-image fusion, meaning the same backbone that powers these creative tools could soon appear in Google Photos, Workspace, and even Android wallpaper generation. If the leaks are right, Nano Banana 2 will understand tone, style, and detail on a whole new level, turning casual prompts into results that actually feel intentional and consistent, not just random good luck. The timing of this release looks intentional, too. Google plans to integrate Nano Banana 2 fully into Gemini's image panel by early 2026 alongside Gemini 3 Pros multimodal updates. There's even talk that the Pixel Fold 2 and Pixel 9 Pro will include real-time camera analysis powered by the same model. So, when Gemini 3 Pro launches, it will be a full spectrum AI ecosystem covering text, image, reasoning, and device integration. Meanwhile, Google also rolled out something more technical, but just as important for developers, the agent development kit for Go or ADK Go. It's part of the same toolkit family that already supports Python and Java, but now it brings the same capabilities to Go developers. ADK is Google's open-source framework for building AI agents directly in code rather than relying on visual workflows. It moves everything logic, orchestration, and tool use into the developer environment, allowing for proper debugging, version control, and deployment anywhere from a laptop to the cloud. With ADK Go, developers get the speed and concurrency of the Go language combined with tight integration into Google Cloud. It includes out-of-the-box support for over 30 databases through something called the MCP toolbox for databases, which makes connecting to realworld data sources extremely easy. The framework's design mirrors the other ADK versions. So whether you're coding in Python, Java, or Go, you get the same structure and tools, a unified ecosystem for building complex AI systems. Now, one of the coolest parts is A2A support, short for agent-to-agent communication. This lets developers create multi- aent systems where different agents collaborate on tasks. A main agent can securely delegate work to specialized sub aents without exposing its internal memory or proprietary logic. Google even",
        "start": 2446.56,
        "duration": 5187.556999999994
    },
    {
        "text": "complex AI systems. Now, one of the coolest parts is A2A support, short for agent-to-agent communication. This lets developers create multi- aent systems where different agents collaborate on tasks. A main agent can securely delegate work to specialized sub aents without exposing its internal memory or proprietary logic. Google even open-source community, so anyone can start experimenting with distributed agent setups. In short, ADK go is a sign that Google wants AI development to feel like real software engineering again, not just prompting. You write your agents like any other service, test them, version them, and deploy them wherever you need. Google just dropped one of the biggest waves of AI breakthroughs we have seen this year, and everything hit at the same time. DeepMind revealed Sema 2, a goal-driven agent that thinks through tasks in full 3D worlds and even trains itself. At the same time, a hidden model inside Google AI Studio started showing nearly perfect handwritten text recognition and shockingly clean symbolic reasoning. And right on top of that, Nano Banana 2 leaked again with visuals matching Gemini's internal image engines, remastering low-quality photos, and following complex text prompts with almost mechanical precision. All of this landed within days, so let's talk about it. All right, so SEMA 2 is basically the new version of Deep Mind's generalist game world agent. The first SEMA launched last year already surprise people because it could follow over 600 instructions across different virtual environments. Stuff like turning left, climbing ladders, opening maps, basic movement, basic interactions. It was not perfect at long tasks, though. When DeepMind compared it to real players, SEMA managed around a 31% completion rate on longer multi-step tasks, while actual players sat closer to 71%. So, it did the job, but did not quite hold the bigger picture together. Sema 2 changes that dynamic. It brings in Gemini as its core reasoning engine, and suddenly the agent is not just following commands anymore. It actually interprets goals, thinks through what steps make sense, explains why it is doing something, and checks its own behavior. Deep Mind trained it on human demonstration videos with language labels, then built on top of that with labels generated by Gemini to fill in gaps. With this mix, Sema 2 almost doubled its performance internally on long horizon tasks and stayed flexible across the different game engines they threw at it. The interesting part is how it generalizes. It can jump into games it was not directly trained on like Asuka, a Viking survival game, or Mind Dojo, which is part of the Minecraft research environment. It handles sketches, emojis, multi-step instructions, and multilingual prompts. It even transfers ideas between worlds. If it figures out how mining works in one environment, it applies that knowledge to harvesting somewhere else without needing extra explanations. In No Man's Sky, for example, the agent read rough terrain, located a distress beacon, and planned the next steps like it had already",
        "start": 2630.0,
        "duration": 5513.157999999994
    },
    {
        "text": "transfers ideas between worlds. If it figures out how mining works in one environment, it applies that knowledge to harvesting somewhere else without needing extra explanations. In No Man's Sky, for example, the agent read rough terrain, located a distress beacon, and planned the next steps like it had already the interaction layer even further when they paired Sema 2 with Genie 3, their realtime world generator. Genie can take an image or a line of text and instantly build a 3D environment around it. And Sema 2 jumps into those newly generated worlds as if they were normal game levels. It navigates, uses tools, completes goals, and reacts naturally even when the scenes are full of visual noise, trees, benches, scattered objects, mobs, pretty much anything Genie feels like dropping in. And because it is all synthetic, the agent does not rely as heavily on human generated data. It starts with human demonstrations and then learns through its own self-directed play where one Gemini model sets challenges, another evaluates attempts, and Sema repeats until it gets better. That cycle builds new skills in places no human has ever labeled. The long-term angle is obviously robotics and DeepMind openly connects Sema 2 to embodied intelligence research. Their team describes it like a hierarchy. Sema handles the highlevel what to do and a lower system handles joint movement, wheels, manipulators, the mechanical stuff. That setup matches what you see in Nvidia's Isaac or Meta's Habitat benchmarks. Real world homes and factories still break agents with clutter, poor lighting or unpredictable objects. But SEMA 2 cuts that distance a little bit because the agent understands semantics before motion. Once the understanding is locked in, bringing in precise controllers becomes a more direct engineering problem. But the bigger shift across Google's ecosystem right now is happening on the reasoning side. And this comes from a separate discovery that blew up online. A historian named Mark Humphre, who normally studies North American history, started experimenting with a mysterious model that popped up inside Google AI Studio during an AB test. AI Studio sometimes runs silent tests where it gives you two model outputs and asks you to pick the better one. Users believe these tests are the earliest signs of Gemini 3 being evaluated before release. Humphre uploaded 18th century handwritten materials, letters, account books, messy journals. These documents are full of old grammar, inconsistent spelling, smudges, vague symbols, and unclear numbers. Most AI models collapse on this stuff because it's not just about pattern recognition. It's also about historical context and reasoning. The new Google model, however, produced something closer to expert level interpretation. In older benchmarks, Gemini 2.5 Pro already hit around a 4% character error rate and an 11% word error rate on complex manuscripts, which was impressive. This new model dropped the CER to around 0.56% and the WER to about 1.22%. That means roughly one error every 200 characters. But the real shock wasn't",
        "start": 2795.76,
        "duration": 5839.7989999999945
    },
    {
        "text": "2.5 Pro already hit around a 4% character error rate and an 11% word error rate on complex manuscripts, which was impressive. This new model dropped the CER to around 0.56% and the WER to about 1.22%. That means roughly one error every 200 characters. But the real shock wasn't behind the accuracy. In a merchants's journal from 1758, the document listed a sugar purchase written as to one loaf sugar 145 at 1/4191. The numbers were unclear, especially the 145, which could mean 14 1.5, 1.45, or something else entirely. Almost every AI model misinterprets this type of shortorthhand, and either guesses wrong or formats it wildly. The new model decoded the units, checked the total cost, and concluded by itself that the correct value was 145 o. It reached that by converting one shilling and 4 p to 16. Calculating that the total of 0, 19 shillings, and 1 penny equals 229 p. Dividing that by 16 to get 14.3125, then converting the remainder into ounces. It even added the LB and Oz labels on its own. This wasn't just transcription. It was multi-step reasoning across historical units, ambiguous handwriting, and context restoration. Humphrey said it felt as if the model understood the ledger. And the thing is the model wasn't specifically trained as a symbolic reasoner. It behaves like one simply because the internal representation has reached a level where reasoning emerges as a side effect. Researchers call this emergent implicit reasoning. And multiple people online have started reporting the same behavior in chemical formulas, ancient currency conversions, manuscript date inference, and deeply ambiguous text. If this holds, it means handwritten text recognition and symbolic reasoning, two of the hardest problems in the field, have moved forward at the same time in the same system. For historians, this changes how entire archives can be read and analyzed since AI can now handle spelling mistakes, vague shortorthhand, and cultural context without breaking rhythm. At the same time, people want transparency because once AI starts applying its own corrections, its interpretation of history could influence the final understanding. Humphre points out that AI should work with human researchers, not replace them, because even expert level reasoning still reflects model biases. And while all this was happening, another part of Google's ecosystem leaked from an entirely different angle. On media.ai, a preview of Nano Banana 2 briefly appeared before the listing was removed. But during that short window, users downloaded multiple sample images. Then more examples surfaced on social media from creators like Mars Everything Tech and LEO. The model shows higher fidelity, much sharper detail, and strong consistency, especially in tasks that involve text. It draws long sentences on whiteboards with matching font weight, letter spacing, and layout precision, an area where image models usually break apart. The remastering ability also jumped noticeably. Lowresolution or blurry images get reconstructed into something clean with accurate colors and sharper lines. Some samples suggest that Nano Banana 2",
        "start": 2961.44,
        "duration": 6192.356999999991
    },
    {
        "text": "draws long sentences on whiteboards with matching font weight, letter spacing, and layout precision, an area where image models usually break apart. The remastering ability also jumped noticeably. Lowresolution or blurry images get reconstructed into something clean with accurate colors and sharper lines. Some samples suggest that Nano Banana 2 based image engines internally tested across Google. The model follows natural language instructions more accurately, handles step-by-step prompt logic, and interprets requests that involve both visual content and world knowledge. People highlighted how it handles banners, social media visuals, and even full compositions that would normally require manual Photoshop work. These upgrades matter because creative workflows rely on fast, high-quality assets. If Nano Banana 2 runs through an API with the same performance we're seeing in leaks, content pipelines for media teams get a huge boost. So, Agabot just dropped a new humanoid that actually talks, moves, and reacts like a real person, not in a flashy demo, but in real world environments. At the same time, Xpang showed off its bare metal robot literally dancing after learning the routine in just two hours. And we finally got an answer to why that robot came with a pair of breasts, while Toyota revealed a walking chair that climbs stairs on legs like some gentle AI pet. Robotics just got weird, advanced, and way too human. So, let's talk about it. Let's start with Agabot A2 because this one might quietly become the robot people actually see in public first. On the surface, it looks clean, futuristic, and surprisingly human in proportion. 169 cm tall, 69 kg in weight, with a turning radius of 60 cm. It's built around human ergonomics, meaning when you're face to face with it, it doesn't feel cold or mechanical. The design has this balance between elegance and power, smooth contour lines, subtle futuristic details, and the goal is to make interaction feel as natural as possible. Inside though, it's loaded with serious tech. Its AI system runs on large language models with full duplex voice interaction, meaning it can hold real-time conversations, not just respond line by line. It even combines those models with retrieval augmented generation, basically rag, to build custom knowledge bases for businesses. So, if a company wants this robot at their reception desk or exhibition stand, it can instantly pull relevant answers from their database and talk like a trained employee. Then there's the sensory setup. The A2 can filter out up to 96% of background noise and still understand you clearly in crowded environments like airports or malls. Its face recognition and lip reading reach a 99% wake up accuracy, which is insane for a robot running in the wild. And they even integrated something called action GPT, a motion generation system that translates spoken intent into physical gestures and natural movements. So, if someone says, \"Wave to the guests,\" it actually performs a human-like wave that fits the tone of the command. But what really makes this",
        "start": 3139.839,
        "duration": 6525.158999999989
    },
    {
        "text": "the wild. And they even integrated something called action GPT, a motion generation system that translates spoken intent into physical gestures and natural movements. So, if someone says, \"Wave to the guests,\" it actually performs a human-like wave that fits the tone of the command. But what really makes this It runs on a 3D slam mapping system called Heimus, combined with a vector flux control algorithm, giving it level four autonomous movement. That means it can move safely in complex spaces, not just pre-mapped areas. The joints are customuilt and mass-produced, each capable of delivering up to 512 Newton meters of peak torque, designed to run for thousands of hours without failure. The movement control system called RTMF uses reinforcement learning to predict motion paths and adjust in real time. Basically, this robot can walk around a busy hall, avoid obstacles, and still look smooth while doing it. For safety, it's got a three-layer monitoring system, hardware system, and business levels with redundant control channels and six HD cameras, plus a 360\u00b0 LAR array. So, no blind spots, no accidental bumps. It's PLLDcertified, meaning it meets industrial-grade safety standards. And maintenance is simple. You can control it remotely via phone or laptop. It supports quick battery swaps and it even has a multi-functional standby station for charging and transport. The runtime is around 2 hours per charge, but since the batteries swap instantly, that's basically endless uptime. All this makes it clear what Agubot's aiming for. Not the flashy sci-fi robot, but something that can actually serve in real business environments. Exhibition halls, hotel lobbies, supermarkets, even fashion shows. anywhere. Human presence is key, but automation helps. It's the kind of robot that blends in rather than showing off. Now, Xpang's story takes a completely different route, and it's much louder. After their AI day in Guanghou, where their humanoid robot Iron, walked out on stage with strangely human posture, people online went wild. Some said it was fake, that there must have been a human in a suit. So, the company's CEO, Hiaopang, decided to prove everyone wrong. and he did it by posting a new lab video showing the bare metal iron robot dancing. This new footage shows the raw mechanical version, no skin, no covers, just the exposed frame moving with almost unsettling precision. And here's the part that really turned heads. He said the dance routine was learned in only 2 hours. They used what he called a comprehensive imitation learning method where they feed human dance data directly into the AI and the robot learns to replicate the movement almost instantly. Previously, reinforcement learning took weeks and still struggled with generalization. Now, Iron's model does it in hours and can adapt the movement dynamically. The key hardware feature behind that lielike motion is what Xpang calls a human-like spine. It adds flexibility in the waist, which lets the upper and lower body move in sync like a real person. When the torso",
        "start": 3308.0,
        "duration": 6857.237999999986
    },
    {
        "text": "Iron's model does it in hours and can adapt the movement dynamically. The key hardware feature behind that lielike motion is what Xpang calls a human-like spine. It adds flexibility in the waist, which lets the upper and lower body move in sync like a real person. When the torso naturally, which explains why people thought the AI Day demo looked too real. Combine that with 82 degrees of freedom, nearly twice what most humanoids use, and you get motion that no longer looks robotic. Now, during the keynote, he confirmed that the robot will come with dextrous hands and an all solidstate battery, giving it longer life and lighter weight. Its AI brain combines three large scale multimodal systems. VLT for vision language task, VLA for vision language action, and VLM for vision language model. Basically, it can see something, describe it, and perform an action tied to that description. The full closed loop of perception and decision. Now, about the elephant in the room or maybe the curve in the chassis, Xping's iron has breasts. And that design choice caused even more chaos online. During the event, he explained why. At first, Xpang's robotics division was exploring quadriped robots, four-legged models similar to Boston Dynamics spot because they're more stable. But he realized those lack hands, and they struggle to turn around or handle human spaces. The world is built for humans, he said. So, robots should be human- shaped to operate efficiently within it. Then there's the emotional factor. He said humanoid robots feel more approachable, more intimate. his words, not mine. Though, judging by what Xpang showed, that clearly translates to giving their robot a pair of breasts. The company believes people are more likely to interact naturally with something that looks and moves like them, even if it blurs the line between tech and, well, personal preference. Leang Chuanmi, the vice president of Xpang Robotics, put it bluntly. We're not just making robots, we're making humans. They even see future versions where you can customize your robot's body type. Slimmer, bulkier, taller, shorter, choose skin softness, hair length, even clothing. Its personalization taken to a whole new level. And apparently, yes, even the breast size will be part of the customization menu. The reasoning behind the feminine form, according to the company, is to study human reaction. They want to see how people respond emotionally to different appearances. MI said they're experimenting with form factors to understand which types create comfort or trust in social environments. Eventually, the plan is to offer full customization like picking car colors, except now it's for a humanoid companion. He also revealed that Xpang plans to begin preparing mass production next April with the goal of entering commercial spaces not long after. So, we're not talking about something 10 years away. They're already moving from R&D to deployment. Now, while China's busy turning robots into near humans, Toyota took a completely different path,",
        "start": 3476.96,
        "duration": 7186.997999999982
    },
    {
        "text": "Xpang plans to begin preparing mass production next April with the goal of entering commercial spaces not long after. So, we're not talking about something 10 years away. They're already moving from R&D to deployment. Now, while China's busy turning robots into near humans, Toyota took a completely different path, the Japan Mobility Show 2025, they introduced a concept called WalkMe. It looks like a small chair at first glance, but instead of wheels, it walks on four legs. Think of it as a personal mobility assistant that doesn't roll. It strolls. The idea is to help people with limited mobility move around freely, even in places where wheelchairs struggle, like stairs or uneven paths. Each of its four pastel colored legs has multiple sensors and actuators, allowing it to bend and adapt to the terrain in real time. It was inspired by animals that excel on rough terrain, goats and crabs specifically. The front legs analyze height and pull the chair forward, while the hind legs push and stabilize. The result is motion that's both steady and natural, even on stairs. Walkme uses LAR and radar to detect obstacles, plus weight sensors and an automatic stabilization system to keep the occupant balanced on slopes. And despite being a mobility aid, it's loaded with AI features. It responds to voice commands. You can literally tell it to go faster or change direction. There's also a manual override with handles if someone wants to steer directly. Comfort wasn't an afterthought, either. The seat molds to the user's body with adjustable armrests and soft padding designed for long use. A small display shows battery percentage and distance traveled. And the internal battery lasts an entire day of operation before needing a charge. When you plug it in, it's as simple as charging a phone. It even has a thermal safety system that pauses movement and alerts the user if any joint overheats. And when it's not in use, the legs retract telescopically, letting it fold down into a compact shape you can store in your car or corner of your home. It's still a concept for now. Toyota hasn't announced plans for consumer release, but it shows a practical use of robotics that's not about replacing people, but helping them move better. Google just dropped something that can pull realtime data from search. Recreate celebrities in massive group photos without a single reference image, rebuild a centuries old painting in a modern setting with perfect storyline consistency, generate a comic strip with stable characters and coherent dialogue, and even turn GPS coordinates into an actual photo of the correct building. And the wild part is that this was not some massive launch event. It landed out of nowhere and the first reactions were basically hold on this looks like AGI level reasoning inside an image model. They call it Nano Banana Pro or officially Gemini 3 Pro image and it is the biggest jump in image generation we",
        "start": 3643.68,
        "duration": 7491.399999999983
    },
    {
        "text": "not some massive launch event. It landed out of nowhere and the first reactions were basically hold on this looks like AGI level reasoning inside an image model. They call it Nano Banana Pro or officially Gemini 3 Pro image and it is the biggest jump in image generation we started. So let's talk about it. All right, so the first moment that made people stop was that wild remake of William Hogarth's Aakes Progress but set in 2025. The original paintings from the 18th century chart a young man's rise and fall. And Nano Banana Pro managed to recreate that same storytelling energy with a ridiculous amount of modern detail. You see the guy getting rich through Dogecoin, monster energy cans everywhere, an old deliveroo job, therapeutic ketamine ads, NFT phases, life extension experiments, a gig economy, prison, only fans burnout, a mega corp eviction notice, and finally a dopamine ward as the modern version of the mad house. Every panel looks like something an illustrator would spend days refining, and Nano Banana Pro just drops it in seconds with the same consistency across the entire sequence. When you compare it to the original Nano Banana or even Gemini 2.5 flash image, the difference is basically night and day. Even Crereee 4.0, which used to dominate many benchmarks, looks like it was working with outdated ideas. One of the reasons for that jump is the fact that Nano Banana Pro pulls real-time data from Google search. In the Shard example, the stock score projected onto the building was not hallucinated. It was the actual result from that moment. That is something other models simply cannot match right now. When you ask it for a topographic map of London made out of embroidered felt, it gives you something you would actually see hanging in a restaurant with recognizable structure instead of random shapes pretending to be geography. And yet, it is not flawless. Some distant skyline structures do not line up perfectly. But considering the entire thing is grounded in live search, this is a new level of realism for generative tools. Where things get even more interesting is how it handles composition. You can drop in Goku, Spongebob, and Squirtle. Ask for a professional IMAX double exposure action poster, and it not only blends the styles correctly, it has them interact in ways that make sense. Goku firing an attack, Spongebob countering, Squirtle spinning water off to the side, all tucked into layered exposure effects that actually look like real poster art. Using the same prompt on Cadream 4.0 zero or on the original Nano Banana looks like something from early 2023, just completely disconnected ideas thrown together. Of course, these upgrades come with some scaling differences. The official Gemini 3 Pro image tier that powers Nano Banana Pro is more expensive to run. At high resolution, it is around seven or eight times the cost of the original Nano Banana and about 3 or four times the",
        "start": 3798.64,
        "duration": 7800.679999999985
    },
    {
        "text": "Of course, these upgrades come with some scaling differences. The official Gemini 3 Pro image tier that powers Nano Banana Pro is more expensive to run. At high resolution, it is around seven or eight times the cost of the original Nano Banana and about 3 or four times the the fastest model either. But then you compare it with OpenAI's highresolution image model and Nano Banana Pro still ends up cheaper. And even if OpenAI eventually drops GPT image 2, at this moment, Google owns this specific category. Google also introduced Synth ID into the mix, embedding an invisible watermark into every image. You can upload any picture into the Gemini app and ask it whether Nano Banana Pro created it. This is rolling out to text, audio, and video soon. Free tier outputs have a visible watermark too. Unless you use higher tier tools or developer modes like AI Studio. Now the fun part is how deep Nano Banana Pro goes with intelligence. The infographic examples alone show that the model is not just drawing. It is reasoning. It creates black death maps with correct dates, historical spreads, and clever visual effects. But this is also where you see the danger. A single wrong word like spared under Paris can slip in even though that region definitely suffered during the plague. And since the other 99% of the image is so good, people might not doublech checkck the final details. The closer these models get to perfection, the easier it is for users to be misled by tiny inaccuracies. Another point that really shows the leap is spatial awareness. If you drop a hedgehog photo and ask Nanobanana Pro to place it into a picture of a cottage doorway with matching lighting and a man looking down at it, it positions the animal almost exactly how you would mentally place it. The original Nano Banana from just a few months ago did something that barely resembled the scene. Even Seam lands somewhere in between, decent, but not quite right. Nano Banana Pro feels like it understands physical space instead of just pattern matching shapes. The comic strip example takes it even further. Give it a mouse character you created years ago in midjourney. And tell it to generate a four panel comic with a turtle, consistent personalities, speech bubbles, and a punchline, and it just does it. The mouse keeps the satchel, the vibe stays the same, the turtle stays grumpy, the slang stays consistent across panels. Then you continue with a new prompt, asking for a second story, but on a medieval gallion, and the personalities stay intact. Sure, a hat might disappear in one panel or a few arrows shift around, but it is functioning as if it understands continuity on a higher level. Google also showed that yes, cloning multiple characters in one frame works extremely well. Up to 14 reference photos can be merged into one complex scene while preserving each person's identity from",
        "start": 3955.28,
        "duration": 8098.8389999999845
    },
    {
        "text": "a few arrows shift around, but it is functioning as if it understands continuity on a higher level. Google also showed that yes, cloning multiple characters in one frame works extremely well. Up to 14 reference photos can be merged into one complex scene while preserving each person's identity from professional studios could do before. What really separates Nano Banana Pro, though, is the amount of control it gives you. You can adjust camera angles, depth of field, lighting conditions, and color grading in ways that used to require full photography workflows. You can swap outfits across scenes, redesign layouts, change day to night, manipulate bokeh, brighten shadows, and keep the entire scene consistent. It even supports 2K and 4K resolutions with proper aspect ratios ranging from ultrawide cinematic to square. And for editing, the new localized refinement tools mean you can draw over a photo and force the model to add specific details exactly where you marked them. You can change a girl's photo by adding a cowboy hat, a guitar, a cat on the table, and it will even adjust the reflection of the new guitar on the table surface, even if you did not circle the reflection zone itself. One of the wildest demos is how well it handles real world data. You can upload a Google Analytics screenshot and tell it to inflate your clicks to 90,000 and impressions to 1.2 million and also update the graph to match the new numbers. The model changes the axis, updates the curve, and keeps the layout perfectly aligned. None of the older models could touch that. It even turns tables of benchmark numbers into graphs, perfectly matching categories like MMLU Pro, GPQA diamond, reasoning scores, agentic tool use scores, and keeping colors and labels accurate. This is not pattern guessing. This is understanding the underlying numerical structure. Then you get into the medical scan example. Four different lesion markers and Nano Banana Pro detected almost all of them correctly. Even though the previous models completely missed the locations or hallucinated patterns, it still makes occasional mistakes like marking an extra spot. But the accuracy jump is real. For fictional characters, it identifies outfits from ReZero or Demon Slayer or South Park or Snow White with a level of precision that previous models simply could not achieve. And when you mix extremely rare species like the Sri Lanka Slender Loris, it knows the general family but still misses details. That part is not solved yet. But every other model fails harder and it still cannot get clock times right. You can ask for 11:15 in a wine glass filled to the top. It fills the wine glass, but the clock lands on some random time. Again, this weakness shows up in every model. So, it is probably a deeper limitation of how Transformers treat numbers and rotation. Where Nano Banana Pro really flexes is world grounding. You can paste coordinates from Hong Kong, and it generates the",
        "start": 4106.48,
        "duration": 8400.360999999977
    },
    {
        "text": "but the clock lands on some random time. Again, this weakness shows up in every model. So, it is probably a deeper limitation of how Transformers treat numbers and rotation. Where Nano Banana Pro really flexes is world grounding. You can paste coordinates from Hong Kong, and it generates the the specific street layout, and even the atmosphere of the district you pointed at. Other models only produce generic Hong Kong streets with random skyscrapers. And when you upload a floor plan, it produces interior designs with correct placement of every wall, door, piano, angle, and lighting. When you upload a photo of an apartment, it produces a 2D floor plan that matches the layout almost exactly. Again, older models cannot even start correctly. Then you drop in a Gundam and ask it to disassemble it into parts. Nano Banana Pro lays out wings, armor, stickers, joints, and it is recognizable. Same Gundam, same components. And when you ask for a model sheet with front, back, and side views, it gets the two swords on the back correct. something earlier models missed almost every time. All right, that is where we will wrap it. Tell me in the comments what you think Google is really aiming for with this jump. Make sure you are subscribed and hit like if this helped. Thanks for watching and I will catch you in the next one.",
        "start": 4259.28,
        "duration": 8525.078999999982
    }
]