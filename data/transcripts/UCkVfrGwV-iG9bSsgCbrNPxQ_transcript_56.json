[
    {
        "text": " So, Grock 5 might actually be the world's first AGI and we have to talk about it. No, the title of this video is not clickbait. And trust me when I say this might be the most important AI video I've personally made this year, and I'm about to explain to you exactly why. So this tweet a few days ago was gaining a little bit of traction and of course it's from Elon Musk. So a lot of people are going to be paying attention to exactly whatever he has to say about any topic. But he actually said that the probability of Grop 5 achieving AGI is now at 10% and rising. Now I even made a video about this I think one or two days ago discussing the fact that Elon Musk is overly optimistic about his AGI timelines. However, there was some information that I completely missed that I will now get to that changes this entire tweet in terms of the context. So, this was a tweet in question that I missed that as I was browsing on Twitter and I was researching certain things I came across. A user by the name of Roy asked if Grock 5 will have continuous learning and Elon Musk replied, \"Dynamic reinforcement learning is important. Grock 5 like smart humans will learn almost immediately.\" Now, this is a very, very important tweet because it changes the context of the entire statement that Grock 5 might actually be the world's first AGI. Now, if you don't know why that is the case, I need to explain to you why this is the case. Continual learning is arguably one of the biggest blockers when it comes to actually achieving an AGI. And if we can actually fix this problem in current AI systems, we will achieve AGI so much quicker because we're essentially learning like humans. As this quote tweet puts it, it says, \"If Elon's hint is literal, Grock 5 might have cracked continuous learning, dynamic reinforcement learning, plus learn almost immediately sounds like a system that updates in real time. No retraining, no static weights, just adaptive cognition. And if true, that would make Grock 5 unbeatable. Continuous learning equals perpetual improvement, and every interaction makes it smarter. We might be witnessing the birth of self-acelerating intelligence. Now, the tweet isn't exaggerating. Everyone knows that the moment you have a system that can learn and improve itself and get continually better over time, that is where the intelligent explosion really occurs. But we need to first explain why this has been such a big problem for such a long time. Like why haven't they managed to crack continuous learning before? Well, the problem is is that there's this thing called catastrophic forgetting. And this is when the AI forgets the old stuff after learning new stuff. So imagine you're training a model to play chess, then you teach it to play checkers. Suddenly it's really good at checkers,",
        "start": 0.08,
        "duration": 342.47900000000004
    },
    {
        "text": "learning before? Well, the problem is is that there's this thing called catastrophic forgetting. And this is when the AI forgets the old stuff after learning new stuff. So imagine you're training a model to play chess, then you teach it to play checkers. Suddenly it's really good at checkers, something called catastrophic forgetting. It's when a model's brain basically overwrites its old knowledge instead of keeping both. And LLMs avoid this by fine tuning carefully so it remembers old things while learning the new ones. And sometimes they use more data at once and sometimes they freeze certain layers. Now the reason you know you might be thinking okay why can't they just like add more storage is because it doesn't store knowledge like a hard drive. It stores it inside of millions of tiny connections which are the neurons and the weight. So when you train it on any new information, it's not like saving a new file. It's more like repainting on the same canvas. The paint mixes with the old and some details inevitably get lost. That's essentially this big problem called catastrophic forgetting. It's the act of retraining that same brain without a true memory system. And this is the blocker for AGI because like humans, you know, we need to be able to learn continuously. We can't be able to, you know, forget certain skills like reading, math, cooking, basically everything. And, you know, LLMs can't keep learning in a safe way. Every time you update it, you risk wiping or corrupting what they already know. So, instead of, you know, becoming smarter, they have to be retrained smarter from scratch every single time, which is why things are so expensive and somewhat silly. Now, humans don't do that. We can learn something new while keeping everything else fine. And that's what real general intelligence requires. And so we have the, you know, understanding that LLMs can memorize the world, but they can't remember new experiences without forgetting the old one. And so we have the information that look LLMs can't learn the way humans learn. And this catastrophic forgetting problem is one of those key key issues. And I was watching this episode of machine learning street talk where they actually, you know, talk about this same issue. Now the the forgetful issue that is a much more fundamental issue in my mind and not just that the fact that you need to every time you fine-tune you have to have some sort of very elegant mixture of data that you know goes into this fine-tuning process so that there's there's no catastrophic forgetting. this is I think actually a fundamental problem. So I the the and and it's a fundamental problem that that you know even open AI has not solved right. , and I think Francois has a great example and I think this is an important example. You know, if you have the perfect weights for a certain",
        "start": 170.959,
        "duration": 626.5570000000001
    },
    {
        "text": "problem. So I the the and and it's a fundamental problem that that you know even open AI has not solved right. , and I think Francois has a great example and I think this is an important example. You know, if you have the perfect weights for a certain model on more examples of that problem, the weights will start to drift and you will actually drift away from the from the correct solution. His answer to that is well, we could make these systems composable, right? Like we can freeze the correct solution and then we can add on top of that. I think there's something to that. I think actually it's possible that there's a research direction that where you know maybe we freeze experts or may maybe we freeze layers for a bunch of reasons that isn't possible right now or but people are trying to do that. But I yeah I think fundamentally compute is not the issue. I think it's this catastrophic forgetfulness. This issue to me became more evident when I was watching this video from Dwarfish Patel which was I think around 2 months ago where he said that I don't believe that AGI is around the corner. And this is a guy who spends his time talking with some of the most educated minds in AI including those who believe that LLMs are currently a dead end. And you know his opinion is one that I would say is probably one of the most important considering he's having these conversations weekly with some of the top minds. Now, in this video where he says why I don't think AGI is around the corner, he says one out of his two blockers that he consistently sees is that current AI systems cannot have continual learning. Like they can't learn how humans do. And this is a remarkably, you know, difficult problems that AI would need to solve in order for us to get to AGI. I'm going to play a short short snippet of this so you can understand why this is important. and I will, you know, as soon as this is done, I'm gonna tie it right back to Grock 5. But the fundamental problem is that LLMs don't get better over time the way a human would. This lack of continual learning is a huge, huge bottleneck. The LLM baseline at many tasks might be higher than the average humans, but there's no way to give a model highle feedback. You're stuck with the abilities you get out of the box. You can keep messing around with the system prompt, but in practice, this just doesn't produce anything close to the kind of learning and improvement that human employees experience. The reason humans are so useful is not mainly their raw intellect. It's their ability to build up context, to interrogate their own failures, and to pick up small improvements and efficiencies as they",
        "start": 315.039,
        "duration": 903.5179999999998
    },
    {
        "text": "just doesn't produce anything close to the kind of learning and improvement that human employees experience. The reason humans are so useful is not mainly their raw intellect. It's their ability to build up context, to interrogate their own failures, and to pick up small improvements and efficiencies as they kid to play the saxophone? Well, you'd have them try to blow into one and then they'd see how it sounds and they'd adjust. Now, imagine if this was the way you had to teach saxophone instead. A student takes one attempt and the moment they make a mistake, you send them away and you write detailed instructions about what went wrong and you call the next student in and the next student reads your notes and tries to play Charlie Parker cult. And when they fail, you refine your instructions and you invite the next student. This just wouldn't work. No matter how well honed your prompt is, no kid is just going to learn how to play the saxophone from reading your instructions. But this is the only modality we have to teach LLMs anything. And so in that clip, you saw him basically explain to you why continual learning is absolutely necessary for an AGI system. So with that being said, now that we have the information that look, we need continual learning. Elon Musk has has literally said that dynamic reinforcement learning is you know essentially important and it's quite likely the Gro 5 will have this feature. It seemed like it was all speculation until this paper dropped a few hours ago. So this is the paper that I personally saw on Twitter which was only I think 2 or 3 days after Elon Musk's tweet and he was you know tweeting about AGI yada yada yada and then I saw this. So there's a paper called continual learning via sparse memory fine-tuning and this basically tackles the direct problem of catastrophic forgetting in LLMs and you have to understand that this is a major major thing because if this is true and this works this is going to completely change the game. So instead of updating all weights like previous ways this method only updates the small parts which are the memory slots that were actually used for the new information. So think of it like this. Instead of scribbling over the whole notebook, you just write on the few pages relevant to the new topic. They use memory layers, which are giant banks of little memories, but each input only touches a few, maybe 32 out of a million when new information comes in. And when this new information comes in, the model checks which memory slots lined up for, you know, most of that input, and then it ranks them using a TF IDF style score. So, it picks slots that are special to that new data, not generic ones, and updates those top t slots while freezing",
        "start": 455.44,
        "duration": 1180.7979999999995
    },
    {
        "text": "comes in, the model checks which memory slots lined up for, you know, most of that input, and then it ranks them using a TF IDF style score. So, it picks slots that are special to that new data, not generic ones, and updates those top t slots while freezing fine-tuning keeps the old knowledge intact because 99.9% of the parameters stay frozen. And the results are incredible. you get only an 11% drop while learning the same amount of new info compared to the full fine-tuning which you know has an 89% drop in old tasks and lura fine-tuning which you know is an improvement but still has a 71% drop in other tasks so essentially this model learns new stuff without nuking old skills and this matters a lot because AGI will need to learn continuously like humans every new experience adds to its mind without wiping out past memories And this paper shows a real method that makes this possible at scale. The learning stays localized. It only updates what's needed and the model keeps its old knowledge intact. Now I was also watching machine learning street talk when they were talking about how you know they had a smaller model that was doing something similar to this and I think this was three weeks ago and they literally said that you know eventually continual learning would actually be solved as per Shallay and they actually built this system which was per task adapting and solving the tasks and they were updating the weights and it was beautiful. So that was an existence proof if nothing else that this thing could work and that was on a Kaggle notebook. Yeah. you know in 10 years this is going to be what like the you know Apollo mission computer I think that I think what you're describing I'm actually not even totally convinced that continual learning is fundamentally the blocker but I think if it is the fundamental blocker that's actually incredible because we will solve continual learning like that's something that's physically possible and and I actually think like it's not so far off and then of course we have the Google deep mind advisor AI on continual learning AGI and the wildest predictions and this one was super interesting as well because he essentially says that you know nobody does continual learning. They do this version by version by version very safe thing. What's the cost of that? Well, obviously we don't want a system that makes the same mistakes tomorrow that it did today. We we prefer something that could learn. But also, if we really are ambitious with our goals, we want a system that will learn more and more and more each day, much as a human, but be able to do that at a greater scale, make new scientific discoveries, new mathematical proofs, , etc., and kind of learn forever. And so that gets back to your question of",
        "start": 596.8,
        "duration": 1467.758999999999
    },
    {
        "text": "want a system that will learn more and more and more each day, much as a human, but be able to do that at a greater scale, make new scientific discoveries, new mathematical proofs, , etc., and kind of learn forever. And so that gets back to your question of system that could generate new learning challenges for itself, learn, like spend a while, figure out how to solve those challenges, and then that new knowledge allows it to go tackle the next problem, the next problem, the next problem forever in every dimension. That's interesting. That's truly the promise of an open-ended algorithm. This thing that I've been working on with my colleagues for like 20 years now. And that specifically is an AI generating algorithm. That was the big proposal I put in my 2019 paper. Generate the problems, learn on them. That then allows you to generate the next set of problems, get better at learning, etc. All forever. I don't know like is it does it make sense to speculate how like the timeline how you think would take us to get to the like real progress in continuous learning? It is possible to produce AGI without solving this problem. Like I do think like version by version like GPT10 will almost certainly be AGI and it doesn't necessarily have had to have solved this problem. It might have solved this problem but it's I can I can envision a world in which you get really far without without yet solving this problem. And so this one was super super interesting to see. So will this mean that Grock 5 will actually be the first AGI based on the straightrough? Well, it's important because, you know, this paper does fix one of the biggest blockers to AGI, which is, of course, forgetting. And it's one of the top reasons that LLMs can't grow. Every time you fine-tune, you overwrite new knowledge. And the paper does show a real way to continuously learn by isolating updates to small relevant memory slots. And if an LLM can add new facts, skills, and experiences without retraining, that's the foundation of long-term learning, a core property of human intelligence. Now remember this is scalable. This method works on layers with millions of slots. Each input only updates a tiny subset which means in theory it can accumulate knowledge forever as long as the new slots are available. And this is kind of like giving an AI massive expandable brain where new experiences don't overwrite old ones just new drawers being filled. And it also does mimic the biological memory. Humans don't rewrite their entire brain when learning. They store new patterns in specific areas. And the sparse updates reflect this local context dependent learning rather than global rewiring. That's a major step from static models towards agent-like learning that evolves with experience. Now, why this might not lead to AGI? In an effort to keep this somewhat",
        "start": 741.92,
        "duration": 1758.5599999999988
    },
    {
        "text": "They store new patterns in specific areas. And the sparse updates reflect this local context dependent learning rather than global rewiring. That's a major step from static models towards agent-like learning that evolves with experience. Now, why this might not lead to AGI? In an effort to keep this somewhat one piece of the puzzle. Forgetting, of course, is a big problem, but AGI needs things like reasoning, planning, long-term goals, emotional modeling, motivation loops, and selfcorrection. The paper just solves the memory stability issue, not autonomous learning, creativity, or understanding. Think of this as fixing the brain's hard drive, not giving it consciousness or reasoning. Now, sparse memory still has physical limits. The system assumes you can keep adding memory slots, but eventually memory runs out or gets too big to efficiently manage. Real AGI would need to compress and abstract knowledge, like humans forgetting the unimportant stuff. This paper doesn't yet show how to decide what to forget intentionally or how to prioritize certain memories. Also, learning here is still supervised and static. The updates depend on label data, which are QA tasks like trivia QA and not self-driven discovery. AGI would need to learn autonomously from experience, not just from training batches. Sparse memory fine-tuning still uses human defined data sets, not an open-ended world model. And there's no reasoning or metalarning just yet. It remembers new facts but doesn't yet generalize across them to form new concepts. For AGI, memory isn't enough. You'd need systems that can use memory to reason, plan, and invent. And right now this method is more like giving GPT a very clever notepad.",
        "start": 890.48,
        "duration": 1926.7999999999995
    }
]