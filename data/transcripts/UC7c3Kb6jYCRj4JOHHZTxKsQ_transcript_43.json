[
    {
        "text": " We finally know what Mirror Morati, the exCTO of OpenAI, is building at Thinking Machines. It has been a total mystery, but now we have a clue and it's interesting and complicated and I'm going to try to break it all down for you. Let's get into it. So, just quickly, Thinking Machines. Today, Thinking Machines Lab is launching our research blog, Connectionism. Our first blog post is defeating non-determinism in LLM inference. If you don't know what that means, don't worry. I will explain it all. So here is the paper. Now what is non-determinism? In the context of artificial intelligence, you've probably noticed this. If you give the same exact prompt to a model twice, three times, four times, you're going to get back different responses. And that is non-determinism. The same input does not always lead to the same output. In fact, it's frequently different. But as they say in the very first sentence, reproducibility is a bedrock of scientific progress. However, it's remarkably difficult to get reproducible results out of large language models. And that is what they're looking to solve. They want to be able to prompt an LLM with the same exact prompt and get back the same exact response to it every single time. Now, there are a large number of use cases where this would be extremely valuable, but also some in which it's not valuable. Think about creative writing, for example. I don't necessarily want this same exact response if I'm trying to write a cool fun story. And not only are they trying to defeat randomness in responses, but they actually think they've identified the cause of it. You might observe that asking Chad GPT the same question multiple times provides different results. This by itself is not surprising since getting a result from a language model involves sampling. Sampling basically just means looking at all the potential results and choosing the one that it has the highest confidence in. But even when we turn the temperature of the model's response down to zero, and if you haven't seen temperature, it basically means reducing the randomness of the sampling of the choosing of the next token. With a temperature of zero, it should be deterministic. But again, it's not. But why aren't large language models deterministic? It might just seem like, well, of course, that's the way they are. But of course, there's a reason they're the way they are. One common hypothesis is that some combination of floatingpoint nonassociivity and concurrent execution leads to non-determinism based on which concurrent core finishes first. And so this sentence has a ton of very technical terms. Let me just break it all down. So floatingoint numbers are numbers that have decimals. So think like 5.23. But how do you know the floatingoint precision? And how many numbers of precision do you go after the decimal? At a certain point, you can't go infinitely. And so you do need to",
        "start": 0.16,
        "duration": 351.2800000000001
    },
    {
        "text": "break it all down. So floatingoint numbers are numbers that have decimals. So think like 5.23. But how do you know the floatingoint precision? And how many numbers of precision do you go after the decimal? At a certain point, you can't go infinitely. And so you do need to randomness. And second, concurrent execution. GPUs. When you give them math, they are executing a bunch of calculations all at the same time and you don't know which one generally is going to finish first. Now, I believe companies like Gro GRQ really fix that issue because their chips are completely symmetrical and you know when you put in a calculation exactly how long it's going to take to get back. But when you don't know that randomness in which result comes back first also adds to the overall non-determinism of the response. But thinking machine says while this hypothesis is not entirely wrong, it doesn't reveal the full picture. For example, even on a GPU, running the same matrix multiplication on the same data repeatedly will always provide bitwise equal results. We're definitely using floatingoint numbers and our GPU definitely has a lot of concurrency. Why don't we see non-determinism in this test? But according to the paper, the real culprit is the batch size. So, think of it like this. You are posing a question and your question gets put into a carpool with other people's questions. The carpool is called a batch. When the system is really busy, the carpool is really big. It's trying to get more done. And when it's quiet, it's very small. And that size quietly changes the order of the tiny add-ups, the math that it does inside of the AI. So, different order, slightly different totals, and sometimes a different next word. Remember, all LLMs are just predicting the next word. Sometimes the next word differs because of this. Little changes, little variances in the ordering of things changes everything. And I know this is all so complicated, but you know what isn't? The sponsor of today's video, Lindy. Lindy is the best way to vibe code agents and apps. Imagine you want to build a fully functional online education platform in under 5 minutes. You can do that with Lindy's new build feature. So, first you describe what you want to build. So, let the agent know who you are and then tell it, \"Build me an online education platform.\" Lindy isn't just generating code. It's researching best practices, building the front end, and here's the key part. If you've watched any of my tutorial videos on Vibe Coding, you know testing is everything. And of course, Lindy writes all the tests for you. Lindy also runs QA on all the features that it builds before deploying. So, look at this. In under five minutes, you can have a fully deployed education platform with working user registration, free course access, premium upgrades with Stripe checkout",
        "start": 175.92,
        "duration": 659.7600000000003
    },
    {
        "text": "of course, Lindy writes all the tests for you. Lindy also runs QA on all the features that it builds before deploying. So, look at this. In under five minutes, you can have a fully deployed education platform with working user registration, free course access, premium upgrades with Stripe checkout taken you weeks previously and cost thousands of dollars built by developers, but Lindy built it in minutes because of their built-in QA process. Lindy not only builds code, but it ships working code. Sign up using my link in the description, get $20 worth of credits. absolutely free. So, what are you going to build with Lindy? Let me know in the comments. Thanks again to Lindy. Now, back to the video. All right, so what is the actual fix? Well, it turns out at least to explain it is pretty simple. Regardless of the size of the batch, you keep the carpool going at the same speed. And so, even though if you're keeping everything the same, you might go a little bit slower, the consistency that you gain is actually much more valuable. Now, this paper goes into the actual math for how all of this gets done. and it goes into a lot of the technical details which to be honest are a little bit beyond my comprehension. So I'm just kind of breaking it down in general terms for you. And so with these two fixes, let me just try to give you an analogy. Imagine you're a restaurant creating bowls. And so for this first fix, you need to make sure each bowl is weighed the same. So even if the kitchen is crowded or the kitchen is empty, no matter what, you're weighing the bowls the same. Now, you're going to lose out on a little bit of speed, but the overall consistency that you gain is going to be far more valuable. Then, you keep the mixing step the same also. So, mix the ingredients in the exact same way. Choose one stable setup regardless of how crowded that batch is. Then, there's a third fix that we'll talk about. Taste in the same order. So, we're going to keep on this kitchen analogy. Even if you bake things in chunks, you want to taste them in the order that you put them in. So in AI, the model looks back at what it wrote. That's the attention part of attention is all you need. And if you cut the text into different chunk sizes, the order of those tiny add-ups can change. So use the same slice every single time. All right. So did it actually work? Well, they used Quen 235B and sampled a,000 completions at temperature zero with the prompt, tell me about Richard Fineman generating a,000 tokens each. Surprisingly, we generate 80 unique completions with the most common of these occurring 78 times. That's the kind of baseline. Then when they enabled their batch invariant kernels, all of",
        "start": 332.96,
        "duration": 944.3190000000006
    },
    {
        "text": "235B and sampled a,000 completions at temperature zero with the prompt, tell me about Richard Fineman generating a,000 tokens each. Surprisingly, we generate 80 unique completions with the most common of these occurring 78 times. That's the kind of baseline. Then when they enabled their batch invariant kernels, all of So the answer is yes and it was perfectly effective. And why is defeating non-determinism so important? Well, if you get the same output from the same input, it becomes much easier to trust, to debug, to audit, to verify. If you know what you're going to get, it just gets easier to use. Plus, if you're running benchmarks now, you actually have stable inputs and outputs, and thus your benchmarks become more trustable. Your users can trust the outputs better. And when you go into audit and figure out why a model thinks the way it does, it becomes easier to do that. So, I'm going to drop the link to the full paper down below. I will say it is very complex. So, if you can read it, congrats. You're way ahead of the curve in AI. If you enjoyed this video, please consider giving a like and subscribe. and I'll see you in the next",
        "start": 477.199,
        "duration": 1039.6790000000005
    }
]