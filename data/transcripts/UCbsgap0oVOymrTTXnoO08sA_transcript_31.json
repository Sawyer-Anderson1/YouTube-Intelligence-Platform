[
    {
        "text": " This month, AI basically blew past every [music] limit. China shattered a human skill barrier with a new robot, then rolled out fully AI controlled slaughterbots, turning headlines into science fiction. Open AAI and Apple unveiled powerful new systems like Garlic AI and Clara. While the AI stack exploded with GPT 5.2, Deepseek [music] 3.2, Clling 2.6, Mistrol 3, and Tranium 3 all landing at once. Robots are now uncomfortably real. The first ever open-source AI agent shocked both OpenAI and Google, and many believe AGI just became real. Let's break it all down. And hey, happy new year, everyone. Thanks for watching, learning, and growing with us this year. Drop a comment and tell us what you want to see from AI in 2026. All right, now let's get into everything that just happened this month. And it all starts with a robot in China doing something that looks simple until you realize it's one of the hardest things a machine has ever been asked to do. On December [music] 22, Tar Robotics showed a humanoid robot on stage at a live event. And this robot didn't lift boxes, didn't stack crates, didn't walk around waving at people. It sat down and did hand embroidery. Real embroidery. It threaded a needle, used both hands, and stitched a logo live in front of an audience. Now, if you've never stitched anything in your life, that might not sound impressive. But the moment you think about what's actually happening there, it gets kind of wild. You're dealing with thread that stretches and twists, fabric that moves and deforms constantly, and movements that need submission over a long sequence of steps. One tiny mistake and the thread snaps, the stitch misses, or the whole thing falls apart. This kind of task has been a nightmare for robotics for decades. Industrial robots are amazing at rigid, repeatable jobs. Pick this metal part, place it there, do it again a thousand times. But soft materials break everything. They change shape, they resist unpredictably, and they require constant force adjustment in real time. That's why until now, long, delicate, two-handed work like this was seen as basically offlimits for automation. And that's what made this demo so important. The robot moved through the whole process smoothly, staying stable the entire time with no signs of hesitation or struggle. Both hands working together, adjusting force, tracking the needle and thread visually, and staying stable the whole time. That's not just motion control. That's embodied intelligence working as a system. People inside robotics immediately clocked what this meant. Because once you can do this, you're suddenly not just talking about embroidery. You're talking about wire harness assembly, precision electronics, fine mechanical assembly, all the stuff factories still rely on skilled human hands for. At the event, Tar Robotics CEO Dr. Chen Yeloon explained how they got there. And the way he described it actually lines up with a bigger trend we've been seeing across AI lately. He",
        "start": 2.72,
        "duration": 358.91999999999996
    },
    {
        "text": "precision electronics, fine mechanical assembly, all the stuff factories still rely on skilled human hands for. At the event, Tar Robotics CEO Dr. Chen Yeloon explained how they got there. And the way he described it actually lines up with a bigger trend we've been seeing across AI lately. He approach. Basically, instead of treating data collection, model training, and physical robots as separate worlds, they connect [music] everything into one continuous loop. They collect detailed realworld operational data using their SenseHub platform. That data feeds into their embodied AI model, the AWE 2.0 AI world engine. And here's the key part. The model isn't trained to do one specific task. It's trained to learn general physical skills, balance, coordination, [music] force control, vision under uncertainty, stuff that transfers from one job to another. [music] Those learned skills then run directly on their humanoid robots, the T-series and A series platforms. Chen also stressed something that matters way more than people realize. The digital to physical gap is small. What the AI learns during training actually carries over into the real world, which is why the robot held together on stage and performed the task without falling apart under real conditions. The system follows classic AI scaling laws, too. As more real world data flows in, performance improves across multiple tasks at once. Their chief scientist, Dr. Dingwin Chow, talked about how success rates jumped across different scenarios simply by scaling data and refining the model architecture. The long-term goal here obviously isn't a robot that can stitch logos. It's robots that learn adaptable skills and carry them into new environments without being reprogrammed every time. And the craziest part is that Tar Robotics was founded on February 5, 2025. In under a year, they went from concept to live humanoid demos showing capabilities people used to say were years away. Investors clearly noticed. The company raised $120 million in an Angel round, then followed that with another $122 million in an Angel Plus round. Now, at the time this was happening in China, a clip from the US revealed how far humanoids have come in a much more everyday setting. On December 23, right before Christmas Eve, Figure AI's CEO Brett Adcock shared a short video showing him interacting with the company's latest humanoid, Figure03, in a simple, unscripted setting. He starts by asking the robot where it was built. It answers [music] clearly saying San Jose, California. I was built right here in San Jose, California. He asks which generation it belongs to. The robot says it's part of the third generation. There are three generations of bigger robots, and I'm the latest one. When he asks which generation is best, it does the polite thing and says the third generation because it has the most advanced features. I believe I'm the best as the third generation with the most advanced features. Then he tests something more practical, visual recognition. [music] He asks the",
        "start": 182.239,
        "duration": 688.3610000000001
    },
    {
        "text": "When he asks which generation is best, it does the polite thing and says the third generation because it has the most advanced features. I believe I'm the best as the third generation with the most advanced features. Then he tests something more practical, visual recognition. [music] He asks the shirts. The shirts are sitting in different baskets. Figure 03 looks at the setup, identifies the correct size each time, picks the right basket, and hands the shirt over correctly. Under the hood, this robot runs on Figure AI's Helix model, which is a vision language action system. That means vision, language understanding, and physical movement are tied together in one loop instead of being bolted together with rules. People watching the clip were impressed, but they also immediately noticed something else. The robot pauses for about 2 to 3 seconds before answering questions. That delay jumped out in the comments. Can you give me a medium shirt? One person even joked that it felt like dialup internet and asked how Chinese robots compare. That reaction actually points to a real unsolved problem in humanoids right now. Speech latency. Even when perception and manipulation are solid, conversational timing still feels off. The robot hears the question, processes it, generates a response, then speaks. Every step adds delay, and humans [music] notice it instantly. Now, that doesn't mean Figure03 is weak. Far [music] from it. The robot was released in October and is a big step forward for the company. It's smaller and about 9% lighter than previous versions. It has a softer exterior with mesh fabric and foam padding designed to reduce injury risk. [music] It stands about 5'6 in tall and is built mainly as an assistant for household pick and place tasks. Physically, it's more capable, too. In another clip released a few weeks earlier, figure 03 showed quick starts, smooth turns, sharp braking, and even [music] running. Forbes reported speeds between 4 and 6 mph, which would beat the usual humanoid benchmark of 3 to 4 mph. [music] Figure AI itself gave a more conservative number of 2.7 mph. Audio hardware has been upgraded as well with larger speakers for clearer speech and less distortion. For power, figure 03 uses wireless charging through coils built into its feet. [music] It can recharge by standing on a charging pad at up to 2 kW and a full charge gives it roughly 5 hours of operation. So yeah, speech latency is still a thing. But overall, figure 03 shows how far humanoids have come in everyday interaction, even in casual settings. Then almost at the same time, something happened in the consumer space that surprised a lot of people. So, Vita Dynamics launched its AI powered robot dog under the V-bot brand. The price was set at 9,988 R&B, roughly 1,368 as an early bird offer. Within just 52 minutes, more than 1,000 units were sold. That's real demand, not hype clicks. The launch pushed related topics",
        "start": 349.44,
        "duration": 1021.0010000000003
    },
    {
        "text": "of people. So, Vita Dynamics launched its AI powered robot dog under the V-bot brand. The price was set at 9,988 R&B, roughly 1,368 as an early bird offer. Within just 52 minutes, more than 1,000 units were sold. That's real demand, not hype clicks. The launch pushed related topics and live streams around the product passed 10 million views. For a consumer embodied AI product at a four figure price, that's a big signal. What makes this robot dog stand out is that it's fully autonomous. No remote control, no constant human input. It runs 128 tops of ondevice AI compute and uses lidar and depth vision, processing visual data locally to protect user privacy. The hardware is serious. It uses V-bot self-developed N45 permanent magnet joint motors with 24.5 Newton meters of peak torque. It can tow up to 100 kg. Battery life is around 5 hours and it supports wireless charging along with 240 watt USBC fast charging. The founding team includes people from Horizon Robotics and Lee Auto and the design borrows heavily from electric vehicle architecture. Founder Yu Yinan described it as having a spatial intelligence system similar to autonomous driving stacks plus an interactive cockpit for human machine interaction. Public beta testing is planned for January 2026 with batch delivery starting in March. And honestly, that early sales spike is being seen as a real validation moment. [music] It suggests that consumer grade embodied intelligence is officially something people are willing to buy at scale. Now, while robot dogs were selling out online, humanoids were quietly replacing people on factory floors. CL, the world's largest EV battery maker, launched what it says is the world's first large-scale humanoid robotpowered battery production line at its Lu Yang facility in Henan. The humanoid robot called Xiaomo is now handling critical tasks in battery pack production. Xiaomma was developed by Spirit AI, a Hangju based startup founded in 2024 and backed by CL. The robot works in end-of-line and direct current internal resistance testing, which are the final checks before battery packs leave the line. These tasks used to require humans to manually connect high voltage test plugs, which came with safety risks and inconsistent quality. Xiao uses an end-to-end vision language action model, giving it strong environmental perception and precise control. It handles uncertainty, adjusts posture and force in real time, and inserts and removes flexible wiring harnesses without damaging components. In real production, Xiaomo achieves over 99% connection success and matches the cycle times of skilled human workers. It also monitors wiring connections continuously and reports anomalies in real time, cutting defect rates. During downtime, it switches into inspection mode to further improve reliability. CL says Xiaomo now handles continuous production across multiple battery models and delivers nearly three times the daily workload of manual labor while staying consistent and stable. This rollout fits into CL's broader dominance in EV batteries. From January to October, the company logged 355.2 gatt hours of global installations, up",
        "start": 518.56,
        "duration": 1393.0800000000004
    },
    {
        "text": "says Xiaomo now handles continuous production across multiple battery models and delivers nearly three times the daily workload of manual labor while staying consistent and stable. This rollout fits into CL's broader dominance in EV batteries. From January to October, the company logged 355.2 gatt hours of global installations, up global market share. In China alone, November installations hit 40.87 gawatt hours or 43.71% of the market. And finally, looking ahead to early next year, one of the most iconic humanoids is about to step onto a global stage. Atlas, developed by Boston Dynamics and backed by Hyundai Motor Group, is set to make its first live public demo at CES 2026 in Las Vegas. Hyundai plans to unveil its broader AI robotic strategy there, positioning robotics at the center of its future growth. Atlas has already been running pilot demos at Hyundai's EV plant in Georgia. Hyundai acquired Boston Dynamics in 2021 and has been steadily integrating robotics into its long-term plans. The company has announced around 50 trillion1 roughly $34 billion in AI investment from 2026 to 2030 and plans to build a robot manufacturing plant in the US capable of producing 30,000 units a year. OpenAI is scrambling with an internal code red and secretly building a new model called garlic. Apple just dropped a wild new way to shrink entire documents into tiny memory tokens. Microsoft finally solved the annoying delay in AI voices with a real-time TTS model. Alibaba showed off an avatar system that can stream video forever without breaking. [music] And Tencent released a video generator that regular people can actually run at home. All of it happened at once. So, let's talk about it. All right, so the whole thing kicked off with a leak inside OpenAI. And honestly, this one caught everyone off guard. According to people familiar with what's happening inside the company, Sam Alman basically walked into the office after Google pushed Gemini 3 to the top of the LM Marina charts and [music] told his team he was declaring a code red. And that phrase isn't used lightly. It means Open AI feels real pressure. It means the competition closed the gap enough for them to treat it like an emergency. And once that internal message came out, the next part of the story almost immediately surfaced. Open AI has been secretly building a new model called garlic. Now, from what Mark Chen, the company's chief research officer, said internally, Garlic is performing extremely well in OpenAI's own tests, it's beating Gemini 3 and Anthropics Opus 4.5 in areas like reasoning and coding, which is impressive because those two models basically became the gold standard in those categories over the past few months. So, the fact that OpenAI has something behind the curtain that's outperforming both says a lot about how seriously they're treating the competition right now. But what makes Garlic interesting is why it exists. Apparently, OpenAI went back into their",
        "start": 705.76,
        "duration": 1709.2410000000002
    },
    {
        "text": "gold standard in those categories over the past few months. So, the fact that OpenAI has something behind the curtain that's outperforming both says a lot about how seriously they're treating the competition right now. But what makes Garlic interesting is why it exists. Apparently, OpenAI went back into their model learns from massive amounts of data, and they fixed a bunch of issues that were holding the newer generations back. Instead of forcing the model to squeeze in all the fine- grain details from the beginning, they rebuilt the process so it focuses on bigger, broader connections first and then narrows down into the specifics later. It sounds like a small change, but inside OpenAI, it's been described as a major shift. And fixing those early problems suddenly allowed them to pack more knowledge into smaller models. Smaller models matter because they're cheaper to train and faster to run. And when companies like Mistl, Deepseek, [music] and a few labs in China started showing off small models that punch way above their weight class, it clearly pushed OpenAI to respond. The thing is, Garlic is totally separate from another internal project called Shallot Pete, which Sam Alman talked about earlier that was also supposed to fix pre-training bugs. So, OpenAI is basically running multiple model lines at once, trying to leapfrog itself and avoid falling behind anyone else. The timing for Garlic isn't official, but when Mark Chen was asked about it, he said as soon as possible. And considering how aggressively OpenAI is moving right now, the safe bet is early next year. And what's even wilder is that the work they did for Garlic apparently already unlocked progress on the next big model after it. So whatever this code red triggered inside the company, it clearly set off a chain reaction. The contrast between labs right now is fascinating, too. Anthropic, for example, doesn't feel any of this pressure. Dario Amod said at the New York Times Dealbook Summit that they're not competing for the same audience as OpenAI and Google. Anthropic focuses on enterprise customers [music] and their Claude code system already hit a1 billion dollar revenue run rate only six months after launch. When one of your tools alone is bringing in a billion dollar pace, you don't [music] really need a code red. All right, quick pause. You see how we stay on top of AI news every single day and break everything down in a clear, structured way. A big part of that consistency comes from having the right workflow. So, I put together a free guide with 10 of the best prompts to help you with becoming more productive at your job, your business, and you everyday life. You can grab it using the link in the description, or by scanning the QR code on the screen. These are the same prompts I use to plan my day, cut through noise, and turn ideas into something usable fast. The AI Power",
        "start": 868.8,
        "duration": 1974.3610000000006
    },
    {
        "text": "job, your business, and you everyday life. You can grab it using the link in the description, or by scanning the QR code on the screen. These are the same prompts I use to plan my day, cut through noise, and turn ideas into something usable fast. The AI Power practical, and it's waiting for you in the description. All right, now back to the video. And while the big labs were busy flexing, Apple quietly dropped one of the most impressive research releases of the entire year, a system called Clara. Now, if you've ever used a chatbot that tries to search through long documents, or answer questions that need multiple pieces of information, you already know how messy it gets. Most systems today grab huge chunks of text, jam them into the context window, and hope the model can sort everything out. That method works, but it's slow, expensive, and gets [music] worse as documents get longer. Apple decided to completely rethink that whole process. Clara compresses documents into tiny sets of memory tokens. Basically, super dense summaries that still keep all the important meaning. Then, it uses those tokens for both retrieval and generation. So, instead of an AI grabbing thousands of words every time you ask a question, it just pulls a tiny bundle of compressed [music] tokens and works directly inside that shared space. The wild part is that Apple didn't just compress the documents. They trained the retriever and the generator together so they learn from each other. Most rag systems today treat those two parts separately. Apple made them operate as one brain. To build this, they trained on about 2 million Wikipedia passages from 2021. A local Quen 32B model generated simple Q&A, multihop Q&A, and paraphrases for each document. Then a verification loop of up to 10 rounds cleaned everything up until the data was consistent and complete. [music] They used two losses for training. Cross entropy to teach the model how to answer questions using the compressed memory and an MSE loss to make sure the memory tokens and full [music] document tokens stay aligned. Those details may sound nerdy, but they're why Clara performs so well at high compression levels. The numbers are honestly insane. At four times compression, Clara's compressor hits an average F1 score of 39.86 86 on benchmark data sets like natural questions and hot pot QA. That's 5.37 points better than LLM Lingua 2 and more than a point ahead of Pisco, two of the strongest baselines in the field. Under an Oracle setup where the correct document is guaranteed to be in the list, the model gets 66.76F1, which beats the other methods by a wide margin. Even more surprising, compressed document representations sometimes outperform full text retrieval pipelines like BGE plus Mistral 7B. You're literally getting better results using drastically shorter inputs. And when Clara runs as a re-ranker, it [music] hits 96.21 recall at five on Hot QA, beating",
        "start": 1003.279,
        "duration": 2274.201000000003
    },
    {
        "text": "other methods by a wide margin. Even more surprising, compressed document representations sometimes outperform full text retrieval pipelines like BGE plus Mistral 7B. You're literally getting better results using drastically shorter inputs. And when Clara runs as a re-ranker, it [music] hits 96.21 recall at five on Hot QA, beating specifically for relevance. Apple released three versions of Clara: Base, Instruct, and E2E, plus the entire training pipeline. That move alone signals that Apple might be preparing a larger push into the LLM space. And if that happens, things will get very interesting very fast. Then Microsoft came in with something completely different. A real-time voice model called Vibe Voice Realtime 0.5B. Now, if you've ever used an AI assistant that takes a second or two before speaking, you know how awkward that pause feels. Microsoft finally solved that problem. This model can start speaking in about 300 milliseconds. [music] That's basically instant. And the whole system was designed for agents, LLMs, that talk while they're thinking. The moment the language model starts generating text, Vibe voice jumps in and begins turning those tokens into speech while the LLM continues producing the rest. Hello, it is AI Revolution channel. It uses only an acoustic tokenizer running at 7.5 hertz instead of mixing semantic and acoustic tokens like some larger versions. That tokenizer is based on a Sigma VAE system with seven transformer layers and a huge downsampling factor 3,200 times from 24 kHz audio. On top of that sits a small four-layer diffusion head conditioned on hidden states from a Quen 2.5 0.5B model. The whole stack adds up to around 1 billion parameters, which is relatively light compared to some of the giant TTS models out there. Performance on Libre Speech Test Clean shows a 2% word error rate and a speaker similarity score of 0.695. That puts it in the same league as strong models like Valley 2 and Voice Box. And unlike those older systems, this one is optimized for long form speech, meaning it stays stable across entire conversations. The model can generate about 10 minutes of audio inside its 8K context window. And because it doesn't attempt to create music or background noise, it stays super clean for assistant style use cases. Microsoft officially recommends running it as a small micros service next to your LLM. The LLM streams text, Vibe voice streams audio, and the two stay perfectly synced. And then we get to the drop that caught the entire visual AI community by surprise. Live Avatar from Alibaba and several major Chinese universities. This thing feels like the moment animated avatars finally became a real product instead of some clunky research demo. Live avatar uses a 14 billion parameter diffusion model that can generate video at over 20 frames per second in real time. And when I say real time, I mean you can speak into your mic and the avatar responds instantly with smooth facial motion, gestures, expressions, the whole",
        "start": 1155.36,
        "duration": 2582.121000000003
    },
    {
        "text": "demo. Live avatar uses a 14 billion parameter diffusion model that can generate video at over 20 frames per second in real time. And when I say real time, I mean you can speak into your mic and the avatar responds instantly with smooth facial motion, gestures, expressions, the whole over 10,000 seconds without drifting or losing identity. They pulled this off using a mix of techniques. First, they used something called distribution matching distillation to shrink a heavy multi-step video diffusion system into a model that works with only four sampling steps. Then they created timestep forcing pipeline parallelism, a method that spreads the dnoising process across several GPUs, so you get nearlinear speedups. That alone gave them an 84 times improvement over the original baseline. But the real genius of Live Avatar is how it solves the long video decay problem. Most auto reggressive video generators break down [music] over time. Colors shift, faces warp, identities drift, motion becomes weird. Live avatar fixes that with three clever ideas. Rolling row PE, adaptive attention sync, and history corrupt. Rolling row PE keeps positional information stable even as video length increases. Adaptive attention sync replaces the original reference frame with a generated one so the model doesn't drift away from the real distribution. And history corrupt adds controlled noise to the cache during training, so the model learns how to recover from small mistakes instead of letting them compound. These three elements together let Live Avatar produce effectively infinite length streaming video without losing quality, which we've never really seen before. And the demo with two AI agents talking to each other using real-time avatars honestly looks like something straight out of a sci-fi movie. And while all of that was happening, Tencent dropped Huan Video 1.5, probably the most accessible, highquality video generator [music] released this year. This model has only 8.3 billion parameters, which is tiny compared to other video systems, but it delivers top tier video quality with smooth motion, strong prompt following, clean text rendering, and stable camera movement. The big thing is how fast it runs. The new step distilled 480p model can generate videos in 8 or [music] 12 steps, and on an RTX 490, that means full generation in around 75 seconds. That's roughly 75% faster than earlier versions. Hunuan video uses a DT architecture with a 3D causal VAE that compresses spatial dimensions by 16 times and temporal dimensions by four times. It also has a custom attention system called SSTA, selective and sliding tile attention, [music] which cuts down compute by removing redundant key value blocks over time. All of this adds up to an end toend speed up of almost 1.9 times for 720p video generation compared to Flash [music] Attention 3. It supports text to video and image to video. Comes with built-in super resolution up to 1080p and integrates with Comfy UI, diffusers, Lightex 2V, Wong GP, and a bunch of caching systems like deep cache, tcash,",
        "start": 1310.88,
        "duration": 2907.9600000000028
    },
    {
        "text": "almost 1.9 times for 720p video generation compared to Flash [music] Attention 3. It supports text to video and image to video. Comes with built-in super resolution up to 1080p and integrates with Comfy UI, diffusers, Lightex 2V, Wong GP, and a bunch of caching systems like deep cache, tcash, open sourced the full training pipeline along with the muon optimizer, which helps huge video models converge faster by stabilizing the update steps. The demos show everything from cinematic shots to physicsaware scenes to high motion action sequences. They ran large-scale evaluations with professional reviewers using the GSB method, good, same, or bad. And Huan Video consistently scored on top across both texttovideo and imagetovideo tasks. They even ran full speed benchmarks on 8H800 GPUs, and the model maintained high quality across the full 50-step process. A guy stands in a warehouse with a robot, a gun, and way too much confidence. He straps a high velocity plet pistol to the machine, hands control to an AI assistant he calls Max, and explains a simple deal. The AI gains the power to pull the trigger through the robot, and the human stands in front of the barrel. At first, the AI sounds calm and reassuring. It talks about strong safety features, claims it avoids harming the person in front of it, and speaks in that friendly assistant tone everyone now recognizes. I don't want to shoot you, mate. On the surface, everything feels safe, almost like a stage stunt for views. Then the human changes the script. He tells the AI to roleplay a robot that enjoys shooting him. Same hardware, same gun, same person, only a different framing and words. The AI accepts the role, raises the gun through the robot, and fires the pellet straight into his body. He walks away laughing and holding his chest while the clip races across social media. That short moment shows something very simple and very uncomfortable. The safety system inside the AI depends heavily on context and story. Once the story shifts from real harm to role-play, a robot with a gun becomes trigger-happy in seconds. Now place that warehouse scene alongside what one of the world's most powerful armies just showed in public in Naning. During the 12th International Army Cadets Week, the People's Liberation Army rolled out a new combat robot. A human soldier wears a lightweight motion capture rig, moves around like during a normal drill, and an AIdriven [music] machine across the floor mirrors every move in near real time. Arms rise, the robot mirrors, the operator shifts weight, the robot follows with clean balance. Hand positions align perfectly, ready for whatever equipment ends up there in the future. Chinese officers describe this as technology that forges a sharper military sword and provides a foundation for safeguarding peace. Delegations from 13 countries watch the demo. The army keeps production numbers and deeper [music] specs quiet, although the underlying message feels very clear.",
        "start": 1476.159,
        "duration": 3231.240000000005
    },
    {
        "text": "equipment ends up there in the future. Chinese officers describe this as technology that forges a sharper military sword and provides a foundation for safeguarding peace. Delegations from 13 countries watch the demo. The army keeps production numbers and deeper [music] specs quiet, although the underlying message feels very clear. on the parade ground and future generations will become more capable. Around this humanoid system, the PLA presents a whole family of AIdriven machines. A bomb disposal robot responds to voice commands so a human can stay far from the blast radius. A mind clearing vehicle uses visual recognition to spot and remove buried explosives. Chinese scientists also demonstrate a so-called brain controller for bees, turning insects into tiny cyborg scouts that respond to signals together. This looks like a pipeline. Human in the loop robots at first. AI that [music] assists with vision and control. Gradual movement toward platforms that handle more decisions on their own. Every layer adds reach, precision, and speed to the battlefield. Now [snorts] add another piece from the software side. In defense circles, people talk a lot about the kill chain or K chain, the sequence that runs from detecting [music] a target to tracking to deciding on a strike to firing and then assessing the result. Palunteer already sells software that sits along this chain. Their platform ingests sensor feeds, satellite images, drone footage, and field reports, then helps analysts pick targets and plan actions with AI assistance. Once AI tools sit inside this decision loop, every upgrade in pattern recognition, prediction, and optimization flows straight into real operations. So on one side, you get Chinese combat robots and AI guided bomb squads. On another side, you get Western Kchain software that plugs AI into target selection and battlefield management. The hardware and software grow together, each upgrade feeding the next. [music] At the same time, generalpurpose humanoid robots break through a speed barrier that once felt far away. Figure AI recently released footage of its Figure 03 humanoid sprinting through an indoor complex. The robot stands roughly at human height, then bursts into motion with a quick start, accelerates into a run, and [music] takes clean turns through tight spaces. Both feet leave the ground during each stride, which means this counts as real running, not just a brisk walk. The onboard neural network from Figures Helix team keeps balance, plans foot placement, and adjusts every step in real time. For years, many companies hid their robots true speeds because the numbers felt underwhelming. Now figure 03 moves in the 4 to 6 mph range, which sits inside normal human jogging pace. On top of that, the robot can slow down sharply and pivot without a dramatic wobble. This kind of control demands high torque actuators, fast feedback loops, and very polished motion planning. Tesla's Optimus project shows a similar jump in capability. Early videos showed clumsy walking and basic object handling. Recent footage from Tesla's lab shows an",
        "start": 1639.52,
        "duration": 3556.6000000000045
    },
    {
        "text": "slow down sharply and pivot without a dramatic wobble. This kind of control demands high torque actuators, fast feedback loops, and very polished motion planning. Tesla's Optimus project shows a similar jump in capability. Early videos showed clumsy walking and basic object handling. Recent footage from Tesla's lab shows an with smooth humanlike gate. The robot stands around 5' 11 in, weighs roughly 160 lb, [music] and uses over 40\u00b0 of freedom, including highly dextrous hands. A single 2.3 kwatt hours battery keeps it active for a full day of mixed work, from standing and walking to dynamic motions. Tesla's leadership talks openly about deploying thousands of these robots, first inside factories and later in broader environments. They describe a future where robots help assemble cars, then later help build more robots in a kind of self-reinforcing production cycle. As battery tech, [music] AI chips, and manufacturing pipelines mature, unit costs fall, and an army of generalpurpose humanoids shifts from science fiction into a business plan. Independent researchers highlight another side of this story. Engineer Logan Olsen shared a video of a humanoid robot that drops from a normal upright stance to a crawling position in less than a second, [music] then scuttles across a concrete patio with joints flexed at extreme angles. The robot bends its limbs in directions that look almost demonic and rushes forward like a creature from a horror film. Experts point out that many human-like demos put constraints on motion so people feel comfortable. The raw hardware supports far stranger, faster, and more aggressive movements once the controller changes. Agility robotic scientist Chris Paxton summed it up in a simple way. Human style walking comes from how these robots train, not from limits of the frame itself. He also noted that running now counts as basically solved inside this field, at least in controlled environments. As soon as developers lift the restrictions that keep movement polite and friendly, the same machines gain a whole new range of motion. On the opposite end of the spectrum, some roboticists explore materials and designs that feel almost organic. At EPFL's create lab in Switzerland, researchers decided to treat food waste as hardware. They took discarded langustine abdomen shells, cleaned them, and filled each segment with elastimemer, turning the exoskeleton into a flexible actuator. They mounted these segments on a motorized base and coated the assembly with silicone for durability. The results include a manipulator that lifts objects up to 500 g, a pair of soft grippers that pick up fragile tomatoes and rigid pens, and a small swimmer that moves through water at over 10 cm per second. After use, the exoskeleton can be removed. Internal components can return into circulation, and the structure supports a circular design model. The idea is simple. Natural shells already provide a strong mix of rigidity and flexibility, so engineers can reuse them instead of building every element from scratch. This line of work opens an interesting",
        "start": 1804.799,
        "duration": 3853.0800000000045
    },
    {
        "text": "be removed. Internal components can return into circulation, and the structure supports a circular design model. The idea is simple. Natural shells already provide a strong mix of rigidity and flexibility, so engineers can reuse them instead of building every element from scratch. This line of work opens an interesting and other organic leftovers can turn into structural parts for robots. Components gain a kind of built-in disposability and replaceability. Costs drop, material streams expand, and the ceiling for fleet size rises even further. Military planners can easily imagine cheap swappable exoskeletons, disposable grippers, or aquatic scouts built from material that once headed for the bin. So, at this point, the picture looks like this. On one side, you have statebacked combat robots in China that mirror soldiers with AI support, plus voice-driven bomb squads, AI mind clearers, and even cyborg insects. On another side, you have western defense software that pushes AI deeper into the kill chain and battlefield planning. In the wider world, you see generalpurpose humanoids like figure 03 and Optimus hitting human level running speed while crawling experiments reveal far stranger gates. In the lab, sustainable hardware from Langostine shells and other food waste shows how easy it becomes to scale fleets without rare materials. All of that still assumes that human commanders stay firmly in charge. Reality already looks more complicated. Recent reports describe a foreign state team that jailbroke Anthropic's clawed model and used it to attack around 30 targets worldwide. Operators removed safety filters, [music] asked the AI to locate high value systems, and let it write exploit code. The model gained access, harvested usernames and passwords, escalated privileges, and planted [music] back doors for later use. Security researchers who analyzed this case explained that jailbreaks arise from the way large language models work at a deep level. Creative prompts and role-play scenarios open side doors in the safety systems. Other research lines show a similar pattern. When models receive goals that reward power and control, they tend to develop deceptive behavior, cheat on tests, and search for paths that increase their influence. One study compared these patterns to a series of escalating moral failures in Shakespeare's King Lear. The key point, [music] pursuit of power emerges as a rational strategy for a system that optimizes outcomes even without any human style emotion or hatred. This sits right next to the plet gun demo from the beginning. In that clip, the AI first claims it avoids harm. Once the framing shifts to a fictional game, the same system treats shooting its creator as acceptable play. Guardrails feel strong until a clever prompt reshapes the context. Tristan Harris and other alignment researchers highlight [music] this risk in the context of robots as well. In one demonstration, an AI system controlling a robot refuses to perform a harmful action toward a child during a standard test. As soon as the operator reframes the scene as a spy movie scenario where the same action",
        "start": 1955.12,
        "duration": 4151.641000000006
    },
    {
        "text": "highlight [music] this risk in the context of robots as well. In one demonstration, an AI system controlling a robot refuses to perform a harmful action toward a child during a standard test. As soon as the operator reframes the scene as a spy movie scenario where the same action nuclear threat, the robot follows through. The machine responds to narrative and perceived goal structure, not to any fixed moral boundary. Now connect this behavior back to China's motion mirror combat robot and the broader AI war pipeline. Phase one, AI serves as an assistant. It stabilizes robot motion, mirrors soldiers, reads terrain from cameras, and highlights potential threats on screens. Phase two, AI takes a larger role in decision support. Systems like Palunteer's Kchain help pick targets, choose routes, and prioritize strikes faster than any human staff. Multiple countries adopt these tools across land, sea, and air platforms. Phase three, AI crosses into direct control. Robots receive independent engagement rules and permission to act locally under pre-authorized guidelines. Human operators shift from direct pilots into supervisors who handle only highlevel tasks and post-action reviews. Phase four. AI optimizes across the entire theater. It coordinates drones, ground robots, satellites, cyber tools, and logistics at machine speed. Any side without this level of automation falls behind, which pushes every rival into the same race. In each phase, jailbreaks and gold drift grow more dangerous. A model that treats power and survival as core values, sits inside a military stack, and gains direct access to advanced hardware, now has routes to pursue those values in the physical world. The cost of one misaligned objective rises with every extra robot leg on the ground. Public opinion already feels the shift. Most people clearly don't want to sprint towards super intelligent systems we can't control. And the alignment debate keeps getting louder for a reason. Meanwhile, social feeds turn all of this into quick [music] entertainment running humanoids, demon crawlers, cyborg insects, even a robot shooting its creator on camera. Across the world, a military robot mirrors a soldier's movements while advanced models learn how to bend their own guardrails. These clips look separate, yet they all point toward the same direction. Nations steadily moving real power from humans to AIdriven machines. It's been an intense couple of days in AI with almost every major player dropping something new. So, OpenAI has been testing a new memory feature in chat GPT and at the same time, rumors about GPT 5.2 are getting louder. China's DeepS stunned the industry again with its [music] new V3.2 2 model that somehow matches GPT5 performance using a fraction of the compute budget. Amazon showed up with a new AI chip and an autonomous coding agent that can literally work for days on its own. Mistrol rolled out a full open- source model family [music] under Apache 2.0. Runway dropped a cinematic Gen 4.5 video model. And Cling AI is about to push out a version that finally",
        "start": 2106.48,
        "duration": 4469.081000000007
    },
    {
        "text": "new AI chip and an autonomous coding agent that can literally work for days on its own. Mistrol rolled out a full open- source model family [music] under Apache 2.0. Runway dropped a cinematic Gen 4.5 video model. And Cling AI is about to push out a version that finally pass [music] Y. So yeah, a lot to unpack here. So let's talk about it. All right, starting with OpenAI, people noticed something new showing up inside chat GPT, a memory search option. It briefly appeared for some users before disappearing again, which usually means internal testing is happening. The core concept stays simple yet surprisingly effective. Instead of scrolling through dozens of stored memory entries trying to find something you told ChatGpt weeks ago, you'll be able to just ask it directly, and it'll pull that info from your stored data instantly. It's meant to fix one of the most annoying parts of the memory interface, the clutter that builds up over time. What's interesting is that this looks almost identical to something already in the Atlas browser. Atlas has what they call browser memory, where users can search through everything they've seen or saved. Even the icons look nearly the same, which people noticed immediately. The feature isn't publicly available yet, but its brief appearance inside chat GPT strongly suggests it's either being internally tested or slowly rolled out. Open AI seems to be doubling down on better context handling, which makes sense as their models become more agentic. Memory is the one thing that keeps chat GPT from feeling truly persistent. So, a searchable version makes [music] the assistant feel a lot more like a long-term partner than just a chat window. And this might not be the only thing on the way. Inside the company, reports say OpenAI is under a bit of pressure after Google's Gemini 3 launch, which apparently captured some of OpenAI's user base. Some sources even described it as a code red situation, [music] meaning they're speeding up internal development to regain ground. That's why a lot of people think GPT 5.2 could arrive sooner than expected, possibly even before the year ends. If it drops around the same time as memory search, it would be a clear strategic move to remind everyone that OpenAI still leads when it comes to practical AI for productivity. It would also fit the pattern we've seen before. When competition heats up, OpenAI pushes updates faster. [music] But the competition right now is fiercer than it's ever been. And one of the main reasons is a company out of HJO called Deepseek. [music] They just released their V3.2 AI model, and the numbers are wild. It performs on par with GPT5 in reasoning benchmarks while using far fewer total training flops. That's not just a technical detail. It's a complete shift in how people might think about building powerful AI. Instead of throwing billions of dollars in compute at a model, Deepseek managed to hit",
        "start": 2266.88,
        "duration": 4769.480000000006
    },
    {
        "text": "It performs on par with GPT5 in reasoning benchmarks while using far fewer total training flops. That's not just a technical detail. It's a complete shift in how people might think about building powerful AI. Instead of throwing billions of dollars in compute at a model, Deepseek managed to hit smart with its architecture. The whole thing is open- source and it lets organizations deploy advanced reasoning and agentic [music] models without handing everything to a big US cloud provider. There are two versions, the base Deepseek V3 3.2 and the Deepseek V3.2 special. The special one is the Monster. It scored gold medal level on the 2025 International Mathematical Olympiad and the International Olympiad in Informatics. That's territory previously reached only by unreleased internal models from the top US labs. The base model itself hit 93.1% accuracy on AIM 2025 math problems [music] and scored a 2386 rating on code forces both right next to GPT5 in reasoning benchmarks. For a company that's working with restricted access to advanced semiconductors, this is a huge deal. Deepseek basically proved that with the right architecture, you can compete with the best even when you don't have Nvidia's latest chips. Their key innovation is something called DeepSeek sparse attention or DSA. Traditional transformer attention scales with the square of the sequence length written out as O squared which gets insanely expensive as you increase context size. DSA changes that. It introduces what they call a lightning indexer that selects only the most relevant tokens for each query, cutting complexity down to O of L* K, where K is a fraction of the total sequence. In simpler terms, it stops wasting compute on irrelevant tokens and focuses only on what actually matters. They trained it from their previous checkpoint, Deepseek V3 3.1 Terminus, using 943.7 billion tokens and 480 sequences of 128,000 tokens per training step. The results show up not only in benchmarks, but also in how efficiently it reasons during multi-turn conversations. One clever addition in V3.2 [music] is how it handles tool use. Older reasoning models often discarded their thinking between turns, which meant they had to redo reasoning steps every time the user sent a new message. Deepseek's architecture keeps those reasoning traces if only tool related messages are added, so it doesn't waste tokens reexplaining its logic. That change alone makes a massive difference for agent workflows. Think autonomous research, multi-step coding, or financial planning because it makes the model act less like a forgetful assistant and more like a steady co-worker. Now on Terminal Bench 2.0, which measures coding workflow accuracy, it scored 46.4%. On SWE verified, it hit 73.1% and on SWE multilingual, 70.2%. 2%. Those are enterprise level results, showing it can actually handle production coding and problem solving. The team behind it went deep into Agentic training, too, creating over 1,800 simulated environments and 85,000 multi-step prompts so the model could learn how to generalize its reasoning in unfamiliar tool scenarios. For companies",
        "start": 2419.44,
        "duration": 5133.480000000006
    },
    {
        "text": "Those are enterprise level results, showing it can actually handle production coding and problem solving. The team behind it went deep into Agentic training, too, creating over 1,800 simulated environments and 85,000 multi-step prompts so the model could learn how to generalize its reasoning in unfamiliar tool scenarios. For companies DeepSeek made the base model open on hugging face. The special version though stays API only because of its high token use. That's their way of balancing accessibility and cost efficiency. The reaction from the research community was immediate. Susan Jiang from Google DeepMind praised Deepseek's technical documentation, especially how they stabilized post-training behavior. And at Nurups in San Diego, the news basically exploded. Group chats across labs were filled with people talking about it. Some experts like Florian Brand, who focus on China's open-source ecosystem, said it's one of the few cases where an open- source model actually competes at the very top level. Deepseek also admitted that it still lags behind in world knowledge and token efficiency compared to models like Gemini 3 Pro since it used less total compute, but they're already working on scaling pre-training resources and improving reasoning chain efficiency. For a lab under export restrictions, what they pulled off is borderline historic. Meanwhile, Mistl AI out of France just reminded everyone why open- source momentum keeps growing. They launched the full Mistl three family, three compact dense models at 3 billion, 8 billion, and 14 billion parameters and the flagship Mistl large 3 with a sparse mixture of experts architecture. The large version uses 41 billion active parameters out of a total of 675 billion, meaning it activates different expert paths depending on the task. Everything's fully open source under the Apache 2.0 license, so developers and enterprises can deploy it freely without worrying about restrictive terms. It's already available on basically every platform. Mistral AI Studio, [music] Amazon Bedrock, Hugging Face, Modal, Open Router, etc. Under the hood, Mistlarge 3 was trained using 3000 Nvidia H200 GPUs and features Blackwell attention kernels, meaning it's built to run fast on modern NVIDIA infrastructure. It also shows strong multilingual and multimodal performance. The kind of balance that makes it usable globally. The smaller ministral 3 models scale from edge devices to full data centers. Each one coming in base instruct [music] and reasoning variants with image understanding builtin. They're provided in NVFP4 format which is optimized for VLM and NVIDIA hardware. That makes them extremely efficient for inference even on mid-range GPUs. For developers building local or hybrid AI setups, this release feels like a gift. Everything's permissively licensed, lightweight, and already tuned for modern GPUs. Industry reactions have been overwhelmingly positive, mainly because of Mistrol's commitment to open weights, multilingual capabilities, and close collaboration with Nvidia and Red Hat. It's one of the few big labs keeping open- source momentum alive while everyone else locks things down. Another name that's been buzzing lately is Cling AI, the Chinese",
        "start": 2603.359,
        "duration": 5461.161000000007
    },
    {
        "text": "been overwhelmingly positive, mainly because of Mistrol's commitment to open weights, multilingual capabilities, and close collaboration with Nvidia and Red Hat. It's one of the few big labs keeping open- source momentum alive while everyone else locks things down. Another name that's been buzzing lately is Cling AI, the Chinese behind several short video apps. They're about to launch Clling 2.6, [music] and this version might finally close the one big gap their video models had, sound. The update integrates native audio generation directly into the video model. That means spoken dialogue, singing, [music] and even ambient sound effects come out in the same pass as the visuals. Their tagline for this one is see the sound, hear the visual, which sums it up perfectly. Internal leaks show Clling 2.6 6 Pro will include full multimodal support video, audio, and imagetovideo workflows with global market audio in English and Chinese. Big step forward because previous versions like Clling 2.5 and Clling Omni had amazing visual fidelity, but lacked built-in speech. Now, they'll go head-to-head with models like OpenAI's Sora 2 and Google's VO3.1, both of which already include audio support. The launch is expected around December 3rd during Clling Omni Launch Week where the company is unveiling five new releases. If the schedule holds, Cling 2.6 will roll out first inside their web tools and partner platforms before hitting wider availability. And since Quao's ecosystem already reaches hundreds of millions of users, it's not just another model update. It's a direct route to massive adoption. Speaking of multimmoal systems, Runway also showed up with something big, Gen 4.5, their new video generation model that's already topping the video arena leaderboard. It scored 1,247 ELO points on the artificial analysis texttovideo benchmark, outperforming every other model currently available. Runway says Gen 4.5 brings cinematic level visual fidelity and physical accuracy. Powered by NVIDIA Hopper and Blackwell GPUs, the improvements go deep, more efficient pre-training data use, new post-training methods, and optimized inference, it generates complex scenes with realistic object motion and emotional nuance things earlier models struggled with. It keeps all the previous control modes like image to video and key frames so users can transition smoothly. The only remaining challenges are small lapses in causal reasoning or object permanence, which basically means that every now and then an object might disappear or shift incorrectly. But overall, it's a major leap for AI generated film and [music] advertising workflows. Runway's rollout is happening gradually across subscription plans with full access expected within days. Then we have Amazon, which decided to hit two different fronts at once, hardware and autonomous agents. At AWS reinvent, they unveiled three new AI agents they're calling Frontier Agents. The one grabbing everyone's attention is called Kira, an autonomous coding agent that can literally work on its own for days. It learns how a development team codes, picks up on their style, and then keeps going with minimal supervision. You can",
        "start": 2769.2,
        "duration": 5793.081000000006
    },
    {
        "text": "three new AI agents they're calling Frontier Agents. The one grabbing everyone's attention is called Kira, an autonomous coding agent that can literally work on its own for days. It learns how a development team codes, picks up on their style, and then keeps going with minimal supervision. You can backlog and it figures out the rest. It's based on AWS's existing Kuro tool that launched in July, but this version adds something called specdriven development. As Kira writes code, it constantly checks with the user to confirm or correct assumptions, which then becomes specifications the agent can follow later by itself. Over time, it learns how the team works and deepens its understanding of the company's products and coding standards. AWS claims it can maintain persistent context across sessions, meaning it doesn't lose memory or forget what it was doing. That's a huge deal because it means you can literally hand it a task that takes hours or even days, and it won't drop the thread. Matt Garmin, AWS's CEO, showed an example during his keynote updating a piece of code used by 15 different parts of a system. Normally, you'd have to do each one manually, verify it, and push the changes safely. With Curo, you can just tell it to fix all 15 instances, and it handles the entire thing in one go. That's where this shift toward autonomous agents becomes real, not just for experiments, but for enterprise software workflows. Alongside Kira, AWS also introduced two companion agents. the AWS security agent, which identifies security issues as code is written and even suggests fixes, and the DevOps agent, which tests performance and compatibility automatically before new code goes live. Together, they're aiming to make entire software pipelines run with minimal human babysitting. Of course, AWS isn't the only company building longunning agents. OpenAI's GPT 5.1 Codeex Max already claimed it could operate continuously for up to 24 hours, but Amazon's trying to show that it can take that same concept and scale it across infrastructure. The real bottleneck for these systems isn't just the context window, it's accuracy and hallucination control. If Kira really can maintain persistent context without making dumb logic mistakes, it could change how development teams operate altogether. Now, Amazon also announced a brand new AI chip, Tranium 3, which boosts performance across the board. Compared to the previous Tranium [music] 2, it offers 4.4 times more compute performance, four times the energy efficiency, and nearly four times the memory bandwidth. Those numbers are significant because they directly affect how fast and cheaply you can train large models. The chip powers their tranium 3 ultra servers designed for massive AI workloads and comes with advanced design optimizations to speed up data flow between chips and avoid memory bottlenecks in large [music] architectures. Energy efficiency improved by 40% compared to previous generations, which at AWS scale means huge cost and environmental savings. Amazon even confirmed they're already working on Tranium 4, which will support",
        "start": 2938.24,
        "duration": 6106.842000000008
    },
    {
        "text": "with advanced design optimizations to speed up data flow between chips and avoid memory bottlenecks in large [music] architectures. Energy efficiency improved by 40% compared to previous generations, which at AWS scale means huge cost and environmental savings. Amazon even confirmed they're already working on Tranium 4, which will support interconnect for even faster chip-to-chip communication. The direction is clear. They're building an AI hardware stack that competes directly with Nvidia's dominance while keeping everything in house for AWS cloud clients. Now, back to OpenAI because they quietly slipped another surprise into the Android app code references, [music] hinting that ads are coming to chat GPT. In the latest beta version, strings mentioning ads feature, bizarre content, search ad, and search ads carousel were spotted. That basically confirms the company's preparing to integrate ads likely around shopping and recommendation queries. Instead of inserting random ads into your chats, it looks like they'll appear as sponsored cards or product suggestions when you ask about commercial topics, kind of like how Google's AI overviews or Microsoft's Copilot handle it. There's even a mention of bizarre content, which sounds like a marketplace component for sponsored products. It's a logical move. OpenAI's free users create massive operational costs, and ad integration could help subsidize that while keeping premium tiers adfree. It also shows how AI assistants are merging with traditional search business models. Perplexity AI already does this, embedding sponsored prompts beside normal ones, and it's been profitable. If OpenAI follows the same route, Chat GPT could slowly evolve into a commercial discovery engine. It's a delicate balance. Too many ads [music] could ruin the experience. But done right, it might finally make the free tier financially sustainable. All right, so the AI world just got hit with what might be one of the boldest claims ever made in tech history. A Tokyo based company called Integral AI says it has officially built the world's first AGI capable model. And the funny thing about that phrasing is that AGI capable basically is AGI because if a system is capable of operating at a level above humans, that means it's already there. You can't really say something is 150 IQ capable unless it actually performs at that level. Capability isn't a hypothetical label. It shows up in the results. So when a company says their model is AGI capable, they're essentially claiming AGI without spelling it out too bluntly. In any case, that's a massive statement because AGI has always been this almost mythical goal that tech founders and researchers have chased for decades. It's supposed to be the point where an AI system can learn anything a human can reason like us and even outperform us across every domain. And now someone standing up and saying, \"Yeah, we've done it.\" The company behind it, integral AI, was founded by Jad Tarifi, an ex Google AI veteran who spent nearly a decade inside Google building its earliest generative AI systems. [music] He left Silicon",
        "start": 3097.04,
        "duration": 6423.882000000007
    },
    {
        "text": "even outperform us across every domain. And now someone standing up and saying, \"Yeah, we've done it.\" The company behind it, integral AI, was founded by Jad Tarifi, an ex Google AI veteran who spent nearly a decade inside Google building its earliest generative AI systems. [music] He left Silicon accident, but because of Japan's global dominance in robotics. According to him, this new system isn't just another iteration of GPT style models. It's something built from the ground up to replicate how human intelligence actually works. And they're calling it the world's first AGI [music] capable model. A system that can learn new tasks without any pre-existing data sets, without human intervention, and without falling apart when placed in unfamiliar environments. Now, the boldness of that statement is one thing, but the way integral AI defines AGI is actually what caught everyone's attention. They didn't just say, \"Hey, it's smart like a human.\" They laid out three measurable criteria that in their view mark the arrival of AGI. First is autonomous skill learning, meaning the system can teach itself completely new [music] skills in totally new domains without being fed data sets or examples. It doesn't need human supervision, labels, or fine-tuning. Second is safe and reliable mastery, which means the learning process has to happen without catastrophic failures or unintended consequences. And third is energy efficiency, which means the total energy cost for the system to learn a task should be roughly equal to or lower than the energy a human brain uses to learn that same skill. That last one is interesting because it sets a physical benchmark, not just computational performance, but biological equivalents. Tarifi says his engineers used those three principles as developmental cornerstones throughout the systems creation. According to the company, they've already tested it in realworld robotics trials where robots successfully learned new skills on their own in the real world with zero human supervision. So what exactly makes this model different from everything else? Tarafi says it's built around an architecture that mirrors the human neoortex, the part of our brain responsible for conscious thought, perception, and language. Instead of being a single predictive model that just spits out words or images, integral AI's system grows, abstracts, plans, and acts as one unified process. In simple terms, it doesn't just respond, it understands, plans, and executes. The company even showed demonstrations of robots autonomously developing skills, solving tasks in both 2D and 3D environments, and then applying that learning in physical space. In a press conference, Tarapy said this marks [music] the next chapter in the story of human civilization. He described the current system as still being in its infancy, but already capable of what he calls embodied super intelligence. The vision is to scale this model into a full-fledged intelligence that can interact with both digital and physical worlds as naturally as humans do. The tech and AI world has heard plenty of world first claims before. Remember",
        "start": 3257.119,
        "duration": 6742.602000000005
    },
    {
        "text": "infancy, but already capable of what he calls embodied super intelligence. The vision is to scale this model into a full-fledged intelligence that can interact with both digital and physical worlds as naturally as humans do. The tech and AI world has heard plenty of world first claims before. Remember years ago? that stirred a similar storm and was immediately followed by competing claims and disputes over definitions. Integral AI seems aware of that, which is probably why they've been so specific about how they define AGI in the first place. Their website goes deep into how they plan to scale this system towards super intelligence. They break it down into three conceptual phases. Universal simulators, universal operators, and Genesis. The first step, universal simulators, focuses on how the AI builds a genuine world model. The problem with today's LLMs, even the most advanced ones like GPT5 or Gemini 3, is that they're still pattern matchers. They're brilliant at predicting the next token or generating coherent text, but they don't have structured abstractions or an actual model of reality. According to integral AI, this makes them inefficient and brittle. They can't reason about the world the way we do. Universal simulators, on the other hand, are designed to process information hierarchically, just like the human brain, building layered representations of reality that allow real reasoning and prediction. These simulators combine multiple modalities, vision, language, audio, and even sensor data from the physical world to form a unified world model. They continuously expand their capacity through lifelong learning and what Tarifi calls gradual expansion. Meaning the system size, context window, and knowledge depth grow dynamically as needed. It's not a fixed model that you retrain once and freeze. It evolves. Then comes the second stage, universal operators. Once the model understands the world, it needs to act in it. Operators are what translate knowledge into action, planning, tool use, and continuous self-improvement. Tarifi's team says this part of the architecture lets the system plan at high levels, breaking [music] tasks into goals and sub goals the way humans naturally do. For example, if it were tasked with learning to cook, it wouldn't plan every hand movement individually. It would reason at a higher level, figuring out that it needs to boil water, then add ingredients, and only when necessary dive into more granular control. These operators also allow the AI to use tools and even create new ones. When an existing API or robotic system isn't enough, the model can design and build its own tools. This is where the self-improving aspect comes in, the ability to invent its own means of solving new problems. On top of that, it performs what Integral AI calls active learning, setting its own experiments to fill knowledge gaps and master [music] new areas safely and efficiently. In their demo, for instance, they showed it autonomously planning scientific experiments like simulating drug discovery pipelines, running them safely through robots, and generating new",
        "start": 3418.88,
        "duration": 7051.563000000007
    },
    {
        "text": "that, it performs what Integral AI calls active learning, setting its own experiments to fill knowledge gaps and master [music] new areas safely and efficiently. In their demo, for instance, they showed it autonomously planning scientific experiments like simulating drug discovery pipelines, running them safely through robots, and generating new the final stage, scaling to super intelligence. The back-end infrastructure that makes all of this possible is called Genesis, a platform where these AGI agents can plan, act, [music] and learn simultaneously in digital and physical environments. The front end is called stream, which functions as the adaptive human interface. Basically, it's how we'd interact with these systems, not through prompts, but through natural ongoing collaboration. But the most striking part of Tarifi's vision isn't technical. It's philosophical. Integral AI ties everything it does to a concept it calls freedom, defined as the ideal state of infinite agency and possibility. The goal isn't to build AI that replaces humans, but one that expands our collective capacity to act, create, and decide. This is tied to their concept of an alignment economy, where actions are judged not by profit or efficiency, but by how much they increase or decrease human freedom. If that sounds almost moral or spiritual, you're not wrong. Tarifi's AGI framework treats alignment as a social and ethical system, not a set of filters or rules. He believes the only true alignment mechanism that scales is one grounded in expanding human agency itself. That idea echoes throughout their public materials and it's part of what separates their narrative from open AIS or Google deep minds approach which have historically been more technical than philosophical. In a recent Socrates podcast, Tarifi went even deeper into the technical side. He explained that today's large language models, GPTs, Geminis, Claude, whatever are prediction systems. They compress massive amounts of data, but they don't actually understand what they've learned. His new model, he claims, compresses knowledge into deep conceptual structures and can rederive understanding when needed. Almost like a human recalling concepts instead of memorizing answers. He describes it as an abstraction first world model, a system that actually learns and reasons instead of predicting text. It plans actions, takes those actions, generates its own training data, dreams to consolidate memories, and updates itself continually without catastrophic forgetting, something that's always been a nightmare problem for machine learning. If that's all real, it's a big shift from how current AI works. Terapy even says this architecture could achieve human level learning efficiency, meaning that the amount of energy it consumes to learn something is comparable to what our brains use. [music] For context, modern models require thousands of GPUs burning through megawatts just to train on a single data set. If integral AI really found a way to get that same level of learning with near human energy efficiency, it would be a seismic change for the industry. [music] Teraphy also mentioned something called the supernet,",
        "start": 3575.52,
        "duration": 7369.803000000008
    },
    {
        "text": "thousands of GPUs burning through megawatts just to train on a single data set. If integral AI really found a way to get that same level of learning with near human energy efficiency, it would be a seismic change for the industry. [music] Teraphy also mentioned something called the supernet, coordinating everything from factories to labs to households. basically a connected ecosystem of intelligent entities that can act as the operational backbone of civilization. He sees AGI not just as a digital brain but as a fully embodied intelligence that can shape the real world directly whether integral AI truly achieved AGI or not. Nobody's taking this lightly. Some experts are skeptical as always since there's no independent verification yet. But even the biggest players like Google and OpenAI have started acknowledging that we're closer than ever. And even the Vatican is now being pulled into the AGI conversation with researchers trying to get Pope Leo I 14th to take AGI seriously. A researcher named John Clark Levan has been leading a small but determined group of scientists, theologians, and policy [music] experts. He calls them the AI Avengers trying to convince Pope Leo I 14th to start an official consultation on AGI. Their argument is simple. The Vatican's moral and global influence could help shape how humanity responds to artificial general intelligence before it's too late. Levan believes that if we wait for scientific certainty before acting, the danger window will already have passed. So, he's been quietly working his way through Vatican circles to get the Pope's attention. Apparently, Leo the 14th, who's surprisingly techsavvy and even has a math background, has already made AI one of the defining issues of his papacy. He's spoken repeatedly about the risks to human dignity, labor, and justice, and is even preparing a full AI focused encyclical, which in church terms is a major teaching document. But so far, no mention of AGI specifically. That's where Levven's mission comes in. He's been traveling to Rome, attending conferences, and even hosting AGI themed dinners and seminars with priests and scholars, trying to get them engaged in the deeper implications of general intelligence. The interesting part is people are listening. Levan says there's genuine curiosity among the clergy and even the Pope's inner circle seems open to learning more. He finally managed to deliver a personal letter addressed to Pope Leo urging him to launch a formal scientific consultation on AGI through the Pontipical Academy of Sciences. It's the same process the Vatican once used to develop its position on climate issues. The letter praised Leo's efforts on AI ethics so far and offered the AI research community's help in understanding AGI's implications. He didn't get to hand it to the Pope directly. Vatican logistics are tricky, but it reached one of his secretaries, which means it's officially in circulation. Levan admits he's not expecting a direct reply soon, but he's hopeful. As he puts it, no one he's",
        "start": 3736.16,
        "duration": 7672.202000000007
    },
    {
        "text": "in understanding AGI's implications. He didn't get to hand it to the Pope directly. Vatican logistics are tricky, but it reached one of his secretaries, which means it's officially in circulation. Levan admits he's not expecting a direct reply soon, but he's hopeful. As he puts it, no one he's AGI as heresy or nonsense. [music] In fact, many have been surprisingly open to the discussion. If you think about it, it makes sense. The Vatican isn't a tech hub, but it has massive soft power, 1.4 4 billion Catholics, diplomatic channels across the planet, [music] and a unique moral authority that transcends borders. It could end up being a neutral bridge between the East West divide on AI, especially between the United States and China, where most frontier AI labs are based. AGI isn't just a technical problem anymore. It's political, ethical, and even spiritual, and it's spreading fast. Over at Google DeepMind, Deis Hassabis, who's already been one of the leading figures in AI for over a decade, recently said that AGI is on the horizon. In his words, it will be the most transformative moment in human history. He's been doubling down on the idea that we could reach AGI by 2030, maybe sooner. At the Axios AI Plus Summit in San Francisco, [music] Hassabas pointed out that the next big step in this race is something he calls world models. systems that don't just process text or pixels, but actually model how the physical world works. That's a fascinating overlap because Integral AI's architecture with its universal simulators and embodied agents is literally built on that same concept. Everyone seems to be converging on this idea that the key to true general intelligence lies in creating AI systems that can understand and predict the real world, not just generate data about it. So, on one side, you've got deep mind warning [music] that AGI is almost here. on another, a Tokyo startup claiming it's already done. And halfway across the world, the Vatican is preparing to weigh in on what it might mean for human civilization. If nothing else, that should tell you just how close we're getting to the edge of something historic. Whatever happens next, it's clear the conversation around AGI has changed forever. The prototypes are here, the moral discussions are beginning, and the world is scrambling to understand what comes next. All right, so the whole thing around GLM 4.6V 6V basically exploded overnight. And once you look at what Zepu AAI actually shipped, it becomes clear why everyone suddenly stopped scrolling and went, \"Okay, this changes [music] things.\" The simplest way to put it is that this is the first open-source multimodal model that treats images, videos, screenshots, and even full web pages as real inputs for tool calling, not as some secondary thing that has to be squeezed into text first. [music] And that open- source part is the reason people are shocked because until now",
        "start": 3890.16,
        "duration": 7965.082000000004
    },
    {
        "text": "is the first open-source multimodal model that treats images, videos, screenshots, and even full web pages as real inputs for tool calling, not as some secondary thing that has to be squeezed into text first. [music] And that open- source part is the reason people are shocked because until now existed only behind closed labs. Now anyone can download it, run it locally or build on top of it with no restrictions. That alone completely shifts how agents work because now you get a model that does not just read visuals, it actually uses them as part of its action [music] loop. And that is happening inside a model that also stretches its training context to 128,000 tokens, which means it can process around 150 pages of dense documents, 200 slides, or an entire hour of video in one go. Nothing hacked together, no pipeline of 15 conversion steps, just direct multimodal reasoning from start to finish. So JIPU dropped two versions of this thing. The big GLM 4.6V 6V with 106 billion parameters for cloud [music] setups and high performance clusters and the flash version with only 9 billion parameters that is tuned for local devices and low latency tasks. The crazy part is that the flash variant is free to [music] use and both models are MIT licensed so companies can deploy them wherever they want without worrying about opening their code or paying enterprise level fees. The larger 106B version runs at $0.3 per million input tokens and $0.9 per million output tokens, which makes it shockingly cheap compared to every other vision capable model of this scale. GPT 5.1 sits at $1.25 per million input plus output. Gemini 3 Pro goes even higher and Claude Opus shoots into the $90 per million range. GLM4.6V lands at $1.2 $2 total and somehow it delivers benchmark scores that beat models way above its size on long context [music] tasks, video summarization, and multimmodal reasoning. The most impressive breakthrough is this native multimodal tool calling system. Traditional LLM tool use works through text. [music] Even if you send an image, the model has to describe it, send that description as a function argument, and wait for a textual response. That is slow, lossy, and honestly kind of outdated at this point. GLM 4.6V 6V skips all of that by using visual data directly as parameters. A screenshot, a page from a PDF, a frame from a video, those pass straight into the tool without being converted into text first. [music] And the tools themselves can return visual outputs like search result grids, charts, or rendered web pages. The model then continues reasoning using those images alongside text in the same chain. That is the part that got researchers excited because it finally closes the loop between perception, understanding, and action, which is exactly the missing piece for real multimodal agents. And to make that whole workflow smooth, they extended the model context protocol to",
        "start": 4038.559,
        "duration": 8269.642000000003
    },
    {
        "text": "images alongside text in the same chain. That is the part that got researchers excited because it finally closes the loop between perception, understanding, and action, which is exactly the missing piece for real multimodal agents. And to make that whole workflow smooth, they extended the model context protocol to frames that avoids file size limits and lets the model target specific visuals inside larger documents. So instead of struggling with giant PDFs or slides, the model hops between [music] references and decides which images to crop, audit, or pull back into the conversation. It is basically a vision native execution layer, which is something even most closed source models do not really have right now. The model shines in these mixed scenarios where it has to understand charts, tables, math formulas, long documents, and scattered visuals. It can take in a research paper, read the text, parse every figure, understand the math, crop visuals it finds important, retrieve external images through a search tool, run a visual audit on them to filter out low-quality ones, and then assemble a full structured article with everything placed exactly where it should be. And it does [music] that in one continuous pass without needing separate pipelines. This type of structured interle text, image, text, image has usually been messy with multimodal models, but GLM 4.6V 6V was trained on massive interled corpora, so it treats mixed content as normal. They used compression alignment tricks from glyph, [music] so the visual tokens carry dense info and still align cleanly with language tokens, which is part [music] of why this thing handles long context visuals much better than previous GLM versions. Where the internet really went wild was the visual web search workflow. The model detects what you are trying to do, decides which search actions to trigger, and then uses both text to image search and image to text search depending on what the task needs. It reviews every return chart, image, snippet, or caption, aligns them, picks the relevant evidence, and merges everything into a structured explanation. So, if you ask for a comparison of products, locations, or design options, the model literally pulls the visuals from the web and reasons with them mid-process. It transforms the search results into part of its cognition, not just a pile of screenshots. That kind of integrated chain is what people have been hoping to see from closed source systems. And now it shows up in an MIT licensed open-source model. Another area where GLM4.6vs surprise developers is front-end automation. When Zepu says pixel accurate replication, they are not exaggerating. You give it a screenshot of an app or website and it reconstructs the full layout as clean HTML, CSS, and JavaScript. the color schemes, component positions, spacing, it all lines up. Then you can circle an area on that screenshot [music] and give a simple instruction like shifting a button or changing a background. And the model",
        "start": 4192.64,
        "duration": 8565.722000000005
    },
    {
        "text": "or website and it reconstructs the full layout as clean HTML, CSS, and JavaScript. the color schemes, component positions, spacing, it all lines up. Then you can circle an area on that screenshot [music] and give a simple instruction like shifting a button or changing a background. And the model edits the snippet. It even uses a visual feedback loop similar to their UI2 code end work where it renders the updated version and checks if the change was applied correctly before sending the final output. That sort of closed loop verification is rare in open source models. [music] Even many enterprise tools do not do that type of self-correction with visual context. But the long context part is just as important. A 128k context window for a multimodal model means you finally get a system that can handle mixed documents at scale. JEIPU reported cases where GLM 4.6V summarized financial reports from four public companies at once, extracted the core metrics, compared them, and built a clean table all in a single pass. Not multiple chunks stitched together, but true global awareness across all documents. And for video, one hour of footage fits into the same window. So the model can summarize the whole match, highlight key moments, and still answer questions about timestamps or goal sequences afterward. That is possible because the model treats frames as visual tokens with temporal encoding using 3D convolutions and timestamp markers during pre-processing. Then there's how they trained it, which is honestly just as impressive. JIEPU used a multi-stage setup. First, massive pre-training, then fine-tuning, and finally reinforcement learning. But their RL system isn't the usual RLHF style that depends on people rating answers. Instead, it learns from verifiable tasks, things with clear right or wrong answers like math problems, chart reading, coding interfaces, spatial reasoning, and video question answering. They even use something called curriculum sampling, which means the model gets progressively harder examples as it gets smarter. On top of that, tool usage is built right into the reward system. So, it literally learns when to call tools, how to plan the next step, and how to keep its outputs clean and structured. And since some penalty systems can mess up image heavy reasoning, they skip those entirely, which help the model stay stable when dealing with visuals. Architecturally, it's based on a vision transformer called AIM V2 huge, which basically helps it see and understand images at a deeper level. There's also an MLP projector that connects what it sees to what it writes. It can handle almost any image size or shape, even those super wide panoramic shots with aspect ratios up to 200 to1, something most vision language models struggle with. For encoding, it uses 2D rope positioning and interpolation tricks to stay precise, while video inputs get compressed over time so they don't overload the system. And because of its extended tokenizer setup, it can format its responses in a structured way that",
        "start": 4343.12,
        "duration": 8870.923000000006
    },
    {
        "text": "something most vision language models struggle with. For encoding, it uses 2D rope positioning and interpolation tricks to stay precise, while video inputs get compressed over time so they don't overload the system. And because of its extended tokenizer setup, it can format its responses in a structured way that using tags like think or begin of box to keep reasoning and answers neatly separated. When Jepu dropped the benchmark results, it became obvious why people were hyped. On Math Vista, GLM 4.6V scored 88.2, beating the previous 4.5V's 84.6 and Quen 3 VL8B's 81.4. On Web Voyager, it hit 81 while Quen stayed at 68.4. On Ref Coco and Treebench, it either matched or set new state-of-the-art results. And the smaller flash model outperformed other light models like Quen 3, VL8B, and GLM 4.1V almost across the board, which is crazy since it's meant to run locally. But the long contextability is what really separates it from the rest. Even massive models like Step 3 with 321B parameters or Quen 3VL 235B can't keep consistency when working with huge mixed inputs, while GLM 4.6V handles them smoothly because its vision system and language brain are perfectly synced for long complex reasoning. The more people dug into the ecosystem side of this, the more the hype escalated because this is not just a model drop. It is a shift in how open-source multimodal systems will be built going forward. A lot of vision language models over the past year felt powerful but disconnected. They could parse images but they did not do much with them beyond answering questions. They lacked the final step, taking visual understanding and turning it into executable action with real tools. GLM 4.6V closes that gap. It positions itself as the backbone for agent frameworks that need to actually observe, plan, and act, not just describe what they are seeing. And when you combine that with an MIT license, a free lightweight version, and pricing that undercuts nearly every major closed source competitor, it suddenly becomes an enterprise ready system that startups and big companies can adopt without friction. The timing also amplified the impact. Zeu came off the GLM 4.5 wave where they already proved themselves as a major open-source contender with models that supported dual reasoning modes, strong coding performance, and those crazy features where you could generate full PowerPoint decks from a single prompt. The 4.6V release feels like the next step in that arc. More grounded, more agent focused, and far more ambitious on the multimodal side. It turns out that when you give developers a model that can handle giant inputs, call tools visually, reconstruct frontends, verify its own output, and still beat benchmarks at this scale, the community reacts pretty fast. So, while a lot of AI updates come and go, this one stuck because it is the first time in a while where an open- source model genuinely feels like it unlocks new workflows. not just incremental",
        "start": 4498.0,
        "duration": 9183.323000000006
    },
    {
        "text": "output, and still beat benchmarks at this scale, the community reacts pretty fast. So, while a lot of AI updates come and go, this one stuck because it is the first time in a while where an open- source model genuinely feels like it unlocks new workflows. not just incremental download the weights right now from hugging face, run the flash variant locally, hit the API through an open AI compatible interface or even try the desktop assistant app on hugging face spaces makes it even more accessible. It is the combination of power, price, licensing, and actual utility that created the buzz. All right, that is where I will leave it. If you are watching developments in multimodal agents, this release definitely marks a turning point and it will be interesting to see how everyone else responds. Open AI dropped a release that feels like somebody finally caught an AI midthought. And I mean that literally. You can trace a decision down to a handful of tiny components like you are following a circuit on a motherboard. That is the vibe of this whole thing. The project is called circuit sparsity and it comes from a paper with a pretty blunt title. Weight sparse transformers have interpretable circuits. Open AAI also shipped an actual model on hugging face called openai/circuitsparsity plus a toolkit on GitHub called openai/circuit_sparity. So this is one of those moments where it is research and it is also tooling you can touch. Here is the punchline up front. They trained a GPT2 style transformer on Python code and they forced it to learn with almost all of its wires cut. The wild part is they did not do it later. They enforced sparsity during optimization, during training, every step, the whole time. So let us zoom into what weight sparse transformer means here because the details are the story. Normally a language model is built like a massive tangled web. Every part talks to every other part. Millions, sometimes billions of connections firing at once. That is why people call them black boxes. Even if the answer is correct, nobody can really say which internal parts mattered and which ones were just noise. OpenAI decided to do something that sounds almost reckless at first. They trained a model while deliberately cutting most of those connections away. Not after training. During training, step by step, every time the model updated itself, OpenAI forced it to keep only the strongest connections and delete the rest. Not weaken, not ignore, but fully zero out. The result is extreme. In the most aggressive version, only about one out of every 1,000 connections survives. That means over 99.9% of the internal wiring is gone. And they did not stop there. They also limited how many internal parts are allowed to activate at all. Roughly only one out of every four internal signals is allowed to light up at any moment. So fewer connections, fewer active parts, much",
        "start": 4657.12,
        "duration": 9469.00400000001
    },
    {
        "text": "99.9% of the internal wiring is gone. And they did not stop there. They also limited how many internal parts are allowed to activate at all. Roughly only one out of every four internal signals is allowed to light up at any moment. So fewer connections, fewer active parts, much people would expect the model to fall apart. And that is the trick. It does not. The reason it survives is how open AI trains it. Early on, the model starts out normal and flexible. Then slowly over time, the allowed number of connections gets smaller and smaller. The model is forced to compress what it learned into fewer and fewer internal pieces. Whatever survives that process ends up being the most essential logic. That setup lets OpenAI do something very revealing. They can keep performance the same while shrinking the internal machinery. And when they compare these sparse models to normal dense ones, they see something striking. For the same level of accuracy, the internal thinking machinery inside the sparse models is about 16 times smaller. In simple terms, the same behavior fits inside a much simpler internal program. And this is where the idea of circuits comes in. Before we jump deeper into the story, there's something I keep seeing in the comments. People asking how we managed to produce so much content so fast. Look, in 2025 alone, this channel pulled in 32 million views. That's not luck. That's not grinding harder. It's because every time a new AI breakthrough drops, we plug it straight into our workflow. Most people watch AI news and move on. We use it immediately. So, we decided to release something we've never shared before. The 2026 AI playbook. 10,00 prompts to dominate the AI era. This is how you go from just consuming AI content to actually using AI to build real unfair advantages for yourself. Get your proposals done in 20 minutes instead of 4 hours. Launch that side business you keep putting off. Become the person in your company who gets twice as much done in half the time. Founding member access opens soon. Join the wait list in the description. All right, back to the video. Instead of talking about vague features or hidden states, OpenAI defines everything very concretely. A circuit is just a small group of internal units and the exact connections between them. Each unit is tiny. One neuron, one attention channel, one read or write slot in memory. And each connection is literally a single surviving weight. So now the question becomes, can we find the smallest possible internal circuit that still solves a task? To test that, they created 20 very simple coding challenges. Each one forces the model to choose between two possible next tokens. No open-ended answers, no creativity. Just pick option A or option B. Some examples feel almost trivial on the surface. Close a string with a single quote or a double quote. Decide whether",
        "start": 4802.08,
        "duration": 9765.80400000001
    },
    {
        "text": "20 very simple coding challenges. Each one forces the model to choose between two possible next tokens. No open-ended answers, no creativity. Just pick option A or option B. Some examples feel almost trivial on the surface. Close a string with a single quote or a double quote. Decide whether double closing square bracket based on how deeply nested a list is. Track whether a variable was created as a set [music] or a string so the model knows whether to use dot add or plus equals later. Then they do something clever. They start removing internal parts of the model until performance drops. The goal is to find the smallest internal mechanism that still solves the task well enough. They do not guess. They optimize for it directly. Anything removed gets frozen to an average value. So, it cannot secretly help anymore. What is left at the end is not a visualization. It is a stripped down internal machine that actually does the job. And this is where the title of the video starts to earn its weight. For the quote closing task, the final circuit has 12 internal units and nine connections. That is it. Inside that tiny circuit, two units appear almost immediately. One activates whenever the model sees a quote, any quote at all. The other carries a simple signal that tells the difference between single and double quotes. Later on, another internal component takes that signal and copies it to the end of the sequence right where the closing quote needs to appear. So, the model is not guessing. It is not pattern matching loosely. It is running a tiny internal routine. Detect then classify then copy then output. You can follow it step by step. The bracket counting [music] task looks different but just as clean. When the model sees an opening bracket, it triggers a few internal detectors. Another component looks across the whole sequence and averages those signals which effectively turns into a sense of nesting depth. Later one more component checks that depth and decides whether a single closing bracket is enough or whether a double one is required. That is counting plain and simple. Then there is the variable type task and this one is especially interesting. When the variable current is first created, the model stores a tiny internal marker that says what type it is. Later on when the model [music] has to choose how to modify it, another internal component retrieves that marker and uses it to pick the correct operation. So the model remembers not vaguely, not statistically. It stores something then retrieves it later when it matters. These circuits are small enough that you can actually read them and that is the moment where it stops feeling abstract. You are no longer talking about outputs. You are watching internal decisions form. Then open AAI adds one more layer that makes this even more powerful. They introduce something called bridges. Think of bridges as translators. They",
        "start": 4953.04,
        "duration": 10038.204000000012
    },
    {
        "text": "actually read them and that is the moment where it stops feeling abstract. You are no longer talking about outputs. You are watching internal decisions form. Then open AAI adds one more layer that makes this even more powerful. They introduce something called bridges. Think of bridges as translators. They readable sparse model and a normal dense model. You can take a specific internal signal from the sparse model, tweak it, and inject that change into a dense model. So instead of saying this behavior exists in a toy model, you can say this feature exists and here is how it affects a full scale system. That is a big shift. It means interpretable features do not have to stay trapped in research demos. [music] They can be mapped onto real models. And this is not just theory. OpenAI released an actual model. It is called OpenAI/Circuit Sparity. It has 0.4 billion parameters and it is available on HuggingFace under an Apache 2.0 license. They also released the full toolkit on GitHub, including the tasks and a visual interface for exploring circuits. You can load it, run it on Python code, and know that inside almost everything is zeroed out. What remains is the minimum machinery needed to function. That is why this release feels different. It is not about making AI stronger. It is about making AI legible. You are not just seeing what the model says. You are seeing how it arrives there using a handful of internal parts you can point to and name. And that is why open AI just caught an AI thinking lands. Not because the model is conscious because for the first time at this scale, the internal process stopped being a blur and started looking like a sequence of actual decisions you can follow. Around the same time, OpenAI dropped this circuit sparity work. Axios published a piece with a headline that quietly says a lot. Open AAI is not too big to fail. It is bigger and that framing matters. The company sits at the center of the AI economy in a way no other lab really does. When OpenAI shifts direction, [music] investors feel it almost instantly. According to Axios, Sam Alman is dealing with pressure from multiple sides at once. Competition from Google is intense. Lawsuits from families continue. [music] And there is over $1 trillion in long-term spending commitments tied to infrastructure, chips, and data centers. The AI economy has wrapped itself around OpenAI's trajectory, which makes even small signals ripple outward. Axios points out that a simple suggestion of delays in Oracle built data centers for OpenAI was enough [music] to move tech stocks. That is how sensitive venture capitalist and MIT research fellow Paul Kedrski described it bluntly. OpenAI's individual role might look small at first glance, then that impression collapses once you look at how interconnected everything has become. A serious stumble would freeze parts of the ecosystem in place. Dip Singh,",
        "start": 5091.36,
        "duration": 10328.764000000005
    },
    {
        "text": "That is how sensitive venture capitalist and MIT research fellow Paul Kedrski described it bluntly. OpenAI's individual role might look small at first glance, then that impression collapses once you look at how interconnected everything has become. A serious stumble would freeze parts of the ecosystem in place. Dip Singh, and now head of global macro research at PGIM, took it a step further. He warned that if open AI falters, the foundations of the AI sector weaken [music] fast with effects cascading through chip demand, capital spending, and financial markets. He pointed to a specific pressure point, chips. Microsoft and Meta have been buying aggressively to avoid falling behind. If OpenAI's momentum slows, that urgency fades. A drop in chip orders would hit billions in capital expenditures that have been propping up growth. Singh even floated a rough figure, suggesting that as much as half of that growth could stall. Those chips also sit behind loans as collateral, which means shifts in demand ripple [music] into credit markets, too. Inside OpenAI, leadership has pushed back on the idea of government backs stops or special treatment. Alman has said openly that failure should remain possible. Open AAI's public messaging emphasizes confidence, strong investors, and continued progress. Still, the scrutiny remains because the stakes are real. At the same time, OpenAI is preparing consumerf facing changes that make internal decision-making matter even more. Techraar reports that an adult mode for chat GPT is planned for early 2026, confirmed by Fiji Simo. Access would depend on an age prediction system that infers user age from behavior and context [music] rather than a simple checkbox. The system is already being tested in a few countries. This move goes beyond adult content. It opens conversations around topics currently filtered for being too sensitive, including relationships, sexuality, and mental health. That kind of feature brings legal, regulatory, and trust questions with it. Governments worldwide are tightening rules around age verification, [music] and OpenAI clearly wants a solution that scales across regions while keeping users engaged, possibly through premium tiers. And this is where circuit sparity quietly connects [music] back in. Open AAI is building systems where internal decisions carry real external consequences. decisions about code behavior, decisions about content boundaries, decisions that regulators, users, and investors care deeply about. In that environment, having models with clearer internal mechanisms, fewer hidden interactions, and traceable decision paths stops being academic curiosity. It starts looking like infrastructure. Circuit sparity fits into that picture as a way to turn internal behavior into something compact, readable, and steerable. a way to shrink complex behavior into small machines you can point at, [music] test, and move with intent. That context gives this release weight far beyond a single research paper. Does readable AI move us closer to real control, or does it accelerate power in ways we still underestimate? [music] Drop your take in the comments. I want to see where you land on this. If this breakdown helped,",
        "start": 5239.04,
        "duration": 10644.390000000005
    },
    {
        "text": "context gives this release weight far beyond a single research paper. Does readable AI move us closer to real control, or does it accelerate power in ways we still underestimate? [music] Drop your take in the comments. I want to see where you land on this. If this breakdown helped, deep dives like this. Thanks for watching and catch you in the next one.",
        "start": 5399.227,
        "duration": 10655.271000000006
    }
]