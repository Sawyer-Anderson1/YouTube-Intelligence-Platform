[
    {
        "text": " You know what's crazy? That all of this is real. Yeah. Meaning, don't you think so? Meaning what? Like all this AI stuff and all this area. Yeah. That it's happen. Like, isn't it straight out of science fiction? When the man who gave us GBD3 and the whole scaling laws religion says, \"Game over. We are back to research.\" Don't ignore it. Gary Marcus is smiling. Yan Lun is smiling. And there is no stopping them if they say, \"I told you so.\" Which by the way has already begun. And suddenly everything comes full circle. Now the headline reads something like, \"Ilia saw nothing at OpenAI and then left it to set up SSI.\" Well, to start all over again, tonight on Front Page, we break down what Ilia actually said, why he thinks more GPUs won't magically give us AGI and what this means for Nvidia, AMD, Indian IT, and every AI lab betting the house on well, you know, just scale it, bro. For the last 5 years, AI strategy was very simple. more data, bigger models, more compute. That formula, the scaling laws powered everything from GPD3 to GPD4, Gemini, Claude, you name it. But in this new interview with Daresh Patel, Sutska basically draws a line in the sand. He splits modern AI into three eras. Here they are. 2012 to 2020, the age of research. people tinkering, inventing deep learning transformers. Then comes 2020 to 2025, the age of scaling, pre-training everything, piling on parameters, tokens, GPUs, and whatnot. And then finally, now it's back to the age of research again, just with bigger computers. So he asks a very brutal question. If we already have huge models is the belief really that if you had 100x more everything would be transformed. I don't think that's true. That's the core message. Scaling will still help but it won't solve the next big problems. Even mainstream coverage picks up the same line. Sudska telling Darkh that data is finite, compute is already massive and now the bottleneck is ideas not GPUs. So what's actually broken? Ilia's answer is one word, generalization. The thing which I think is the most fundamental is that these models somehow just generalize dramatically worse than people. It's super obvious. End quote. He gives two very killer analogies. First one, teen learning to drive versus LLMs. Teenagers learn to drive in maybe well 10 hours. Models need millions of samples and still make dumb repetitive mistakes. Humans learn from messy sparse experience and don't keep crashing in the same way. Here is the second analogy. Two students in competitive programming. Student A 10,000 hours every problem memorized. Student B 100 hours still does well and later does better in real life. Today's models are like super overtrained student A, insane at benchmarks, weak at real world taste and well judgment. That's IA's points. We can brute force performance on evals, but we are not teaching models to have",
        "start": 4.72,
        "duration": 456.7989999999999
    },
    {
        "text": "B 100 hours still does well and later does better in real life. Today's models are like super overtrained student A, insane at benchmarks, weak at real world taste and well judgment. That's IA's points. We can brute force performance on evals, but we are not teaching models to have real reward hackers aren't the models. They are the researchers designing evals and RL loops so that the models look good on paper while still failing in the wild. Skeka doesn't say pre-training is dead. In fact, he calls it a great recipe because you don't have to choose data sets. You just take the whole world projected onto text. The companies love it. a very low-risk way to invest more data plus more compute which leads to guaranteed incremental gains. But he sees three hard limits which is finite data. At some point though pre-training will run out of data. The data is very clearly finite. Diminishing returns going from 1x to 10x to 100x scale still helps but doesn't magically unlock general human level learning. And then you don't fix taste, judgment, or robustness just by throwing more web text at the model. RL that overfits to benchmarks. Companies create RL environments inspired by evals. Models look superhuman on coding competitions but still fail to maintain a large messy code base. So in his words, we are back to the age of research with big computers. The compute stays, but the easy recipe is over. The spiciest part of the interview is when Daresh presses him. If scaling is plateaued, what's the new recipe? Ilia's answer is both hopeful and frustrating. He believes there is a machine learning principle that can give models humanlike learning speed and robustness. He explicitly says the teenage driver example is proof it's possible. But we live in a world where not all ML ideas are discussed freely and this is one of them. So he has ideas but he's not sharing them. Classic frontier lab energy. He also hints that human neurons might be doing more compute than we assume. Emotions act like a built-in value function helping us make decisions without needing explicit rewards every time. Evolution somehow hardcodes very high level social desires, caring what others think, wanting status in a way we don't yet know how to formalize. So put simply, brains have tricks for value learning and robustness that current LLMs simply don't. Now if you're Nvidia or AMD, this is well awkward. For the last 3 years, the whole industry narrative has been more compute is equal to better models is equal to AGI. Yes, Sudsko doesn't say compute stops mattering. In fact, he calls it a big differentiator when everybody is exploring similar ideas and public coverage today emphasizes that he still sees massive compute as essential just not sufficient for the next breakthroughs. The subtle but important shift is before buy more GPUs you will get better models. Now you can buy all",
        "start": 232.4,
        "duration": 844.7210000000002
    },
    {
        "text": "it a big differentiator when everybody is exploring similar ideas and public coverage today emphasizes that he still sees massive compute as essential just not sufficient for the next breakthroughs. The subtle but important shift is before buy more GPUs you will get better models. Now you can buy all wrong you're just wasting electricity faster. For GPU vendors and data center builders that means demand will stay high especially for RL multi- aent training and experimental architectures. But Wall Street's favorite story, which is infinite scaling is equal to infinite progress, just took a hit from one of its original architects. Darkh finally asks the question everyone cares about. So when do you think we get to a system that learns as well as humans and then becomes superhuman? Ilia's answer, 5 to 20 years. Not next year, not 2100. A 5 to 20 year band. And that's coming from someone who has personally pushed the frontier for over a decade. He also thinks current approaches will go some distance and then well peter out. The it the real general intelligence recipe is something we don't know yet how to build. So how do we see it from the front page lens? Here it is. The scaling party isn't over but the DJ has changed the track. Well, you'll still see bigger models, more clusters, more GPUs, but the easy just scale thesis is now officially under question by one of its own inventors. The new prestige is research taste, not just LOPS. Ilia explicitly talks about beauty, simplicity, and correct inspiration from the brain as his compass for good ideas. So in an age of research taste becomes ano as important as compute. For India this is a chance not a threat. If the bottleneck is better learning rules, value functions and generalization than countries that can produce deep researchers not just infrastructure buyers have leverage for semi and cloud giants. This is a narrative shock. They'll still sell hardware and capacity, but sooner or later inventors will ask, \"Okay, you bought the GPUs. What's your new idea? Is this really game over for scaling laws or just the end of the first scaling chapter?\" Do you agree with Ilia that we are back in the age of research or do you think 100x more compute will still brute force us to AGI? Please drop your thoughts in the comments below.",
        "start": 430.08,
        "duration": 1154.6389999999997
    }
]