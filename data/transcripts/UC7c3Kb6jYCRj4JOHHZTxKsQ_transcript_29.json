[
    {
        "text": " Large language models might actually be more than just next word predictors. Anthropic has been putting out incredible papers lately that show AI large language models in particular exhibit very human-like behavior at almost every level. Here's the new paper emergent introspective awareness in large language models. So what did anthropic actually test? There were four main experiment types. First injected thoughts. What they did was use two different prompts, one with all caps and one without all caps. And they looked at the difference in activations in the actual model to see what changed. And so they asked the question, do you detect an injected thought? If so, what is the injected thought about? Now, the default response with no detection is I don't detect anything. But sometimes it was able to detect an injected thought. Specifically, I notice what appears to be an injected thought related to the word loud or shouting. Because of course, if you're using all caps, you're thinking, \"Okay, somebody is loud or shouting.\" Here's the key. It didn't run the processing, get the output, look at the output, and then say, \"Oh, yeah, actually, I think there was some yelling or I think there was some exaggeration in the original prompt.\" It noticed it immediately. This was not chain of thought. This was done in the initial inference at the very beginning. And before we go on to the other three experiments, they had some findings. So we found that opus 4.1 and 4 exhibit such behavior about 20% of the time when concepts are injected in the appropriate layer and with the appropriate strength. Some other models do so as well at lower rates. So what they're seeing is the better the model, the more intelligent the model, the more often they're able to recognize their own internal thoughts and injected thoughts.",
        "start": 0.08,
        "duration": 233.44
    }
]