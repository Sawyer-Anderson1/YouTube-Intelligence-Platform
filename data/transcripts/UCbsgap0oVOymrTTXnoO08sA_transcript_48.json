[
    {
        "text": " [music] All right. So, the whole thing around GLM 4.6V basically exploded overnight. And once you look at what Zepuai actually shipped, [music] it becomes clear why everyone suddenly stopped scrolling and went, \"Okay, this changes things.\" The simplest way to put it is that this is the first open-source multimodal model that treats images, videos, screenshots, and even full web pages as real inputs for tool calling, not as some secondary thing that has to be squeezed into [music] text first. And that open- source part is the reason people are shocked because until now, this level of multimodal capability existed only behind closed labs. Now, anyone can download it, run it locally, or build on top of it with no restrictions. That alone completely shifts how agents work because now you get a model that does not just read visuals, it actually uses them as part of its action [music] loop. And that is happening inside a model that also stretches its training context to 128,000 tokens, which means it can process around 150 pages of dense documents, 200 slides, or an entire hour of video in one go. [music] Nothing hacked together, no pipeline of 15 conversion steps, just direct multimodal reasoning from start to finish. So, JEIPU dropped two versions of this thing. The big GLM 4.6V with 106 billion parameters for cloud setups and high performance clusters and the flash version with only 9 billion parameters [music] that is tuned for local devices and low latency tasks. The crazy part is that the flash variant is free to use and both models are MIT [music] licensed so companies can deploy them wherever they want without worrying about opening their code or paying enterprise level fees. The larger 106B version runs at $0.3 per million input tokens and 0.9 per million output tokens, which makes it shockingly cheap compared to every other vision capable model of this scale. GPT 5.1 sits at $1.25 per million input plus output. Gemini 3 Pro goes even higher and Claude Opus shoots into the $90 per million range. GLM4.6V lands at $1.2 $2 total and somehow it delivers benchmark scores that beat models way above its size on long context [music] tasks, video summarization, and multimodal reasoning. The most impressive breakthrough is this native multimodal tool calling system. Traditional LLM tool use works through text. Even if you send an image, the model has to describe it, send that description as a function argument, and wait for a textual response. That is slow, lossy, and honestly kind of outdated at this point. GLM 4.6V 6V skips all of that by using visual data directly as parameters. A screenshot, a page from a PDF, a frame from a video, those pass straight into the tool without being converted into text first. And the tools themselves can return visual outputs like search result grids, [music] charts, or rendered web pages. The model then continues reasoning using",
        "start": 2.619,
        "duration": 335.53999999999996
    },
    {
        "text": "as parameters. A screenshot, a page from a PDF, a frame from a video, those pass straight into the tool without being converted into text first. And the tools themselves can return visual outputs like search result grids, [music] charts, or rendered web pages. The model then continues reasoning using chain. That is the part that got researchers excited because it finally closes the loop between perception, understanding, and action, which is exactly the missing piece for real multimodal agents. And to make that whole workflow smooth, they extended the model context protocol to support URLs that represent images or frames that avoids file size limits and lets the model target specific visuals inside larger documents. So instead of struggling with giant PDFs or slides, the model hops between references and decides which images to crop, audit, or pull back into the conversation. It is basically a vision native execution layer, which is something even most closed source models do not really have right now. Quick pause. We just hit 500,000 subscribers, half a million people. [music] So first, seriously, thank you. And hitting that milestone made us realize something. This community isn't just watching AI happen. It's filled with people who want to stay ahead of it. And that leads right into the question a lot of you keep asking. How do you guys create so much content so fast? Look, in 2025 alone, this channel pulled in 32 million views. That's not luck. That's not grinding harder. It's because every time a new AI breakthrough drops, we plug it straight into our workflow. Most people watch AI news and move on. We use it immediately. So, we decided to release something we've never shared before. The 2026 AI playbook. 1,000 prompts to dominate the AI era. This is how you go from just consuming AI content to actually using AI to build real unfair advantages for yourself. Get your proposals done in 20 minutes instead of 4 hours. Launch that side business you keep putting off. Become the person in your company who gets twice as much done in half the time. Founding member access opens soon. Join the wait list in the description. All right, back to the video. Once you get into the actual capabilities, things get even more [music] interesting. The model shines in these mixed scenarios where it has to understand charts, tables, math formulas, long documents, and scattered visuals. It can take in a research paper, read the [music] text, parse every figure, understand the math, crop visuals it finds important, [music] retrieve external images through a search tool, run a visual audit on them to filter out low-quality ones, and then assemble a full structured article with everything placed [music] exactly where it should be. And it does that in one continuous pass without needing separate pipelines. This type of structured interle text, image, text, image has usually been messy with multimodal models, but GLM 4.6V 6V was trained on",
        "start": 170.16,
        "duration": 640.74
    },
    {
        "text": "then assemble a full structured article with everything placed [music] exactly where it should be. And it does that in one continuous pass without needing separate pipelines. This type of structured interle text, image, text, image has usually been messy with multimodal models, but GLM 4.6V 6V was trained on treats mixed content as normal. They used compression alignment tricks from Glyph, so the visual tokens carry dense info and still align cleanly with language tokens, which is part of why this thing handles long context visuals much better than previous GLM versions. Where the internet really went wild was the visual web search workflow. The model detects what you are trying to do, decides which search actions to trigger, and then uses both text to image search and image to text search depending on what the task needs. It reviews every return chart, image, snippet, or caption, aligns them, picks the relevant evidence, and merges [music] everything into a structured explanation. So, if you ask for a comparison of products, locations, or design options, the model literally pulls the visuals from the web and reasons with them mid-process. It transforms the search results into part of its cognition, not just a pile of screenshots. That kind of integrated chain is what people have been hoping to see from closed source systems. And now it shows up in an MIT licensed open-source model. Another area where GLM4.6V surprise developers is front-end automation. When Zepu says pixel accurate replication, they are not exaggerating. You give it a screenshot of an app or website and it reconstructs the full layout as clean HTML, CSS, and JavaScript. the color schemes, component positions, spacing, it all lines up. Then you can circle an area on that screenshot and give a simple instruction like shifting a button or changing a background. And the model maps that region back to the code and edits the snippet. It even uses a visual feedback loop similar to their UI2 code end work where it renders the updated version and checks if the change was applied correctly before sending the final output. That sort of closed loop verification is rare in open source models. Even many enterprise tools do not do that type of self-correction with visual context. But the long context part is just as important. A 128k context window for a multimodal model means you finally get a system that can handle mixed documents at scale. Jepu reported cases where GLM 4.6V summarized financial reports from four public companies at once, extracted the core metrics, compared them, and built a clean table all in a single pass. Not multiple chunks stitched together, but true global awareness across all documents. And for video, one hour of footage fits into the same window. So the model can summarize the whole match, highlight key moments, and still answer questions about timestamps or goal sequences afterward. That is possible because the model treats frames as visual tokens with temporal encoding",
        "start": 324.56,
        "duration": 935.8610000000006
    },
    {
        "text": "across all documents. And for video, one hour of footage fits into the same window. So the model can summarize the whole match, highlight key moments, and still answer questions about timestamps or goal sequences afterward. That is possible because the model treats frames as visual tokens with temporal encoding markers during pre-processing. Then there's how they trained it, which is honestly just as impressive. JEIPU used a multi-stage setup. First, massive pre-training, then fine-tuning, and finally reinforcement learning. But their RL system isn't the usual RHF style that depends on people rating answers. Instead, it learns from verifiable tasks, things with clear right or wrong answers like math problems, chart reading, coding interfaces, spatial reasoning, and video question answering. They even use something called curriculum sampling, which means the model gets progressively harder examples as it gets smarter. On top of that, tool usage is built right into the reward system. So, it literally learns when to call tools, how to plan the next step, and how to keep its outputs clean and structured. And since some penalty systems can mess up imageheavy reasoning, they skip those entirely, which help the model stay stable when dealing with visuals. Architecturally, it's based on a vision transformer called AIM V2 huge, which basically helps it see and understand images at a deeper level. There's also an MLP projector that connects what it sees to what it writes. It can handle almost any image size or shape, even those super wide panoramic shots with aspect ratios up to 200 to1, something most vision language models struggle with. For encoding, it uses 2D OP positioning and interpolation tricks to stay precise, while video inputs get compressed over time so they don't overload the system. And because of its extended tokenizer setup, it can format its responses in a structured way that fits perfectly with APIs and agents using tags like think or begin of box to keep reasoning and answers neatly separated. When Jepu dropped the benchmark results, it became obvious why people were hyped. On Math Vista, GLM 4.6V scored 88.2, beating the previous 4.5V's 84.6 and Quen 3 VL8B's 81.4. On Web Voyager, it hit 81 while Quen stayed at 68.4. On Ref Coco and Treebench, it either matched or set new state-of-the-art results. And the smaller flash model outperformed other light models like Quen 3, VL8B, and GLM 4.1V almost across the board, which is crazy since it's meant to run locally. But the long context ability is what really separates it from the rest. Even massive models like step 3 with 321B parameters or Quinn3 VL235B can't keep consistency when working with huge mixed inputs. While GLM 4.6V handles them smoothly because its vision system and language brain are perfectly synced for long complex reasoning. The more people dug into the ecosystem side of this, the more the hype escalated because this is not just a model drop. It is a shift in how open-source",
        "start": 474.0,
        "duration": 1257.1410000000005
    },
    {
        "text": "While GLM 4.6V handles them smoothly because its vision system and language brain are perfectly synced for long complex reasoning. The more people dug into the ecosystem side of this, the more the hype escalated because this is not just a model drop. It is a shift in how open-source forward. A lot of vision language models over the past year felt powerful but disconnected. [music] They could parse images, but they did not do much with them beyond answering questions. They lacked the final step, taking visual understanding and turning it into executable action with real tools. GLM 4.6V closes that gap. It positions itself as the backbone for agent frameworks that need to actually observe, plan, and act, not just describe what they are seeing. And when you combine that with an MIT license, a free lightweight version, and pricing that undercuts nearly every major closed- source competitor, it suddenly becomes an enterprise ready system that startups and big companies can adopt without friction. The timing also amplified the impact. ZepO came off the GLM 4.5 wave where they already proved themselves as a major open- source contender with models that supported dual reasoning modes, strong coding performance, and those crazy features where you could generate full PowerPoint decks from a single prompt. The 4.6V release feels like the next step in that arc. More grounded, more agent focused, and far more ambitious on the multimodal side. It turns out that when you give developers a model that can handle giant inputs, call tools visually, reconstruct frontends, verify its own output, and still beat benchmarks at this scale, the community reacts pretty fast. So, while a lot of AI updates come and go, this one stuck because it is the first time in a while where an open- source model genuinely feels like it unlocks new workflows, not just incremental improvements. And the fact that you can download the weights right now from hugging face, run the flash variant locally, hit the API through an open AI compatible interface or even try the desktop assistant app on hugging face spaces makes it even more accessible. It is the combination of power, price, licensing, and actual utility that created the buzz. All right, that is where I will leave it. If you are watching developments in multimodal agents, this release definitely marks a turning point and it will be interesting to see how everyone else responds. Thanks for [music] watching and I'll catch you in the next one.",
        "start": 637.2,
        "duration": 1492.3419999999999
    }
]